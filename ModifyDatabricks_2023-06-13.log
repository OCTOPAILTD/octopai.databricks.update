/Formula1-Project-Solutions (1)/Section-16-17-18/utils/1.prepare_for_incremental_load
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/9.create_processed_database
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/0.ingest_all_files
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/2.ingest_races_file
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/5.ingest_results_file
/Formula1-Project-Solutions (1)/Section-16-17-18/raw/1.create_raw_tables
/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/2.find_dominant_teams
/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/3.viz_dominant_drivers
/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/4.viz_dominant_teams
/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/1.find_dominant_drivers
/Formula1-Project-Solutions (1)/Section-12/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions (1)/Section-12/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions (1)/Section-12/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions (1)/Section-12/ingestion/5.ingest_results_file
/Formula1-Project-Solutions (1)/Section-12/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions (1)/Section-12/ingestion/0.ingest_all_files
/Formula1-Project-Solutions (1)/Section-12/ingestion/2.ingest_races_file
/Formula1-Project-Solutions (1)/Section-12/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions (1)/Section-12/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions (1)/Section-20/formula1/set-up/mount_adls_storage
/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/4.viz_dominant_teams
/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/3.viz_dominant_drivers
/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/2.find_dominant_teams
/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/1.find_dominant_drivers
/Formula1-Project-Solutions (1)/Section-20/formula1/raw/1.create_raw_tables
/Formula1-Project-Solutions (1)/Section-20/formula1/utils/1.prepare_for_incremental_load
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/4.sql_temp_view_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/9.sql_joins_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/6.sql_objects_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/5.sql_temp_view_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/3.aggregation_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/7.sql_basics_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/2.join_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/8.sql_functions_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/10.delta_lake_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/demo/1.filter_demo
/Formula1-Project-Solutions (1)/Section-20/formula1/includes/common_functions
/Formula1-Project-Solutions (1)/Section-20/formula1/includes/configuration
/Formula1-Project-Solutions (1)/Section-20/formula1/trans/1.race_results
/Formula1-Project-Solutions (1)/Section-20/formula1/trans/4.calculated_race_results
/Formula1-Project-Solutions (1)/Section-20/formula1/trans/3.constructor_standings
/Formula1-Project-Solutions (1)/Section-20/formula1/trans/0.create_presentation_database
/Formula1-Project-Solutions (1)/Section-20/formula1/trans/2.driver_standings
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/9.create_processed_database
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/2.ingest_races_file
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/0.ingest_all_files
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/5.ingest_results_file
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/3.viz_dominant_drivers
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/2.find_dominant_teams
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/4.viz_dominant_teams
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/1.find_dominant_drivers
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/set-up/mount_adls_storage
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/0.ingest_all_files
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/9.create_processed_database
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/5.ingest_results_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/2.ingest_races_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/3.constructor_standings
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/2.driver_standings
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/1.race_results
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/4.calculated_race_results
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/0.create_presentation_database
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/4.sql_temp_view_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/8.sql_functions_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/10.delta_lake_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/9.sql_joins_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/1.filter_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/5.sql_temp_view_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/6.sql_objects_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/2.join_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/3.aggregation_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/7.sql_basics_demo
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/includes/common_functions
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/includes/configuration
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/utils/1.prepare_for_incremental_load
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/raw/1.create_raw_tables
/Formula1-Project-Solutions (1)/Section-14/trans/2.driver_standings
/Formula1-Project-Solutions (1)/Section-14/trans/1.race_results
/Formula1-Project-Solutions (1)/Section-14/trans/3.constructor_standings
/Formula1-Project-Solutions (1)/Section-19/trans/1.race_results
/Formula1-Project-Solutions (1)/Section-19/trans/4.calculated_race_results
/Formula1-Project-Solutions (1)/Section-19/trans/3.constructor_standings
/Formula1-Project-Solutions (1)/Section-19/trans/0.create_presentation_database
/Formula1-Project-Solutions (1)/Section-19/trans/2.driver_standings
/Formula1-Project-Solutions (1)/Section-19/includes/common_functions
/Formula1-Project-Solutions (1)/Section-19/includes/configuration
/Formula1-Project-Solutions (1)/Section-19/ingestion/9.create_processed_database
/Formula1-Project-Solutions (1)/Section-19/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions (1)/Section-19/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions (1)/Section-19/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions (1)/Section-19/ingestion/2.ingest_races_file
/Formula1-Project-Solutions (1)/Section-19/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions (1)/Section-19/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions (1)/Section-19/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions (1)/Section-19/ingestion/0.ingest_all_files
/Formula1-Project-Solutions (1)/Section-19/ingestion/5.ingest_results_file
/Formula1-Project-Solutions (1)/Section-06/mount_adls_storage-lesson-6
/Formula1-Project-Solutions (1)/Section-06/mount_adls_storage-lesson-9
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/5.ingest_results_file
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/2.ingest_races_file
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/8.ingest_qualifying_file
/Users/dora@octopai.com/Sql_PY (1)
/Users/zacayd@octopai.com/splineScaleExample
/Users/zacayd@octopai.com/ScalaCode
/Users/zacayd@octopai.com/SplineCode
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/utils/1.prepare_for_incremental_load
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/9.create_processed_database
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/0.ingest_all_files
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/7.ingest_lap_times_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/8.ingest_qualifying_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/6.ingest_pit_stops_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/4.ingest_drivers_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/1.ingest_circuits_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/2.ingest_races_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/3.ingest_constructors_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/5.ingest_results_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/raw/1.create_raw_tables
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/2.find_dominant_teams
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/3.viz_dominant_drivers
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/4.viz_dominant_teams
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/1.find_dominant_drivers
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/1.ingest_circuits_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/7.ingest_lap_times_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/4.ingest_drivers_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/5.ingest_results_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/8.ingest_qualifying_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/0.ingest_all_files
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/2.ingest_races_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/3.ingest_constructors_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/6.ingest_pit_stops_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/set-up/mount_adls_storage
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/4.viz_dominant_teams
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/3.viz_dominant_drivers
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/2.find_dominant_teams
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/1.find_dominant_drivers
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/raw/1.create_raw_tables
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/utils/1.prepare_for_incremental_load
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/4.sql_temp_view_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/9.sql_joins_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/6.sql_objects_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/5.sql_temp_view_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/3.aggregation_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/7.sql_basics_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/2.join_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/8.sql_functions_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/10.delta_lake_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/1.filter_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/includes/common_functions
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/includes/configuration
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/1.race_results
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/4.calculated_race_results
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/3.constructor_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/0.create_presentation_database
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/2.driver_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/7.ingest_lap_times_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/6.ingest_pit_stops_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/3.ingest_constructors_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/4.ingest_drivers_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/1.ingest_circuits_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/9.create_processed_database
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/2.ingest_races_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/0.ingest_all_files
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/5.ingest_results_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/8.ingest_qualifying_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/3.viz_dominant_drivers
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/2.find_dominant_teams
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/4.viz_dominant_teams
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/1.find_dominant_drivers
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/set-up/mount_adls_storage
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/0.ingest_all_files
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/7.ingest_lap_times_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/9.create_processed_database
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/4.ingest_drivers_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/5.ingest_results_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/8.ingest_qualifying_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/3.ingest_constructors_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/2.ingest_races_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/6.ingest_pit_stops_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/1.ingest_circuits_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/3.constructor_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/2.driver_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/1.race_results
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/4.calculated_race_results
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/0.create_presentation_database
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/4.sql_temp_view_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/8.sql_functions_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/10.delta_lake_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/9.sql_joins_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/1.filter_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/5.sql_temp_view_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/6.sql_objects_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/2.join_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/3.aggregation_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/7.sql_basics_demo
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/includes/common_functions
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/includes/configuration
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/utils/1.prepare_for_incremental_load
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/raw/1.create_raw_tables
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-14/trans/2.driver_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-14/trans/1.race_results
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-14/trans/3.constructor_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/1.race_results
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/4.calculated_race_results
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/3.constructor_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/0.create_presentation_database
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/2.driver_standings
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/includes/common_functions
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/includes/configuration
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/9.create_processed_database
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/3.ingest_constructors_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/4.ingest_drivers_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/6.ingest_pit_stops_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/2.ingest_races_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/8.ingest_qualifying_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/7.ingest_lap_times_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/1.ingest_circuits_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/0.ingest_all_files
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/5.ingest_results_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-6
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-9
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/7.ingest_lap_times_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/5.ingest_results_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/2.ingest_races_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/3.ingest_constructors_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/4.ingest_drivers_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/1.ingest_circuits_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/6.ingest_pit_stops_file
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/8.ingest_qualifying_file
/Users/zacayd@octopai.com/pythonz
/Users/zacayd@octopai.com/Sandvine_BillingRecon
/Users/zacayd@octopai.com/SqlExample
/Users/zacayd@octopai.com/Zacay
/Users/zacayd@octopai.com/Scala
/Users/zacayd@octopai.com/Scala2
/Users/zacayd@octopai.com/NoteBookTest
/Users/zacayd@octopai.com/2022-10-31 - DBFS Example
/Users/zacayd@octopai.com/Scala1
/Users/zacayd@octopai.com/pythony
/Users/zacayd@octopai.com/py2
/Users/zacayd@octopai.com/TEST
/Users/zacayd@octopai.com/ScalaHarverster
/Users/zacayd@octopai.com/MyTest
/Users/zacayd@octopai.com/Azure Blob Storage
/Users/zacayd@octopai.com/MySQL
/Users/zacayd@octopai.com/ExampleDLT
/Users/zacayd@octopai.com/ExampleDLT_SQL
/Users/zacayd@octopai.com/Untitled Notebook 2023-06-12 15:44:30
/Users/zacayd@octopai.com/MyLogs
/Users/zacayd@octopai.com/Helper
/Users/zacayd@octopai.com/Digicel_SandvineBilling
/Users/zacayd@octopai.com/Digicel
/Users/zacayd@octopai.com/Scalal
/Users/zacayd@octopai.com/2022-10-25 - DBFS Example
/Users/zacayd@octopai.com/pythonX
/Users/zacayd@octopai.com/Sql_PY
/Users/zacayd@octopai.com/ScalaPython
/Shared/Untitled Notebook 2023-05-23 09:28:27
/Shared/DL_BULKSMS_JAM
/Shared/AAP_CLIENT
/Shared/Sandvine_BillingRecon
/Shared/SELFCARE_USAGE
/Shared/ShakeIT_WonReward
/Shared/ODS_Digicel_App
/Shared/ODS_Digicel_App_User
/Shared/ODS_Selfcare_Incremental
/Shared/ODS_ShakeITAttempt
/Shared/Data-Delivery/EDW/NB_CargaTbContratoVarejo
/Shared/Querying the Delta Live Tables event log
/Shared/DL_BULKSMS_JAM_TEST
/Formula1-Project-Solutions/Section-16-17-18/utils/1.prepare_for_incremental_load
/Formula1-Project-Solutions/Section-16-17-18/analysis/3.viz_dominant_drivers
/Formula1-Project-Solutions/Section-16-17-18/analysis/4.viz_dominant_teams
/Formula1-Project-Solutions/Section-16-17-18/analysis/2.find_dominant_teams
/Formula1-Project-Solutions/Section-16-17-18/analysis/1.find_dominant_drivers
/Formula1-Project-Solutions/Section-16-17-18/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/2.ingest_races_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/9.create_processed_database
/Formula1-Project-Solutions/Section-16-17-18/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/5.ingest_results_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions/Section-16-17-18/ingestion/0.ingest_all_files
/Formula1-Project-Solutions/Section-16-17-18/raw/1.create_raw_tables
/Formula1-Project-Solutions/Section-14/trans/3.constructor_standings
/Formula1-Project-Solutions/Section-14/trans/2.driver_standings
/Formula1-Project-Solutions/Section-14/trans/1.race_results
/Formula1-Project-Solutions/Section-12/ingestion/2.ingest_races_file
/Formula1-Project-Solutions/Section-12/ingestion/5.ingest_results_file
/Formula1-Project-Solutions/Section-12/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions/Section-12/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions/Section-12/ingestion/0.ingest_all_files
/Formula1-Project-Solutions/Section-12/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions/Section-12/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions/Section-12/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions/Section-12/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-6
/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-9
/Formula1-Project-Solutions/Section-09-10-11/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions/Section-09-10-11/ingestion/5.ingest_results_file
/Formula1-Project-Solutions/Section-09-10-11/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions/Section-09-10-11/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions/Section-09-10-11/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions/Section-09-10-11/ingestion/2.ingest_races_file
/Formula1-Project-Solutions/Section-09-10-11/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions/Section-09-10-11/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions/Section-19/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions/Section-19/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions/Section-19/ingestion/5.ingest_results_file
/Formula1-Project-Solutions/Section-19/ingestion/9.create_processed_database
/Formula1-Project-Solutions/Section-19/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions/Section-19/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions/Section-19/ingestion/0.ingest_all_files
/Formula1-Project-Solutions/Section-19/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions/Section-19/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions/Section-19/ingestion/2.ingest_races_file
/Formula1-Project-Solutions/Section-19/trans/4.calculated_race_results
/Formula1-Project-Solutions/Section-19/trans/0.create_presentation_database
/Formula1-Project-Solutions/Section-19/trans/3.constructor_standings
/Formula1-Project-Solutions/Section-19/trans/1.race_results
/Formula1-Project-Solutions/Section-19/trans/2.driver_standings
/Formula1-Project-Solutions/Section-19/includes/common_functions
/Formula1-Project-Solutions/Section-19/includes/configuration
/Formula1-Project-Solutions/Section-20/formula1/set-up/mount_adls_storage
/Formula1-Project-Solutions/Section-20/formula1/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions/Section-20/formula1/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions/Section-20/formula1/ingestion/5.ingest_results_file
/Formula1-Project-Solutions/Section-20/formula1/ingestion/2.ingest_races_file
/Formula1-Project-Solutions/Section-20/formula1/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions/Section-20/formula1/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions/Section-20/formula1/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions/Section-20/formula1/ingestion/9.create_processed_database
/Formula1-Project-Solutions/Section-20/formula1/ingestion/0.ingest_all_files
/Formula1-Project-Solutions/Section-20/formula1/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions/Section-20/formula1/utils/1.prepare_for_incremental_load
/Formula1-Project-Solutions/Section-20/formula1/trans/3.constructor_standings
/Formula1-Project-Solutions/Section-20/formula1/trans/4.calculated_race_results
/Formula1-Project-Solutions/Section-20/formula1/trans/1.race_results
/Formula1-Project-Solutions/Section-20/formula1/trans/0.create_presentation_database
/Formula1-Project-Solutions/Section-20/formula1/trans/2.driver_standings
/Formula1-Project-Solutions/Section-20/formula1/analysis/3.viz_dominant_drivers
/Formula1-Project-Solutions/Section-20/formula1/analysis/1.find_dominant_drivers
/Formula1-Project-Solutions/Section-20/formula1/analysis/2.find_dominant_teams
/Formula1-Project-Solutions/Section-20/formula1/analysis/4.viz_dominant_teams
/Formula1-Project-Solutions/Section-20/formula1/raw/1.create_raw_tables
/Formula1-Project-Solutions/Section-20/formula1/demo/8.sql_functions_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/5.sql_temp_view_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/6.sql_objects_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/2.join_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/9.sql_joins_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/4.sql_temp_view_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/3.aggregation_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/10.delta_lake_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/7.sql_basics_demo
/Formula1-Project-Solutions/Section-20/formula1/demo/1.filter_demo
/Formula1-Project-Solutions/Section-20/formula1/includes/configuration
/Formula1-Project-Solutions/Section-20/formula1/includes/common_functions
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/7.ingest_lap_times_file
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/2.ingest_races_file
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/5.ingest_results_file
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/9.create_processed_database
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/3.ingest_constructors_file
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/1.ingest_circuits_file
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/0.ingest_all_files
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/4.ingest_drivers_file
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/8.ingest_qualifying_file
/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/6.ingest_pit_stops_file
/Formula1-Project-Solutions/Z-End of Course/formula1/set-up/mount_adls_storage
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/6.sql_objects_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/7.sql_basics_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/8.sql_functions_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/4.sql_temp_view_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/3.aggregation_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/1.filter_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/2.join_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/5.sql_temp_view_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/9.sql_joins_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/demo/10.delta_lake_demo
/Formula1-Project-Solutions/Z-End of Course/formula1/includes/common_functions
/Formula1-Project-Solutions/Z-End of Course/formula1/includes/configuration
/Formula1-Project-Solutions/Z-End of Course/formula1/utils/1.prepare_for_incremental_load
/Formula1-Project-Solutions/Z-End of Course/formula1/raw/1.create_raw_tables
/Formula1-Project-Solutions/Z-End of Course/formula1/trans/0.create_presentation_database
/Formula1-Project-Solutions/Z-End of Course/formula1/trans/2.driver_standings
/Formula1-Project-Solutions/Z-End of Course/formula1/trans/1.race_results
/Formula1-Project-Solutions/Z-End of Course/formula1/trans/3.constructor_standings
/Formula1-Project-Solutions/Z-End of Course/formula1/trans/4.calculated_race_results
/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/3.viz_dominant_drivers
/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/1.find_dominant_drivers
/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/4.viz_dominant_teams
/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/2.find_dominant_teams
/Formula1-Project-Solutions/Querying the Delta Live Tables event log
/AAP_USER
/ShakeIT_Attempt
/ODS_Selfcare_Incremental
/ODS_ShakeITWonReward
/ODS_ShakeITWonReward (1)
Successfully wrote content to notebooks.json.
json file was created notebooks.json
number of notebooks 420
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

print("seee")

# COMMAND ----------

print("see1e")
/Formula1-Project-Solutions (1)/Section-16-17-18/utils/1.prepare_for_incremental_load
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/9.create_processed_database
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/0.ingest_all_files
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.lap_times")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/7.ingest_lap_times_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.qualifying")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/8.ingest_qualifying_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.pit_stops")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/6.ingest_pit_stops_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.drivers")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/4.ingest_drivers_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/1.ingest_circuits_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("parquet").saveAsTable("f1_processed.races")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/2.ingest_races_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.constructors")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/3.ingest_constructors_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-16-17-18/ingestion/5.ingest_results_file
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_raw;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for CSV files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create circuits table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.circuits;
CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,
circuitRef STRING,
name STRING,
location STRING,
country STRING,
lat DOUBLE,
lng DOUBLE,
alt INT,
url STRING
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/circuits.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.circuits;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create races table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.races;
CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT,
year INT,
round INT,
circuitId INT,
name STRING,
date DATE,
time STRING,
url STRING)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/races.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.races;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for JSON files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create constructors table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.constructors;
CREATE TABLE IF NOT EXISTS f1_raw.constructors(
constructorId INT,
constructorRef STRING,
name STRING,
nationality STRING,
url STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/constructors.json")

-- COMMAND ----------

SELECT * FROM f1_raw.constructors;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create drivers table
-- MAGIC * Single Line JSON
-- MAGIC * Complex structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.drivers;
CREATE TABLE IF NOT EXISTS f1_raw.drivers(
driverId INT,
driverRef STRING,
number INT,
code STRING,
name STRUCT<forename: STRING, surname: STRING>,
dob DATE,
nationality STRING,
url STRING)
USING json
OPTIONS (path "/mnt/formula1dl/raw/drivers.json")

-- COMMAND ----------

-- MAGIC %md ##### Create results table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.results;
CREATE TABLE IF NOT EXISTS f1_raw.results(
resultId INT,
raceId INT,
driverId INT,
constructorId INT,
number INT,grid INT,
position INT,
positionText STRING,
positionOrder INT,
points INT,
laps INT,
time STRING,
milliseconds INT,
fastestLap INT,
rank INT,
fastestLapTime STRING,
fastestLapSpeed FLOAT,
statusId STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/results.json")

-- COMMAND ----------

SELECT * FROM f1_raw.results

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create pit stops table
-- MAGIC * Multi Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.pit_stops;
CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(
driverId INT,
duration STRING,
lap INT,
milliseconds INT,
raceId INT,
stop INT,
time STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/pit_stops.json", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.pit_stops;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for list of files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Lap Times Table
-- MAGIC * CSV file
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.lap_times;
CREATE TABLE IF NOT EXISTS f1_raw.lap_times(
raceId INT,
driverId INT,
lap INT,
position INT,
time STRING,
milliseconds INT
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/lap_times")

-- COMMAND ----------

SELECT * FROM f1_raw.lap_times

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Qualifying Table
-- MAGIC * JSON file
-- MAGIC * MultiLine JSON
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.qualifying;
CREATE TABLE IF NOT EXISTS f1_raw.qualifying(
constructorId INT,
driverId INT,
number INT,
position INT,
q1 STRING,
q2 STRING,
q3 STRING,
qualifyId INT,
raceId INT)
USING json
OPTIONS (path "/mnt/formula1dl/raw/qualifying", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.qualifying

-- COMMAND ----------

DESC EXTENDED f1_raw.qualifying;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-16-17-18/raw/1.create_raw_tables
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2011
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/2.find_dominant_teams
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Report on Dominant Formula 1 Drivers </h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_drivers
AS
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) driver_rank
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/3.viz_dominant_drivers
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Report on Dominant Formula 1 Teams </h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_teams
AS
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) team_rank
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT * FROM v_dominant_teams;

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/4.viz_dominant_teams
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2010
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-16-17-18/analysis/1.find_dominant_drivers
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/1.ingest_circuits_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/lap_times")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/7.ingest_lap_times_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/drivers")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/4.ingest_drivers_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').parquet(f"{processed_folder_path}/results")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/5.ingest_results_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/qualifying")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/8.ingest_qualifying_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result
/Formula1-Project-Solutions (1)/Section-12/ingestion/0.ingest_all_files
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode('overwrite').partitionBy('race_year').parquet(f'{processed_folder_path}/races')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/2.ingest_races_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/constructors")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/3.ingest_constructors_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/pit_stops")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-12/ingestion/6.ingest_pit_stops_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

storage_account_name = "formula1dl"
client_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-id")
tenant_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-tenant-id")
client_secret        = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-secret")

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

mount_adls("presentation")

# COMMAND ----------

mount_adls("demo")

# COMMAND ----------

dbutils.fs.unmount("/mnt/formula1dl/demo")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/presentation")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/demo")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/set-up/mount_adls_storage
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Teams of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_teams
AS
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) team_rank
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT * FROM v_dominant_teams;

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/4.viz_dominant_teams
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Drivers of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_drivers
AS
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) driver_rank
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/3.viz_dominant_drivers
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2011
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/2.find_dominant_teams
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2010
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/analysis/1.find_dominant_drivers
'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_raw;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for CSV files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create circuits table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.circuits;
CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,
circuitRef STRING,
name STRING,
location STRING,
country STRING,
lat DOUBLE,
lng DOUBLE,
alt INT,
url STRING
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/circuits.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.circuits;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create races table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.races;
CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT,
year INT,
round INT,
circuitId INT,
name STRING,
date DATE,
time STRING,
url STRING)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/races.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.races;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for JSON files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create constructors table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.constructors;
CREATE TABLE IF NOT EXISTS f1_raw.constructors(
constructorId INT,
constructorRef STRING,
name STRING,
nationality STRING,
url STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/constructors.json")

-- COMMAND ----------

SELECT * FROM f1_raw.constructors;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create drivers table
-- MAGIC * Single Line JSON
-- MAGIC * Complex structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.drivers;
CREATE TABLE IF NOT EXISTS f1_raw.drivers(
driverId INT,
driverRef STRING,
number INT,
code STRING,
name STRUCT<forename: STRING, surname: STRING>,
dob DATE,
nationality STRING,
url STRING)
USING json
OPTIONS (path "/mnt/formula1dl/raw/drivers.json")

-- COMMAND ----------

-- MAGIC %md ##### Create results table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.results;
CREATE TABLE IF NOT EXISTS f1_raw.results(
resultId INT,
raceId INT,
driverId INT,
constructorId INT,
number INT,grid INT,
position INT,
positionText STRING,
positionOrder INT,
points INT,
laps INT,
time STRING,
milliseconds INT,
fastestLap INT,
rank INT,
fastestLapTime STRING,
fastestLapSpeed FLOAT,
statusId STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/results.json")

-- COMMAND ----------

SELECT * FROM f1_raw.results

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create pit stops table
-- MAGIC * Multi Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.pit_stops;
CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(
driverId INT,
duration STRING,
lap INT,
milliseconds INT,
raceId INT,
stop INT,
time STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/pit_stops.json", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.pit_stops;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for list of files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Lap Times Table
-- MAGIC * CSV file
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.lap_times;
CREATE TABLE IF NOT EXISTS f1_raw.lap_times(
raceId INT,
driverId INT,
lap INT,
position INT,
time STRING,
milliseconds INT
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/lap_times")

-- COMMAND ----------

SELECT * FROM f1_raw.lap_times

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Qualifying Table
-- MAGIC * JSON file
-- MAGIC * MultiLine JSON
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.qualifying;
CREATE TABLE IF NOT EXISTS f1_raw.qualifying(
constructorId INT,
driverId INT,
number INT,
position INT,
q1 STRING,
q2 STRING,
q3 STRING,
qualifyId INT,
raceId INT)
USING json
OPTIONS (path "/mnt/formula1dl/raw/qualifying", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.qualifying

-- COMMAND ----------

DESC EXTENDED f1_raw.qualifying;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/raw/1.create_raw_tables
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Drop all the tables

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_processed CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed";

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_presentation CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation";

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/utils/1.prepare_for_incremental_load
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Access dataframes using SQL
# MAGIC ##### Objectives
# MAGIC 1. Create temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

race_results_df.createOrReplaceTempView("v_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC FROM v_race_results
# MAGIC WHERE race_year = 2020

# COMMAND ----------

p_race_year = 2020

# COMMAND ----------

race_results_2019_df = spark.sql(f"SELECT * FROM v_race_results WHERE race_year = {p_race_year}")

# COMMAND ----------

display(race_results_2019_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Global Temporary Views
# MAGIC 1. Create global temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell
# MAGIC 4. Acesss the view from another notebook

# COMMAND ----------

race_results_df.createOrReplaceGlobalTempView("gv_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW TABLES IN global_temp;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------

spark.sql("SELECT * \
  FROM global_temp.gv_race_results").show()

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/4.sql_temp_view_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_presentation;

-- COMMAND ----------

DESC driver_standings

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2018
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2018

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2020
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2020;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Inner Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Left Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  LEFT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Right Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  RIGHT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Full Join
-- MAGIC

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  FULL JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Semi Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  SEMI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Anti Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  ANTI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC Cross Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  CROSS JOIN v_driver_standings_2020 d_2020

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/9.sql_joins_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Lesson Objectives
-- MAGIC 1. Spark SQL documentation
-- MAGIC 1. Create Database demo
-- MAGIC 1. Data tab in the UI
-- MAGIC 1. SHOW command
-- MAGIC 1. DESCRIBE command
-- MAGIC 1. Find the current database

-- COMMAND ----------

CREATE DATABASE demo;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS demo;

-- COMMAND ----------

SHOW databases;

-- COMMAND ----------

DESCRIBE DATABASE demo; 

-- COMMAND ----------

DESCRIBE DATABASE EXTENDED demo; 

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

USE demo;

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Managed Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create managed table using Python
-- MAGIC 1. Create managed table using SQL
-- MAGIC 1. Effect of dropping a managed table
-- MAGIC 1. Describe table 

-- COMMAND ----------

-- MAGIC %run "../includes/configuration"

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").saveAsTable("demo.race_results_python")

-- COMMAND ----------

USE demo;
SHOW TABLES;

-- COMMAND ----------

DESC EXTENDED race_results_python;

-- COMMAND ----------

SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

CREATE TABLE demo.race_results_sql
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

DESC EXTENDED demo.race_results_sql;

-- COMMAND ----------

DROP TABLE demo.race_results_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### External Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create external table using Python
-- MAGIC 1. Create external table using SQL
-- MAGIC 1. Effect of dropping an external table

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").option("path", f"{presentation_folder_path}/race_results_ext_py").saveAsTable("demo.race_results_ext_py")

-- COMMAND ----------

DESC EXTENDED demo.race_results_ext_py

-- COMMAND ----------

CREATE TABLE demo.race_results_ext_sql
(race_year INT,
race_name STRING,
race_date TIMESTAMP,
circuit_location STRING,
driver_name STRING,
driver_number INT,
driver_nationality STRING,
team STRING,
grid INT,
fastest_lap INT,
race_time STRING,
points FLOAT,
position INT,
created_date TIMESTAMP
)
USING parquet
LOCATION "/mnt/formula1dl/presentation/race_results_ext_sql"

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

INSERT INTO demo.race_results_ext_sql
SELECT * FROM demo.race_results_ext_py WHERE race_year = 2020;

-- COMMAND ----------

SELECT COUNT(1) FROM demo.race_results_ext_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

DROP TABLE demo.race_results_ext_sql

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Views on tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create Temp View
-- MAGIC 1. Create Global Temp View
-- MAGIC 1. Create Permanent View

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_race_results;

-- COMMAND ----------

CREATE OR REPLACE GLOBAL TEMP VIEW gv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2012;

-- COMMAND ----------

SELECT * FROM global_temp.gv_race_results

-- COMMAND ----------

SHOW TABLES IN global_temp;

-- COMMAND ----------

CREATE OR REPLACE VIEW demo.pv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2000;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

SELECT * FROM demo.pv_race_results;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/6.sql_objects_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM v_race_results

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/5.sql_temp_view_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC #### Aggregate functions demo

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Built-in Aggregate functions

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

display(race_results_df)

# COMMAND ----------

demo_df = race_results_df.filter("race_year=2020")

# COMMAND ----------

display(demo_df)

# COMMAND ----------

from pyspark.sql.functions import count, countDistinct, sum

# COMMAND ----------

demo_df.select(count("*")).show()

# COMMAND ----------

demo_df.select(count("race_name")).show()

# COMMAND ----------

demo_df.select(countDistinct("race_name")).show()

# COMMAND ----------

demo_df.select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points"), countDistinct("race_name")) \
.withColumnRenamed("sum(points)", "total_points") \
.withColumnRenamed("count(DISTINCT race_name)", "number_of_races") \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### groupBy

# COMMAND ----------

demo_df\
.groupBy("driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Window Functions

# COMMAND ----------

demo_df = race_results_df.filter("race_year in (2019, 2020)")

# COMMAND ----------

demo_grouped_df = demo_df\
.groupBy("race_year", "driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) 

# COMMAND ----------

display(demo_grouped_df)

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank

driverRankSpec = Window.partitionBy("race_year").orderBy(desc("total_points"))
demo_grouped_df.withColumn("rank", rank().over(driverRankSpec)).show(100)

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/3.aggregation_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SHOW DATABASES;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SELECT *
 FROM drivers;

-- COMMAND ----------

DESC drivers;

-- COMMAND ----------

SELECT *
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob AS date_of_birth
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob 
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01'
ORDER BY dob DESC;

-- COMMAND ----------

SELECT *
  FROM drivers
 ORDER BY nationality ASC,
          dob DESC;

-- COMMAND ----------

SELECT name, nationality,dob 
 FROM drivers
WHERE (nationality = 'British'
  AND dob >= '1990-01-01') 
   OR nationality = 'Indian'
ORDER BY dob DESC;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/7.sql_basics_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Spark Join Transformation

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.filter("circuit_id < 70") \
.withColumnRenamed("name", "circuit_name")

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races").filter("race_year = 2019") \
.withColumnRenamed("name", "race_name")

# COMMAND ----------

display(circuits_df)

# COMMAND ----------

display(races_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Inner Join

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "inner") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Outer Joins

# COMMAND ----------

# Left Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "left") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# right Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "right") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# full Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "full") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Semi Joins

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "semi") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Anti Joins

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, circuits_df.circuit_id == races_df.circuit_id, "anti") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Cross Joins

# COMMAND ----------

race_circuits_df = races_df.crossJoin(circuits_df)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

race_circuits_df.count()

# COMMAND ----------

int(races_df.count()) * int(circuits_df.count())

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/2.join_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SELECT *, CONCAT(driver_ref, '-', code) AS new_driver_ref
  FROM drivers

-- COMMAND ----------

SELECT *, SPLIT(name, ' ')[0] forename, SPLIT(name, ' ')[1] surname
  FROM drivers

-- COMMAND ----------

SELECT *, current_timestamp
  FROM drivers

-- COMMAND ----------

SELECT *, date_format(dob, 'dd-MM-yyyy')
  FROM drivers

-- COMMAND ----------

SELECT *, date_add(dob, 1)
  FROM drivers

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers;

-- COMMAND ----------

SELECT MAX(dob) 
  FROM drivers;

-- COMMAND ----------

SELECT * FROM drivers WHERE dob = '2000-05-11'

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers
 WHERE nationality = 'British' ;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 HAVING COUNT(*) > 100
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, name, dob, RANK() OVER(PARTITION BY nationality ORDER BY dob DESC) AS age_rank
  FROM drivers
 ORDER BY nationality, age_rank

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/8.sql_functions_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC 1. Write data to delta lake (managed table)
# MAGIC 2. Write data to delta lake (external table)
# MAGIC 3. Read data from delta lake (Table)
# MAGIC 4. Read data from delta lake (File)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE DATABASE IF NOT EXISTS f1_demo
# MAGIC LOCATION '/mnt/formula1dl/demo'

# COMMAND ----------

results_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/results.json")

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").saveAsTable("f1_demo.results_managed")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").save("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE f1_demo.results_external
# MAGIC USING DELTA
# MAGIC LOCATION '/mnt/formula1dl/demo/results_external'

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_external

# COMMAND ----------

results_external_df = spark.read.format("delta").load("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

display(results_external_df)

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").partitionBy("constructorId").saveAsTable("f1_demo.results_partitioned")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW PARTITIONS f1_demo.results_partitioned

# COMMAND ----------

# MAGIC %md
# MAGIC 1. Update Delta Table
# MAGIC 2. Delete From Delta Table

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC UPDATE f1_demo.results_managed
# MAGIC   SET points = 11 - position
# MAGIC WHERE position <= 10

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.update("position <= 10", { "points": "21 - position" } ) 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.results_managed
# MAGIC WHERE position > 10;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.delete("points = 0") 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %md
# MAGIC Upsert using merge

# COMMAND ----------

drivers_day1_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId <= 10") \
.select("driverId", "dob", "name.forename", "name.surname")

# COMMAND ----------

display(drivers_day1_df)

# COMMAND ----------

drivers_day1_df.createOrReplaceTempView("drivers_day1")

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day2_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 6 AND 15") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

drivers_day2_df.createOrReplaceTempView("drivers_day2")

# COMMAND ----------

display(drivers_day2_df)

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day3_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 1 AND 5 OR driverId BETWEEN 16 AND 20") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_merge (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %md Day1

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day1 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 2

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day2 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 3

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/drivers_merge")

deltaTable.alias("tgt").merge(
    drivers_day3_df.alias("upd"),
    "tgt.driverId = upd.driverId") \
  .whenMatchedUpdate(set = { "dob" : "upd.dob", "forename" : "upd.forename", "surname" : "upd.surname", "updatedDate": "current_timestamp()" } ) \
  .whenNotMatchedInsert(values =
    {
      "driverId": "upd.driverId",
      "dob": "upd.dob",
      "forename" : "upd.forename", 
      "surname" : "upd.surname", 
      "createdDate": "current_timestamp()"
    }
  ) \
  .execute()

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC 1. History & Versioning
# MAGIC 2. Time Travel
# MAGIC 3. Vaccum

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

df = spark.read.format("delta").option("timestampAsOf", '2021-06-23T15:40:33.000+0000').load("/mnt/formula1dl/demo/drivers_merge")

# COMMAND ----------

display(df)

# COMMAND ----------

# MAGIC %sql
# MAGIC VACUUM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SET spark.databricks.delta.retentionDurationCheck.enabled = false;
# MAGIC VACUUM f1_demo.drivers_merge RETAIN 0 HOURS

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.drivers_merge WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql 
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 3;

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING f1_demo.drivers_merge VERSION AS OF 3 src
# MAGIC    ON (tgt.driverId = src.driverId)
# MAGIC WHEN NOT MATCHED THEN
# MAGIC    INSERT *

# COMMAND ----------

# MAGIC %sql DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %md
# MAGIC Transaction Logs

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_txn (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM  f1_demo.drivers_txn
# MAGIC WHERE driverId = 1;

# COMMAND ----------

for driver_id in range(3, 20):
  spark.sql(f"""INSERT INTO f1_demo.drivers_txn
                SELECT * FROM f1_demo.drivers_merge
                WHERE driverId = {driver_id}""")

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Convert Parquet to Delta

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_convert_to_delta (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING PARQUET

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_convert_to_delta
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA f1_demo.drivers_convert_to_delta

# COMMAND ----------

df = spark.table("f1_demo.drivers_convert_to_delta")

# COMMAND ----------

df.write.format("parquet").save("/mnt/formula1dl/demo/drivers_convert_to_delta_new")

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA parquet.`/mnt/formula1dl/demo/drivers_convert_to_delta_new`

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/10.delta_lake_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races")

# COMMAND ----------

races_filtered_df = races_df.filter("race_year = 2019 and round <= 5")

# COMMAND ----------

races_filtered_df = races_df.where((races_df["race_year"] == 2019) & (races_df["round"] <= 5))

# COMMAND ----------

display(races_filtered_df)

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/demo/1.filter_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
def add_ingestion_date(input_df):
  output_df = input_df.withColumn("ingestion_date", current_timestamp())
  return output_df

# COMMAND ----------

def re_arrange_partition_column(input_df, partition_column):
  column_list = []
  for column_name in input_df.schema.names:
    if column_name != partition_column:
      column_list.append(column_name)
  column_list.append(partition_column)
  output_df = input_df.select(column_list)
  return output_df

# COMMAND ----------

def overwrite_partition(input_df, db_name, table_name, partition_column):
  output_df = re_arrange_partition_column(input_df, partition_column)
  spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    output_df.write.mode("overwrite").insertInto(f"{db_name}.{table_name}")
  else:
    output_df.write.mode("overwrite").partitionBy(partition_column).format("parquet").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------

def df_column_to_list(input_df, column_name):
  df_row_list = input_df.select(column_name) \
                        .distinct() \
                        .collect()
  
  column_value_list = [row[column_name] for row in df_row_list]
  return column_value_list

# COMMAND ----------

def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):
  spark.conf.set("spark.databricks.optimizer.dynamicPartitionPruning","true")

  from delta.tables import DeltaTable
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    deltaTable = DeltaTable.forPath(spark, f"{folder_path}/{table_name}")
    deltaTable.alias("tgt").merge(
        input_df.alias("src"),
        merge_condition) \
      .whenMatchedUpdateAll()\
      .whenNotMatchedInsertAll()\
      .execute()
  else:
    input_df.write.mode("overwrite").partitionBy(partition_column).format("delta").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/includes/common_functions
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

raw_folder_path = "/mnt/formula1dl/raw"
processed_folder_path = "/mnt/formula1dl/processed"
presentation_folder_path = "/mnt/formula1dl/presentation"

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/includes/configuration
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

drivers_df = spark.read.format("delta").load(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.format("delta").load(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.format("delta").load(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.format("delta").load(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.format("delta").load(f"{processed_folder_path}/results") \
.filter(f"file_date = '{v_file_date}'") \
.withColumnRenamed("time", "race_time") \
.withColumnRenamed("race_id", "result_race_id") \
.withColumnRenamed("file_date", "result_file_date") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.result_race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_id", "race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position", "result_file_date") \
                          .withColumn("created_date", current_timestamp()) \
                          .withColumnRenamed("result_file_date", "file_date") 

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_presentation', 'race_results', presentation_folder_path, merge_condition, 'race_id')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.race_results;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/trans/1.race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

spark.sql(f"""
              CREATE TABLE IF NOT EXISTS f1_presentation.calculated_race_results
              (
              race_year INT,
              team_name STRING,
              driver_id INT,
              driver_name STRING,
              race_id INT,
              position INT,
              points INT,
              calculated_points INT,
              created_date TIMESTAMP,
              updated_date TIMESTAMP
              )
              USING DELTA
""")

# COMMAND ----------

spark.sql(f"""
              CREATE OR REPLACE TEMP VIEW race_result_updated
              AS
              SELECT races.race_year,
                     constructors.name AS team_name,
                     drivers.driver_id,
                     drivers.name AS driver_name,
                     races.race_id,
                     results.position,
                     results.points,
                     11 - results.position AS calculated_points
                FROM f1_processed.results 
                JOIN f1_processed.drivers ON (results.driver_id = drivers.driver_id)
                JOIN f1_processed.constructors ON (results.constructor_id = constructors.constructor_id)
                JOIN f1_processed.races ON (results.race_id = races.race_id)
               WHERE results.position <= 10
                 AND results.file_date = '{v_file_date}'
""")

# COMMAND ----------

spark.sql(f"""
              MERGE INTO f1_presentation.calculated_race_results tgt
              USING race_result_updated upd
              ON (tgt.driver_id = upd.driver_id AND tgt.race_id = upd.race_id)
              WHEN MATCHED THEN
                UPDATE SET tgt.position = upd.position,
                           tgt.points = upd.points,
                           tgt.calculated_points = upd.calculated_points,
                           tgt.updated_date = current_timestamp
              WHEN NOT MATCHED
                THEN INSERT (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, created_date ) 
                     VALUES (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, current_timestamp)
       """)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM race_result_updated;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM f1_presentation.calculated_race_results;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/trans/4.calculated_race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce constructor standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

merge_condition = "tgt.team = src.team AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'constructor_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.constructor_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.constructor_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;
/Formula1-Project-Solutions (1)/Section-20/formula1/trans/3.constructor_standings
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation"

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/trans/0.create_presentation_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'driver_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.driver_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.driver_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/trans/2.driver_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'lap_times', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.lap = src.lap AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'lap_times', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/7.ingest_lap_times_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'pit_stops', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.stop = src.stop AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'pit_stops', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.pit_stops;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/{v_file_date}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source)) \
                                             .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.constructors")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.constructors;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/{v_file_date}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.drivers")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.drivers

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.circuits")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/1.ingest_circuits_file
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/9.create_processed_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("delta").saveAsTable("f1_processed.races")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.races;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/0.ingest_all_files
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/{v_file_date}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC De-dupe the dataframe

# COMMAND ----------

results_deduped_df = results_final_df.dropDuplicates(['race_id', 'driver_id'])

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

# MAGIC %md
# MAGIC Method 1

# COMMAND ----------

# for race_id_list in results_final_df.select("race_id").distinct().collect():
#   if (spark._jsparkSession.catalog().tableExists("f1_processed.results")):
#     spark.sql(f"ALTER TABLE f1_processed.results DROP IF EXISTS PARTITION (race_id = {race_id_list.race_id})")

# COMMAND ----------

# results_final_df.write.mode("append").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

# MAGIC %md
# MAGIC Method 2

# COMMAND ----------

# overwrite_partition(results_final_df, 'f1_processed', 'results', 'race_id')

# COMMAND ----------

merge_condition = "tgt.result_id = src.result_id AND tgt.race_id = src.race_id"
merge_delta_data(results_deduped_df, 'f1_processed', 'results', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC   FROM f1_processed.results;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_id, driver_id, COUNT(1) 
# MAGIC FROM f1_processed.results
# MAGIC GROUP BY race_id, driver_id
# MAGIC HAVING COUNT(1) > 1
# MAGIC ORDER BY race_id, driver_id DESC;

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_processed.results WHERE race_id = 540 AND driver_id = 229;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/5.ingest_results_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'qualifying', 'race_id')

# COMMAND ----------

merge_condition = "tgt.qualify_id = src.qualify_id AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'qualifying', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-20/formula1/ingestion/8.ingest_qualifying_file
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Drivers of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_drivers
AS
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) driver_rank
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/3.viz_dominant_drivers
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2011
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/2.find_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Teams of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_teams
AS
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) team_rank
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT * FROM v_dominant_teams;

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/4.viz_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2010
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/analysis/1.find_dominant_drivers
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

storage_account_name = "formula1dl"
client_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-id")
tenant_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-tenant-id")
client_secret        = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-secret")

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

mount_adls("presentation")

# COMMAND ----------

mount_adls("demo")

# COMMAND ----------

dbutils.fs.unmount("/mnt/formula1dl/demo")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/presentation")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/demo")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/set-up/mount_adls_storage
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/0.ingest_all_files
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'lap_times', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.lap = src.lap AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'lap_times', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/7.ingest_lap_times_file
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/9.create_processed_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/{v_file_date}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.drivers")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.drivers

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/{v_file_date}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC De-dupe the dataframe

# COMMAND ----------

results_deduped_df = results_final_df.dropDuplicates(['race_id', 'driver_id'])

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

# MAGIC %md
# MAGIC Method 1

# COMMAND ----------

# for race_id_list in results_final_df.select("race_id").distinct().collect():
#   if (spark._jsparkSession.catalog().tableExists("f1_processed.results")):
#     spark.sql(f"ALTER TABLE f1_processed.results DROP IF EXISTS PARTITION (race_id = {race_id_list.race_id})")

# COMMAND ----------

# results_final_df.write.mode("append").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

# MAGIC %md
# MAGIC Method 2

# COMMAND ----------

# overwrite_partition(results_final_df, 'f1_processed', 'results', 'race_id')

# COMMAND ----------

merge_condition = "tgt.result_id = src.result_id AND tgt.race_id = src.race_id"
merge_delta_data(results_deduped_df, 'f1_processed', 'results', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC   FROM f1_processed.results;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_id, driver_id, COUNT(1) 
# MAGIC FROM f1_processed.results
# MAGIC GROUP BY race_id, driver_id
# MAGIC HAVING COUNT(1) > 1
# MAGIC ORDER BY race_id, driver_id DESC;

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_processed.results WHERE race_id = 540 AND driver_id = 229;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/5.ingest_results_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'qualifying', 'race_id')

# COMMAND ----------

merge_condition = "tgt.qualify_id = src.qualify_id AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'qualifying', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/8.ingest_qualifying_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/{v_file_date}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source)) \
                                             .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.constructors")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.constructors;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("delta").saveAsTable("f1_processed.races")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.races;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'pit_stops', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.stop = src.stop AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'pit_stops', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.pit_stops;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.circuits")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/ingestion/1.ingest_circuits_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce constructor standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

merge_condition = "tgt.team = src.team AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'constructor_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.constructor_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.constructor_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;
/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/3.constructor_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'driver_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.driver_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.driver_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/2.driver_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

drivers_df = spark.read.format("delta").load(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.format("delta").load(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.format("delta").load(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.format("delta").load(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.format("delta").load(f"{processed_folder_path}/results") \
.filter(f"file_date = '{v_file_date}'") \
.withColumnRenamed("time", "race_time") \
.withColumnRenamed("race_id", "result_race_id") \
.withColumnRenamed("file_date", "result_file_date") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.result_race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_id", "race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position", "result_file_date") \
                          .withColumn("created_date", current_timestamp()) \
                          .withColumnRenamed("result_file_date", "file_date") 

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_presentation', 'race_results', presentation_folder_path, merge_condition, 'race_id')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.race_results;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/1.race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

spark.sql(f"""
              CREATE TABLE IF NOT EXISTS f1_presentation.calculated_race_results
              (
              race_year INT,
              team_name STRING,
              driver_id INT,
              driver_name STRING,
              race_id INT,
              position INT,
              points INT,
              calculated_points INT,
              created_date TIMESTAMP,
              updated_date TIMESTAMP
              )
              USING DELTA
""")

# COMMAND ----------

spark.sql(f"""
              CREATE OR REPLACE TEMP VIEW race_result_updated
              AS
              SELECT races.race_year,
                     constructors.name AS team_name,
                     drivers.driver_id,
                     drivers.name AS driver_name,
                     races.race_id,
                     results.position,
                     results.points,
                     11 - results.position AS calculated_points
                FROM f1_processed.results 
                JOIN f1_processed.drivers ON (results.driver_id = drivers.driver_id)
                JOIN f1_processed.constructors ON (results.constructor_id = constructors.constructor_id)
                JOIN f1_processed.races ON (results.race_id = races.race_id)
               WHERE results.position <= 10
                 AND results.file_date = '{v_file_date}'
""")

# COMMAND ----------

spark.sql(f"""
              MERGE INTO f1_presentation.calculated_race_results tgt
              USING race_result_updated upd
              ON (tgt.driver_id = upd.driver_id AND tgt.race_id = upd.race_id)
              WHEN MATCHED THEN
                UPDATE SET tgt.position = upd.position,
                           tgt.points = upd.points,
                           tgt.calculated_points = upd.calculated_points,
                           tgt.updated_date = current_timestamp
              WHEN NOT MATCHED
                THEN INSERT (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, created_date ) 
                     VALUES (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, current_timestamp)
       """)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM race_result_updated;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM f1_presentation.calculated_race_results;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/4.calculated_race_results
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation"

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/trans/0.create_presentation_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Access dataframes using SQL
# MAGIC ##### Objectives
# MAGIC 1. Create temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

race_results_df.createOrReplaceTempView("v_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC FROM v_race_results
# MAGIC WHERE race_year = 2020

# COMMAND ----------

p_race_year = 2020

# COMMAND ----------

race_results_2019_df = spark.sql(f"SELECT * FROM v_race_results WHERE race_year = {p_race_year}")

# COMMAND ----------

display(race_results_2019_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Global Temporary Views
# MAGIC 1. Create global temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell
# MAGIC 4. Acesss the view from another notebook

# COMMAND ----------

race_results_df.createOrReplaceGlobalTempView("gv_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW TABLES IN global_temp;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------

spark.sql("SELECT * \
  FROM global_temp.gv_race_results").show()

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/4.sql_temp_view_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SELECT *, CONCAT(driver_ref, '-', code) AS new_driver_ref
  FROM drivers

-- COMMAND ----------

SELECT *, SPLIT(name, ' ')[0] forename, SPLIT(name, ' ')[1] surname
  FROM drivers

-- COMMAND ----------

SELECT *, current_timestamp
  FROM drivers

-- COMMAND ----------

SELECT *, date_format(dob, 'dd-MM-yyyy')
  FROM drivers

-- COMMAND ----------

SELECT *, date_add(dob, 1)
  FROM drivers

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers;

-- COMMAND ----------

SELECT MAX(dob) 
  FROM drivers;

-- COMMAND ----------

SELECT * FROM drivers WHERE dob = '2000-05-11'

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers
 WHERE nationality = 'British' ;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 HAVING COUNT(*) > 100
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, name, dob, RANK() OVER(PARTITION BY nationality ORDER BY dob DESC) AS age_rank
  FROM drivers
 ORDER BY nationality, age_rank

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/8.sql_functions_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC 1. Write data to delta lake (managed table)
# MAGIC 2. Write data to delta lake (external table)
# MAGIC 3. Read data from delta lake (Table)
# MAGIC 4. Read data from delta lake (File)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE DATABASE IF NOT EXISTS f1_demo
# MAGIC LOCATION '/mnt/formula1dl/demo'

# COMMAND ----------

results_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/results.json")

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").saveAsTable("f1_demo.results_managed")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").save("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE f1_demo.results_external
# MAGIC USING DELTA
# MAGIC LOCATION '/mnt/formula1dl/demo/results_external'

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_external

# COMMAND ----------

results_external_df = spark.read.format("delta").load("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

display(results_external_df)

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").partitionBy("constructorId").saveAsTable("f1_demo.results_partitioned")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW PARTITIONS f1_demo.results_partitioned

# COMMAND ----------

# MAGIC %md
# MAGIC 1. Update Delta Table
# MAGIC 2. Delete From Delta Table

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC UPDATE f1_demo.results_managed
# MAGIC   SET points = 11 - position
# MAGIC WHERE position <= 10

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.update("position <= 10", { "points": "21 - position" } ) 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.results_managed
# MAGIC WHERE position > 10;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.delete("points = 0") 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %md
# MAGIC Upsert using merge

# COMMAND ----------

drivers_day1_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId <= 10") \
.select("driverId", "dob", "name.forename", "name.surname")

# COMMAND ----------

display(drivers_day1_df)

# COMMAND ----------

drivers_day1_df.createOrReplaceTempView("drivers_day1")

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day2_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 6 AND 15") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

drivers_day2_df.createOrReplaceTempView("drivers_day2")

# COMMAND ----------

display(drivers_day2_df)

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day3_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 1 AND 5 OR driverId BETWEEN 16 AND 20") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_merge (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %md Day1

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day1 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 2

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day2 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 3

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/drivers_merge")

deltaTable.alias("tgt").merge(
    drivers_day3_df.alias("upd"),
    "tgt.driverId = upd.driverId") \
  .whenMatchedUpdate(set = { "dob" : "upd.dob", "forename" : "upd.forename", "surname" : "upd.surname", "updatedDate": "current_timestamp()" } ) \
  .whenNotMatchedInsert(values =
    {
      "driverId": "upd.driverId",
      "dob": "upd.dob",
      "forename" : "upd.forename", 
      "surname" : "upd.surname", 
      "createdDate": "current_timestamp()"
    }
  ) \
  .execute()

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC 1. History & Versioning
# MAGIC 2. Time Travel
# MAGIC 3. Vaccum

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

df = spark.read.format("delta").option("timestampAsOf", '2021-06-23T15:40:33.000+0000').load("/mnt/formula1dl/demo/drivers_merge")

# COMMAND ----------

display(df)

# COMMAND ----------

# MAGIC %sql
# MAGIC VACUUM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SET spark.databricks.delta.retentionDurationCheck.enabled = false;
# MAGIC VACUUM f1_demo.drivers_merge RETAIN 0 HOURS

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.drivers_merge WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql 
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 3;

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING f1_demo.drivers_merge VERSION AS OF 3 src
# MAGIC    ON (tgt.driverId = src.driverId)
# MAGIC WHEN NOT MATCHED THEN
# MAGIC    INSERT *

# COMMAND ----------

# MAGIC %sql DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %md
# MAGIC Transaction Logs

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_txn (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM  f1_demo.drivers_txn
# MAGIC WHERE driverId = 1;

# COMMAND ----------

for driver_id in range(3, 20):
  spark.sql(f"""INSERT INTO f1_demo.drivers_txn
                SELECT * FROM f1_demo.drivers_merge
                WHERE driverId = {driver_id}""")

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Convert Parquet to Delta

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_convert_to_delta (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING PARQUET

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_convert_to_delta
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA f1_demo.drivers_convert_to_delta

# COMMAND ----------

df = spark.table("f1_demo.drivers_convert_to_delta")

# COMMAND ----------

df.write.format("parquet").save("/mnt/formula1dl/demo/drivers_convert_to_delta_new")

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA parquet.`/mnt/formula1dl/demo/drivers_convert_to_delta_new`

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/10.delta_lake_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_presentation;

-- COMMAND ----------

DESC driver_standings

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2018
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2018

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2020
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2020;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Inner Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Left Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  LEFT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Right Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  RIGHT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Full Join
-- MAGIC

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  FULL JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Semi Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  SEMI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Anti Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  ANTI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC Cross Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  CROSS JOIN v_driver_standings_2020 d_2020

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/9.sql_joins_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races")

# COMMAND ----------

races_filtered_df = races_df.filter("race_year = 2019 and round <= 5")

# COMMAND ----------

races_filtered_df = races_df.where((races_df["race_year"] == 2019) & (races_df["round"] <= 5))

# COMMAND ----------

display(races_filtered_df)

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/1.filter_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM v_race_results

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/5.sql_temp_view_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Lesson Objectives
-- MAGIC 1. Spark SQL documentation
-- MAGIC 1. Create Database demo
-- MAGIC 1. Data tab in the UI
-- MAGIC 1. SHOW command
-- MAGIC 1. DESCRIBE command
-- MAGIC 1. Find the current database

-- COMMAND ----------

CREATE DATABASE demo;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS demo;

-- COMMAND ----------

SHOW databases;

-- COMMAND ----------

DESCRIBE DATABASE demo; 

-- COMMAND ----------

DESCRIBE DATABASE EXTENDED demo; 

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

USE demo;

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Managed Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create managed table using Python
-- MAGIC 1. Create managed table using SQL
-- MAGIC 1. Effect of dropping a managed table
-- MAGIC 1. Describe table 

-- COMMAND ----------

-- MAGIC %run "../includes/configuration"

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").saveAsTable("demo.race_results_python")

-- COMMAND ----------

USE demo;
SHOW TABLES;

-- COMMAND ----------

DESC EXTENDED race_results_python;

-- COMMAND ----------

SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

CREATE TABLE demo.race_results_sql
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

DESC EXTENDED demo.race_results_sql;

-- COMMAND ----------

DROP TABLE demo.race_results_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### External Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create external table using Python
-- MAGIC 1. Create external table using SQL
-- MAGIC 1. Effect of dropping an external table

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").option("path", f"{presentation_folder_path}/race_results_ext_py").saveAsTable("demo.race_results_ext_py")

-- COMMAND ----------

DESC EXTENDED demo.race_results_ext_py

-- COMMAND ----------

CREATE TABLE demo.race_results_ext_sql
(race_year INT,
race_name STRING,
race_date TIMESTAMP,
circuit_location STRING,
driver_name STRING,
driver_number INT,
driver_nationality STRING,
team STRING,
grid INT,
fastest_lap INT,
race_time STRING,
points FLOAT,
position INT,
created_date TIMESTAMP
)
USING parquet
LOCATION "/mnt/formula1dl/presentation/race_results_ext_sql"

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

INSERT INTO demo.race_results_ext_sql
SELECT * FROM demo.race_results_ext_py WHERE race_year = 2020;

-- COMMAND ----------

SELECT COUNT(1) FROM demo.race_results_ext_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

DROP TABLE demo.race_results_ext_sql

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Views on tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create Temp View
-- MAGIC 1. Create Global Temp View
-- MAGIC 1. Create Permanent View

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_race_results;

-- COMMAND ----------

CREATE OR REPLACE GLOBAL TEMP VIEW gv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2012;

-- COMMAND ----------

SELECT * FROM global_temp.gv_race_results

-- COMMAND ----------

SHOW TABLES IN global_temp;

-- COMMAND ----------

CREATE OR REPLACE VIEW demo.pv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2000;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

SELECT * FROM demo.pv_race_results;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/6.sql_objects_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Spark Join Transformation

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.filter("circuit_id < 70") \
.withColumnRenamed("name", "circuit_name")

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races").filter("race_year = 2019") \
.withColumnRenamed("name", "race_name")

# COMMAND ----------

display(circuits_df)

# COMMAND ----------

display(races_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Inner Join

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "inner") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Outer Joins

# COMMAND ----------

# Left Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "left") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# right Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "right") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# full Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "full") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Semi Joins

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "semi") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Anti Joins

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, circuits_df.circuit_id == races_df.circuit_id, "anti") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Cross Joins

# COMMAND ----------

race_circuits_df = races_df.crossJoin(circuits_df)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

race_circuits_df.count()

# COMMAND ----------

int(races_df.count()) * int(circuits_df.count())

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/2.join_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC #### Aggregate functions demo

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Built-in Aggregate functions

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

display(race_results_df)

# COMMAND ----------

demo_df = race_results_df.filter("race_year=2020")

# COMMAND ----------

display(demo_df)

# COMMAND ----------

from pyspark.sql.functions import count, countDistinct, sum

# COMMAND ----------

demo_df.select(count("*")).show()

# COMMAND ----------

demo_df.select(count("race_name")).show()

# COMMAND ----------

demo_df.select(countDistinct("race_name")).show()

# COMMAND ----------

demo_df.select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points"), countDistinct("race_name")) \
.withColumnRenamed("sum(points)", "total_points") \
.withColumnRenamed("count(DISTINCT race_name)", "number_of_races") \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### groupBy

# COMMAND ----------

demo_df\
.groupBy("driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Window Functions

# COMMAND ----------

demo_df = race_results_df.filter("race_year in (2019, 2020)")

# COMMAND ----------

demo_grouped_df = demo_df\
.groupBy("race_year", "driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) 

# COMMAND ----------

display(demo_grouped_df)

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank

driverRankSpec = Window.partitionBy("race_year").orderBy(desc("total_points"))
demo_grouped_df.withColumn("rank", rank().over(driverRankSpec)).show(100)

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/3.aggregation_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SHOW DATABASES;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SELECT *
 FROM drivers;

-- COMMAND ----------

DESC drivers;

-- COMMAND ----------

SELECT *
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob AS date_of_birth
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob 
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01'
ORDER BY dob DESC;

-- COMMAND ----------

SELECT *
  FROM drivers
 ORDER BY nationality ASC,
          dob DESC;

-- COMMAND ----------

SELECT name, nationality,dob 
 FROM drivers
WHERE (nationality = 'British'
  AND dob >= '1990-01-01') 
   OR nationality = 'Indian'
ORDER BY dob DESC;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/demo/7.sql_basics_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
def add_ingestion_date(input_df):
  output_df = input_df.withColumn("ingestion_date", current_timestamp())
  return output_df

# COMMAND ----------

def re_arrange_partition_column(input_df, partition_column):
  column_list = []
  for column_name in input_df.schema.names:
    if column_name != partition_column:
      column_list.append(column_name)
  column_list.append(partition_column)
  output_df = input_df.select(column_list)
  return output_df

# COMMAND ----------

def overwrite_partition(input_df, db_name, table_name, partition_column):
  output_df = re_arrange_partition_column(input_df, partition_column)
  spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    output_df.write.mode("overwrite").insertInto(f"{db_name}.{table_name}")
  else:
    output_df.write.mode("overwrite").partitionBy(partition_column).format("parquet").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------

def df_column_to_list(input_df, column_name):
  df_row_list = input_df.select(column_name) \
                        .distinct() \
                        .collect()
  
  column_value_list = [row[column_name] for row in df_row_list]
  return column_value_list

# COMMAND ----------

def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):
  spark.conf.set("spark.databricks.optimizer.dynamicPartitionPruning","true")

  from delta.tables import DeltaTable
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    deltaTable = DeltaTable.forPath(spark, f"{folder_path}/{table_name}")
    deltaTable.alias("tgt").merge(
        input_df.alias("src"),
        merge_condition) \
      .whenMatchedUpdateAll()\
      .whenNotMatchedInsertAll()\
      .execute()
  else:
    input_df.write.mode("overwrite").partitionBy(partition_column).format("delta").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/includes/common_functions
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

raw_folder_path = "/mnt/formula1dl/raw"
processed_folder_path = "/mnt/formula1dl/processed"
presentation_folder_path = "/mnt/formula1dl/presentation"

# COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/includes/configuration
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Drop all the tables

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_processed CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed";

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_presentation CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation";

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/utils/1.prepare_for_incremental_load
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_raw;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for CSV files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create circuits table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.circuits;
CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,
circuitRef STRING,
name STRING,
location STRING,
country STRING,
lat DOUBLE,
lng DOUBLE,
alt INT,
url STRING
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/circuits.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.circuits;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create races table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.races;
CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT,
year INT,
round INT,
circuitId INT,
name STRING,
date DATE,
time STRING,
url STRING)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/races.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.races;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for JSON files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create constructors table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.constructors;
CREATE TABLE IF NOT EXISTS f1_raw.constructors(
constructorId INT,
constructorRef STRING,
name STRING,
nationality STRING,
url STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/constructors.json")

-- COMMAND ----------

SELECT * FROM f1_raw.constructors;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create drivers table
-- MAGIC * Single Line JSON
-- MAGIC * Complex structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.drivers;
CREATE TABLE IF NOT EXISTS f1_raw.drivers(
driverId INT,
driverRef STRING,
number INT,
code STRING,
name STRUCT<forename: STRING, surname: STRING>,
dob DATE,
nationality STRING,
url STRING)
USING json
OPTIONS (path "/mnt/formula1dl/raw/drivers.json")

-- COMMAND ----------

-- MAGIC %md ##### Create results table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.results;
CREATE TABLE IF NOT EXISTS f1_raw.results(
resultId INT,
raceId INT,
driverId INT,
constructorId INT,
number INT,grid INT,
position INT,
positionText STRING,
positionOrder INT,
points INT,
laps INT,
time STRING,
milliseconds INT,
fastestLap INT,
rank INT,
fastestLapTime STRING,
fastestLapSpeed FLOAT,
statusId STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/results.json")

-- COMMAND ----------

SELECT * FROM f1_raw.results

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create pit stops table
-- MAGIC * Multi Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.pit_stops;
CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(
driverId INT,
duration STRING,
lap INT,
milliseconds INT,
raceId INT,
stop INT,
time STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/pit_stops.json", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.pit_stops;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for list of files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Lap Times Table
-- MAGIC * CSV file
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.lap_times;
CREATE TABLE IF NOT EXISTS f1_raw.lap_times(
raceId INT,
driverId INT,
lap INT,
position INT,
time STRING,
milliseconds INT
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/lap_times")

-- COMMAND ----------

SELECT * FROM f1_raw.lap_times

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Qualifying Table
-- MAGIC * JSON file
-- MAGIC * MultiLine JSON
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.qualifying;
CREATE TABLE IF NOT EXISTS f1_raw.qualifying(
constructorId INT,
driverId INT,
number INT,
position INT,
q1 STRING,
q2 STRING,
q3 STRING,
qualifyId INT,
raceId INT)
USING json
OPTIONS (path "/mnt/formula1dl/raw/qualifying", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.qualifying

-- COMMAND ----------

DESC EXTENDED f1_raw.qualifying;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Z-End of Course/formula1/raw/1.create_raw_tables
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

display(driver_standings_df.filter("race_year = 2020"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

display(final_df.filter("race_year = 2020"))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/driver_standings")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-14/trans/2.driver_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

drivers_df = spark.read.parquet(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.parquet(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.parquet(f"{processed_folder_path}/results") \
.withColumnRenamed("time", "race_time") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position") \
                          .withColumn("created_date", current_timestamp())

# COMMAND ----------

display(final_df.filter("race_year == 2020 and race_name == 'Abu Dhabi Grand Prix'").orderBy(final_df.points.desc()))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-14/trans/1.race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

display(constructor_standings_df.filter("race_year = 2020"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

display(final_df.filter("race_year = 2020"))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/constructor_standings")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-14/trans/3.constructor_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

drivers_df = spark.read.parquet(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.parquet(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.parquet(f"{processed_folder_path}/results") \
.filter(f"file_date = '{v_file_date}'") \
.withColumnRenamed("time", "race_time") \
.withColumnRenamed("race_id", "result_race_id") \
.withColumnRenamed("file_date", "result_file_date") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.result_race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_id", "race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position", "result_file_date") \
                          .withColumn("created_date", current_timestamp()) \
                          .withColumnRenamed("result_file_date", "file_date")

# COMMAND ----------

overwrite_partition(final_df, 'f1_presentation', 'race_results', 'race_id')

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/trans/1.race_results
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

CREATE TABLE f1_presentation.calculated_race_results
USING parquet
AS
SELECT races.race_year,
       constructors.name AS team_name,
       drivers.name AS driver_name,
       results.position,
       results.points,
       11 - results.position AS calculated_points
  FROM results 
  JOIN f1_processed.drivers ON (results.driver_id = drivers.driver_id)
  JOIN f1_processed.constructors ON (results.constructor_id = constructors.constructor_id)
  JOIN f1_processed.races ON (results.race_id = races.race_id)
 WHERE results.position <= 10

-- COMMAND ----------

SELECT * FROM f1_presentation.calculated_race_results

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/trans/4.calculated_race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce constructor standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

overwrite_partition(final_df, 'f1_presentation', 'constructor_standings', 'race_year')

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/trans/3.constructor_standings
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation"

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/trans/0.create_presentation_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

overwrite_partition(final_df, 'f1_presentation', 'driver_standings', 'race_year')

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/trans/2.driver_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
def add_ingestion_date(input_df):
  output_df = input_df.withColumn("ingestion_date", current_timestamp())
  return output_df_

# COMMAND ----------

def re_arrange_partition_column(input_df, partition_column):
  column_list = []
  for column_name in input_df.schema.names:
    if column_name != partition_column:
      column_list.append(column_name)
  column_list.append(partition_column)
  output_df = input_df.select(column_list)
  return output_df

# COMMAND ----------

def overwrite_partition(input_df, db_name, table_name, partition_column):
  output_df = re_arrange_partition_column(input_df, partition_column)
  spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    output_df.write.mode("overwrite").insertInto(f"{db_name}.{table_name}")
  else:
    output_df.write.mode("overwrite").partitionBy(partition_column).format("parquet").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------

def df_column_to_list(input_df, column_name):
  df_row_list = input_df.select(column_name) \
                        .distinct() \
                        .collect()
  
  column_value_list = [row[column_name] for row in df_row_list]
  return column_value_list

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/includes/common_functions
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

raw_folder_path = "/mnt/formula1dl/raw"
processed_folder_path = "/mnt/formula1dl/processed"
presentation_folder_path = "/mnt/formula1dl/presentation"

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/includes/configuration
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/ingestion/9.create_processed_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/{v_file_date}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source)) \
                                             .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.constructors")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.constructors;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-19/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/{v_file_date}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.drivers")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.drivers

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-19/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

overwrite_partition(final_df, 'f1_processed', 'pit_stops', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-19/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("parquet").saveAsTable("f1_processed.races")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.races;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-19/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

overwrite_partition(final_df, 'f1_processed', 'qualifying', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-19/ingestion/8.ingest_qualifying_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

overwrite_partition(final_df, 'f1_processed', 'lap_times', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-19/ingestion/7.ingest_lap_times_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions (1)/Section-19/ingestion/1.ingest_circuits_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/ingestion/0.ingest_all_files
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/{v_file_date}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

# MAGIC %md
# MAGIC Method 1

# COMMAND ----------

# for race_id_list in results_final_df.select("race_id").distinct().collect():
#   if (spark._jsparkSession.catalog().tableExists("f1_processed.results")):
#     spark.sql(f"ALTER TABLE f1_processed.results DROP IF EXISTS PARTITION (race_id = {race_id_list.race_id})")

# COMMAND ----------

# results_final_df.write.mode("append").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

# MAGIC %md
# MAGIC Method 2

# COMMAND ----------

overwrite_partition(results_final_df, 'f1_processed', 'results', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_id, COUNT(1) 
# MAGIC FROM f1_processed.results
# MAGIC GROUP BY race_id
# MAGIC ORDER BY race_id DESC;

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-19/ingestion/5.ingest_results_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

storage_account_name = "formula1dl"
client_id            = ""
tenant_id            = ""
client_secret        = ""

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

container_name = "raw"
dbutils.fs.mount(
  source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
  mount_point = f"/mnt/{storage_account_name}/{container_name}",
  extra_configs = configs)

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.mounts()

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-06/mount_adls_storage-lesson-6
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

storage_account_name = "formula1dl"
client_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-id")
tenant_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-tenant-id")
client_secret        = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-secret")

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-06/mount_adls_storage-lesson-9
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv("/mnt/formula1dl/raw/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = lap_times_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/lap_times")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/7.ingest_lap_times_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json("/mnt/formula1dl/raw/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("ingestion_date", current_timestamp()) 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_columns_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').parquet("/mnt/formula1dl/processed/results")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/5.ingest_results_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv("/mnt/formula1dl/raw/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import current_timestamp, to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("ingestion_date", current_timestamp()) \
                                  .withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_timestamp_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode('overwrite').partitionBy('race_year').parquet('/mnt/formula1dl/processed/races')

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json("/mnt/formula1dl/raw/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

constructor_final_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/constructors")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json("/mnt/formula1dl/raw/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, current_timestamp, lit

# COMMAND ----------

drivers_with_columns_df = drivers_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("ingestion_date", current_timestamp()) \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname")))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/drivers")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv("/mnt/formula1dl/raw/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") 

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

circuits_final_df = circuits_renamed_df.withColumn("ingestion_date", current_timestamp()) 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/circuits")

# COMMAND ----------

display(spark.read.parquet("/mnt/formula1dl/processed/circuits"))

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/1.ingest_circuits_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json("/mnt/formula1dl/raw/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = pit_stops_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/pit_stops")

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json("/mnt/formula1dl/raw/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = qualifying_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/qualifying")

# COMMAND ----------

display(spark.read.parquet('/mnt/formula1dl/processed/qualifying'))

# COMMAND ----------


/Formula1-Project-Solutions (1)/Section-09-10-11/ingestion/8.ingest_qualifying_file
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

import pyspark
from pyspark.sql import SparkSession



spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data = [("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  ]
columns = ["firstname","lastname","country","state"]
df = spark.createDataFrame(data = data, schema = columns)
df.show(truncate=False)

df.createOrReplaceTempView("df_tbl")


spark.sql('Create table tt as select *from df_tbl')
          

/Users/dora@octopai.com/Sql_PY (1)
--  MAGIC %scala
--  MAGIC import scala.util.parsing.json.JSON
--  MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
--  MAGIC import za.co.absa.spline.agent.AgentConfig
--  MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
--  MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
--  MAGIC import org.apache.commons.configuration.Configuration
--  MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
--  MAGIC import za.co.absa.spline.harvester.HarvestingContext
--  MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
--  MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
--  MAGIC import za.co.absa.spline.producer.model.ReadOperation
--  MAGIC import za.co.absa.spline.producer.model.WriteOperation
--  MAGIC import za.co.absa.spline.producer.model.DataOperation
--  MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
--  MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
--  MAGIC
--  MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
--  MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
--  MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
--  MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
--  MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
--  MAGIC val workspaceUrl=tagMap("browserHostName")
--  MAGIC
--  MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
--  MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
--  MAGIC val user = tagMap("user")
--  MAGIC val name = notebookPath(notebookPath.size-1)
--  MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
--  MAGIC "user" -> user,
--  MAGIC "workspaceName" ->workspaceName,
--  MAGIC "workspaceUrl" -> workspaceUrl,
--  MAGIC "name" -> name,
--  MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
--  MAGIC "timestamp" -> System.currentTimeMillis)
--  MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
--  MAGIC
--  MAGIC
--  MAGIC class CustomFilter extends PostProcessingFilter {
--  MAGIC   def this(conf: Configuration) = this()
--  MAGIC
--  MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
--  MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
--  MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
--  MAGIC
--  MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC }
--  MAGIC
--  MAGIC
--  MAGIC
--  MAGIC val myInstance = new CustomFilter()
--  MAGIC
--  MAGIC
--  MAGIC spark.enableLineageTracking(
--  MAGIC   AgentConfig.builder()
--  MAGIC     .postProcessingFilter(myInstance)
--  MAGIC     .build()
--  MAGIC )
--  COMMAND ----------

'-- Databricks notebook source
-- MAGIC %python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
-- COMMAND ----------

import za.co.absa.spline.harvester.SparkLineageInitializer._


System.setProperty("spline.lineageDispatcher","http")
System.setProperty("spline.lineageDispatcher.http.producer.url","http://20.9.85.169:8080/producer")

spark.enableLineageTracking()

val m1=spark.read.options(Map("inferSchema" -> "true","header" -> "true")).csv("/FileStore/tables/circuits-2.csv").as("m1")

/Users/zacayd@octopai.com/splineScaleExample
--  MAGIC %scala
--  MAGIC import scala.util.parsing.json.JSON
--  MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
--  MAGIC import za.co.absa.spline.agent.AgentConfig
--  MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
--  MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
--  MAGIC import org.apache.commons.configuration.Configuration
--  MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
--  MAGIC import za.co.absa.spline.harvester.HarvestingContext
--  MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
--  MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
--  MAGIC import za.co.absa.spline.producer.model.ReadOperation
--  MAGIC import za.co.absa.spline.producer.model.WriteOperation
--  MAGIC import za.co.absa.spline.producer.model.DataOperation
--  MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
--  MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
--  MAGIC
--  MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
--  MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
--  MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
--  MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
--  MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
--  MAGIC val workspaceUrl=tagMap("browserHostName")
--  MAGIC
--  MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
--  MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
--  MAGIC val user = tagMap("user")
--  MAGIC val name = notebookPath(notebookPath.size-1)
--  MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
--  MAGIC "user" -> user,
--  MAGIC "workspaceName" ->workspaceName,
--  MAGIC "workspaceUrl" -> workspaceUrl,
--  MAGIC "name" -> name,
--  MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
--  MAGIC "timestamp" -> System.currentTimeMillis)
--  MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
--  MAGIC
--  MAGIC
--  MAGIC class CustomFilter extends PostProcessingFilter {
--  MAGIC   def this(conf: Configuration) = this()
--  MAGIC
--  MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
--  MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
--  MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
--  MAGIC
--  MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC }
--  MAGIC
--  MAGIC
--  MAGIC
--  MAGIC val myInstance = new CustomFilter()
--  MAGIC
--  MAGIC
--  MAGIC spark.enableLineageTracking(
--  MAGIC   AgentConfig.builder()
--  MAGIC     .postProcessingFilter(myInstance)
--  MAGIC     .build()
--  MAGIC )
--  COMMAND ----------

'-- Databricks notebook source
-- MAGIC %python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
-- COMMAND ----------

import za.co.absa.spline.harvester.HarvestingContext
import za.co.absa.spline.harvester.MetadataCollectingFilter
import za.co.absa.spline.harvester.MetadataCollectingFilterSpec
import org.apache.commons.configuration.Configuration
import za.co.absa.spline.harvester.SparkLineageInitializer._

val splineConf: Configuration = StandardSplineConfigurationStack(spark)

spark.enableLineageTracking(new MetadataCollectingFilterSpec(splineConf) {
  override def extraMetadataForExecutionEvent(
    event: ExecutionEvent,
    ctx: HarvestingContext): Map[String, Any] =
    Map("foo" -> "bar1")

  override def extraMetadataForExecutionPlan(
    plan: ExecutionPlan,
    ctx: HarvestingContext): Map[String, Any] =
    Map("notebookInfo" -> notebookInfoJson)

  override def extraMetadataForReadOperation(
    operation: ReadOperation,
    ctx: HarvestingContext): Map[String, Any] =
    Map("foo" -> "bar3")

  override def extraMetadataForWriteOperation(
    operation: WriteOperation,
    ctx: HarvestingContext): Map[String, Any] =
    Map("foo" -> "bar4")

  override def extraMetadataForDataOperation(
    operation: DataOperation,
    ctx: HarvestingContext): Map[String, Any] =
    Map("foo" -> "bar5")
})
/Users/zacayd@octopai.com/ScalaCode
--  MAGIC %scala
--  MAGIC import scala.util.parsing.json.JSON
--  MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
--  MAGIC import za.co.absa.spline.agent.AgentConfig
--  MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
--  MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
--  MAGIC import org.apache.commons.configuration.Configuration
--  MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
--  MAGIC import za.co.absa.spline.harvester.HarvestingContext
--  MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
--  MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
--  MAGIC import za.co.absa.spline.producer.model.ReadOperation
--  MAGIC import za.co.absa.spline.producer.model.WriteOperation
--  MAGIC import za.co.absa.spline.producer.model.DataOperation
--  MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
--  MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
--  MAGIC
--  MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
--  MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
--  MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
--  MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
--  MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
--  MAGIC val workspaceUrl=tagMap("browserHostName")
--  MAGIC
--  MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
--  MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
--  MAGIC val user = tagMap("user")
--  MAGIC val name = notebookPath(notebookPath.size-1)
--  MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
--  MAGIC "user" -> user,
--  MAGIC "workspaceName" ->workspaceName,
--  MAGIC "workspaceUrl" -> workspaceUrl,
--  MAGIC "name" -> name,
--  MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
--  MAGIC "timestamp" -> System.currentTimeMillis)
--  MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
--  MAGIC
--  MAGIC
--  MAGIC class CustomFilter extends PostProcessingFilter {
--  MAGIC   def this(conf: Configuration) = this()
--  MAGIC
--  MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
--  MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
--  MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
--  MAGIC
--  MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC
--  MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
--  MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
--  MAGIC }
--  MAGIC
--  MAGIC
--  MAGIC
--  MAGIC val myInstance = new CustomFilter()
--  MAGIC
--  MAGIC
--  MAGIC spark.enableLineageTracking(
--  MAGIC   AgentConfig.builder()
--  MAGIC     .postProcessingFilter(myInstance)
--  MAGIC     .build()
--  MAGIC )
--  COMMAND ----------

'-- Databricks notebook source
-- MAGIC %python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
-- COMMAND ----------


import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
import za.co.absa.spline.harvester.HarvestingContext
import org.apache.commons.configuration.Configuration
import za.co.absa.spline.harvester.SparkLineageInitializer._
import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
import za.co.absa.spline.producer.model.v1_1._
import za.co.absa.spline.producer.model._
import scala.util.parsing.json.JSON
import scala.concurrent.duration.Duration
import scala.util.{Failure, Success, Try}

val splineConf: Configuration = StandardSplineConfigurationStack(spark)


spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
  //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
  //val test = dbutils.notebook.getContext.notebookPath
  val notebookInformationJson = dbutils.notebook.getContext.toJson
  val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
  val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]

  val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
  val notebookPath = extraContextMap("notebook_path").split("/")
  
  val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
  val user = tagMap("user")
  val name = notebookPath(notebookPath.size-1)
  
  val notebookInfo = Map("notebookURL" -> notebookURL,  
                "user" -> user, 
                "name" -> name,
                "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
                "timestamp" -> System.currentTimeMillis)
  val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
  
  override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
    override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
    override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
    override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
    override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
    override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
  })
})
/Users/zacayd@octopai.com/SplineCode
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Drop all the tables

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_processed CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed";

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_presentation CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation";

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/utils/1.prepare_for_incremental_load
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/9.create_processed_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/0.ingest_all_files
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.lap_times")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/7.ingest_lap_times_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.qualifying")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/8.ingest_qualifying_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.pit_stops")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.drivers")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/1.ingest_circuits_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("parquet").saveAsTable("f1_processed.races")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.constructors")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/ingestion/5.ingest_results_file
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_raw;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for CSV files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create circuits table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.circuits;
CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,
circuitRef STRING,
name STRING,
location STRING,
country STRING,
lat DOUBLE,
lng DOUBLE,
alt INT,
url STRING
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/circuits.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.circuits;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create races table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.races;
CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT,
year INT,
round INT,
circuitId INT,
name STRING,
date DATE,
time STRING,
url STRING)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/races.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.races;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for JSON files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create constructors table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.constructors;
CREATE TABLE IF NOT EXISTS f1_raw.constructors(
constructorId INT,
constructorRef STRING,
name STRING,
nationality STRING,
url STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/constructors.json")

-- COMMAND ----------

SELECT * FROM f1_raw.constructors;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create drivers table
-- MAGIC * Single Line JSON
-- MAGIC * Complex structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.drivers;
CREATE TABLE IF NOT EXISTS f1_raw.drivers(
driverId INT,
driverRef STRING,
number INT,
code STRING,
name STRUCT<forename: STRING, surname: STRING>,
dob DATE,
nationality STRING,
url STRING)
USING json
OPTIONS (path "/mnt/formula1dl/raw/drivers.json")

-- COMMAND ----------

-- MAGIC %md ##### Create results table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.results;
CREATE TABLE IF NOT EXISTS f1_raw.results(
resultId INT,
raceId INT,
driverId INT,
constructorId INT,
number INT,grid INT,
position INT,
positionText STRING,
positionOrder INT,
points INT,
laps INT,
time STRING,
milliseconds INT,
fastestLap INT,
rank INT,
fastestLapTime STRING,
fastestLapSpeed FLOAT,
statusId STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/results.json")

-- COMMAND ----------

SELECT * FROM f1_raw.results

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create pit stops table
-- MAGIC * Multi Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.pit_stops;
CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(
driverId INT,
duration STRING,
lap INT,
milliseconds INT,
raceId INT,
stop INT,
time STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/pit_stops.json", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.pit_stops;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for list of files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Lap Times Table
-- MAGIC * CSV file
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.lap_times;
CREATE TABLE IF NOT EXISTS f1_raw.lap_times(
raceId INT,
driverId INT,
lap INT,
position INT,
time STRING,
milliseconds INT
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/lap_times")

-- COMMAND ----------

SELECT * FROM f1_raw.lap_times

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Qualifying Table
-- MAGIC * JSON file
-- MAGIC * MultiLine JSON
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.qualifying;
CREATE TABLE IF NOT EXISTS f1_raw.qualifying(
constructorId INT,
driverId INT,
number INT,
position INT,
q1 STRING,
q2 STRING,
q3 STRING,
qualifyId INT,
raceId INT)
USING json
OPTIONS (path "/mnt/formula1dl/raw/qualifying", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.qualifying

-- COMMAND ----------

DESC EXTENDED f1_raw.qualifying;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/raw/1.create_raw_tables
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2011
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/2.find_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Report on Dominant Formula 1 Drivers </h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_drivers
AS
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) driver_rank
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/3.viz_dominant_drivers
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Report on Dominant Formula 1 Teams </h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_teams
AS
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) team_rank
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT * FROM v_dominant_teams;

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/4.viz_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2010
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-16-17-18/analysis/1.find_dominant_drivers
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/1.ingest_circuits_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/lap_times")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/7.ingest_lap_times_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/drivers")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').parquet(f"{processed_folder_path}/results")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/5.ingest_results_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/qualifying")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/8.ingest_qualifying_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/0.ingest_all_files
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode('overwrite').partitionBy('race_year').parquet(f'{processed_folder_path}/races')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/constructors")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/pit_stops")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-12/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

storage_account_name = "formula1dl"
client_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-id")
tenant_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-tenant-id")
client_secret        = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-secret")

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

mount_adls("presentation")

# COMMAND ----------

mount_adls("demo")

# COMMAND ----------

dbutils.fs.unmount("/mnt/formula1dl/demo")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/presentation")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/demo")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/set-up/mount_adls_storage
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Teams of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_teams
AS
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) team_rank
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT * FROM v_dominant_teams;

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/4.viz_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Drivers of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_drivers
AS
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) driver_rank
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/3.viz_dominant_drivers
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2011
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/2.find_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2010
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/analysis/1.find_dominant_drivers
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_raw;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for CSV files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create circuits table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.circuits;
CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,
circuitRef STRING,
name STRING,
location STRING,
country STRING,
lat DOUBLE,
lng DOUBLE,
alt INT,
url STRING
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/circuits.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.circuits;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create races table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.races;
CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT,
year INT,
round INT,
circuitId INT,
name STRING,
date DATE,
time STRING,
url STRING)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/races.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.races;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for JSON files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create constructors table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.constructors;
CREATE TABLE IF NOT EXISTS f1_raw.constructors(
constructorId INT,
constructorRef STRING,
name STRING,
nationality STRING,
url STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/constructors.json")

-- COMMAND ----------

SELECT * FROM f1_raw.constructors;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create drivers table
-- MAGIC * Single Line JSON
-- MAGIC * Complex structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.drivers;
CREATE TABLE IF NOT EXISTS f1_raw.drivers(
driverId INT,
driverRef STRING,
number INT,
code STRING,
name STRUCT<forename: STRING, surname: STRING>,
dob DATE,
nationality STRING,
url STRING)
USING json
OPTIONS (path "/mnt/formula1dl/raw/drivers.json")

-- COMMAND ----------

-- MAGIC %md ##### Create results table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.results;
CREATE TABLE IF NOT EXISTS f1_raw.results(
resultId INT,
raceId INT,
driverId INT,
constructorId INT,
number INT,grid INT,
position INT,
positionText STRING,
positionOrder INT,
points INT,
laps INT,
time STRING,
milliseconds INT,
fastestLap INT,
rank INT,
fastestLapTime STRING,
fastestLapSpeed FLOAT,
statusId STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/results.json")

-- COMMAND ----------

SELECT * FROM f1_raw.results

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create pit stops table
-- MAGIC * Multi Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.pit_stops;
CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(
driverId INT,
duration STRING,
lap INT,
milliseconds INT,
raceId INT,
stop INT,
time STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/pit_stops.json", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.pit_stops;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for list of files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Lap Times Table
-- MAGIC * CSV file
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.lap_times;
CREATE TABLE IF NOT EXISTS f1_raw.lap_times(
raceId INT,
driverId INT,
lap INT,
position INT,
time STRING,
milliseconds INT
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/lap_times")

-- COMMAND ----------

SELECT * FROM f1_raw.lap_times

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Qualifying Table
-- MAGIC * JSON file
-- MAGIC * MultiLine JSON
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.qualifying;
CREATE TABLE IF NOT EXISTS f1_raw.qualifying(
constructorId INT,
driverId INT,
number INT,
position INT,
q1 STRING,
q2 STRING,
q3 STRING,
qualifyId INT,
raceId INT)
USING json
OPTIONS (path "/mnt/formula1dl/raw/qualifying", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.qualifying

-- COMMAND ----------

DESC EXTENDED f1_raw.qualifying;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/raw/1.create_raw_tables
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Drop all the tables

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_processed CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed";

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_presentation CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation";

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/utils/1.prepare_for_incremental_load
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Access dataframes using SQL
# MAGIC ##### Objectives
# MAGIC 1. Create temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

race_results_df.createOrReplaceTempView("v_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC FROM v_race_results
# MAGIC WHERE race_year = 2020

# COMMAND ----------

p_race_year = 2020

# COMMAND ----------

race_results_2019_df = spark.sql(f"SELECT * FROM v_race_results WHERE race_year = {p_race_year}")

# COMMAND ----------

display(race_results_2019_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Global Temporary Views
# MAGIC 1. Create global temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell
# MAGIC 4. Acesss the view from another notebook

# COMMAND ----------

race_results_df.createOrReplaceGlobalTempView("gv_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW TABLES IN global_temp;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------

spark.sql("SELECT * \
  FROM global_temp.gv_race_results").show()

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/4.sql_temp_view_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_presentation;

-- COMMAND ----------

DESC driver_standings

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2018
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2018

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2020
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2020;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Inner Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Left Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  LEFT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Right Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  RIGHT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Full Join
-- MAGIC

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  FULL JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Semi Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  SEMI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Anti Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  ANTI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC Cross Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  CROSS JOIN v_driver_standings_2020 d_2020

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/9.sql_joins_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Lesson Objectives
-- MAGIC 1. Spark SQL documentation
-- MAGIC 1. Create Database demo
-- MAGIC 1. Data tab in the UI
-- MAGIC 1. SHOW command
-- MAGIC 1. DESCRIBE command
-- MAGIC 1. Find the current database

-- COMMAND ----------

CREATE DATABASE demo;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS demo;

-- COMMAND ----------

SHOW databases;

-- COMMAND ----------

DESCRIBE DATABASE demo; 

-- COMMAND ----------

DESCRIBE DATABASE EXTENDED demo; 

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

USE demo;

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Managed Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create managed table using Python
-- MAGIC 1. Create managed table using SQL
-- MAGIC 1. Effect of dropping a managed table
-- MAGIC 1. Describe table 

-- COMMAND ----------

-- MAGIC %run "../includes/configuration"

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").saveAsTable("demo.race_results_python")

-- COMMAND ----------

USE demo;
SHOW TABLES;

-- COMMAND ----------

DESC EXTENDED race_results_python;

-- COMMAND ----------

SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

CREATE TABLE demo.race_results_sql
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

DESC EXTENDED demo.race_results_sql;

-- COMMAND ----------

DROP TABLE demo.race_results_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### External Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create external table using Python
-- MAGIC 1. Create external table using SQL
-- MAGIC 1. Effect of dropping an external table

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").option("path", f"{presentation_folder_path}/race_results_ext_py").saveAsTable("demo.race_results_ext_py")

-- COMMAND ----------

DESC EXTENDED demo.race_results_ext_py

-- COMMAND ----------

CREATE TABLE demo.race_results_ext_sql
(race_year INT,
race_name STRING,
race_date TIMESTAMP,
circuit_location STRING,
driver_name STRING,
driver_number INT,
driver_nationality STRING,
team STRING,
grid INT,
fastest_lap INT,
race_time STRING,
points FLOAT,
position INT,
created_date TIMESTAMP
)
USING parquet
LOCATION "/mnt/formula1dl/presentation/race_results_ext_sql"

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

INSERT INTO demo.race_results_ext_sql
SELECT * FROM demo.race_results_ext_py WHERE race_year = 2020;

-- COMMAND ----------

SELECT COUNT(1) FROM demo.race_results_ext_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

DROP TABLE demo.race_results_ext_sql

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Views on tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create Temp View
-- MAGIC 1. Create Global Temp View
-- MAGIC 1. Create Permanent View

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_race_results;

-- COMMAND ----------

CREATE OR REPLACE GLOBAL TEMP VIEW gv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2012;

-- COMMAND ----------

SELECT * FROM global_temp.gv_race_results

-- COMMAND ----------

SHOW TABLES IN global_temp;

-- COMMAND ----------

CREATE OR REPLACE VIEW demo.pv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2000;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

SELECT * FROM demo.pv_race_results;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/6.sql_objects_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM v_race_results

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/5.sql_temp_view_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC #### Aggregate functions demo

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Built-in Aggregate functions

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

display(race_results_df)

# COMMAND ----------

demo_df = race_results_df.filter("race_year=2020")

# COMMAND ----------

display(demo_df)

# COMMAND ----------

from pyspark.sql.functions import count, countDistinct, sum

# COMMAND ----------

demo_df.select(count("*")).show()

# COMMAND ----------

demo_df.select(count("race_name")).show()

# COMMAND ----------

demo_df.select(countDistinct("race_name")).show()

# COMMAND ----------

demo_df.select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points"), countDistinct("race_name")) \
.withColumnRenamed("sum(points)", "total_points") \
.withColumnRenamed("count(DISTINCT race_name)", "number_of_races") \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### groupBy

# COMMAND ----------

demo_df\
.groupBy("driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Window Functions

# COMMAND ----------

demo_df = race_results_df.filter("race_year in (2019, 2020)")

# COMMAND ----------

demo_grouped_df = demo_df\
.groupBy("race_year", "driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) 

# COMMAND ----------

display(demo_grouped_df)

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank

driverRankSpec = Window.partitionBy("race_year").orderBy(desc("total_points"))
demo_grouped_df.withColumn("rank", rank().over(driverRankSpec)).show(100)

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/3.aggregation_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SHOW DATABASES;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SELECT *
 FROM drivers;

-- COMMAND ----------

DESC drivers;

-- COMMAND ----------

SELECT *
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob AS date_of_birth
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob 
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01'
ORDER BY dob DESC;

-- COMMAND ----------

SELECT *
  FROM drivers
 ORDER BY nationality ASC,
          dob DESC;

-- COMMAND ----------

SELECT name, nationality,dob 
 FROM drivers
WHERE (nationality = 'British'
  AND dob >= '1990-01-01') 
   OR nationality = 'Indian'
ORDER BY dob DESC;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/7.sql_basics_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Spark Join Transformation

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.filter("circuit_id < 70") \
.withColumnRenamed("name", "circuit_name")

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races").filter("race_year = 2019") \
.withColumnRenamed("name", "race_name")

# COMMAND ----------

display(circuits_df)

# COMMAND ----------

display(races_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Inner Join

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "inner") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Outer Joins

# COMMAND ----------

# Left Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "left") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# right Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "right") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# full Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "full") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Semi Joins

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "semi") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Anti Joins

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, circuits_df.circuit_id == races_df.circuit_id, "anti") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Cross Joins

# COMMAND ----------

race_circuits_df = races_df.crossJoin(circuits_df)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

race_circuits_df.count()

# COMMAND ----------

int(races_df.count()) * int(circuits_df.count())

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/2.join_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SELECT *, CONCAT(driver_ref, '-', code) AS new_driver_ref
  FROM drivers

-- COMMAND ----------

SELECT *, SPLIT(name, ' ')[0] forename, SPLIT(name, ' ')[1] surname
  FROM drivers

-- COMMAND ----------

SELECT *, current_timestamp
  FROM drivers

-- COMMAND ----------

SELECT *, date_format(dob, 'dd-MM-yyyy')
  FROM drivers

-- COMMAND ----------

SELECT *, date_add(dob, 1)
  FROM drivers

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers;

-- COMMAND ----------

SELECT MAX(dob) 
  FROM drivers;

-- COMMAND ----------

SELECT * FROM drivers WHERE dob = '2000-05-11'

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers
 WHERE nationality = 'British' ;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 HAVING COUNT(*) > 100
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, name, dob, RANK() OVER(PARTITION BY nationality ORDER BY dob DESC) AS age_rank
  FROM drivers
 ORDER BY nationality, age_rank

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/8.sql_functions_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC 1. Write data to delta lake (managed table)
# MAGIC 2. Write data to delta lake (external table)
# MAGIC 3. Read data from delta lake (Table)
# MAGIC 4. Read data from delta lake (File)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE DATABASE IF NOT EXISTS f1_demo
# MAGIC LOCATION '/mnt/formula1dl/demo'

# COMMAND ----------

results_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/results.json")

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").saveAsTable("f1_demo.results_managed")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").save("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE f1_demo.results_external
# MAGIC USING DELTA
# MAGIC LOCATION '/mnt/formula1dl/demo/results_external'

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_external

# COMMAND ----------

results_external_df = spark.read.format("delta").load("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

display(results_external_df)

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").partitionBy("constructorId").saveAsTable("f1_demo.results_partitioned")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW PARTITIONS f1_demo.results_partitioned

# COMMAND ----------

# MAGIC %md
# MAGIC 1. Update Delta Table
# MAGIC 2. Delete From Delta Table

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC UPDATE f1_demo.results_managed
# MAGIC   SET points = 11 - position
# MAGIC WHERE position <= 10

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.update("position <= 10", { "points": "21 - position" } ) 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.results_managed
# MAGIC WHERE position > 10;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.delete("points = 0") 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %md
# MAGIC Upsert using merge

# COMMAND ----------

drivers_day1_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId <= 10") \
.select("driverId", "dob", "name.forename", "name.surname")

# COMMAND ----------

display(drivers_day1_df)

# COMMAND ----------

drivers_day1_df.createOrReplaceTempView("drivers_day1")

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day2_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 6 AND 15") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

drivers_day2_df.createOrReplaceTempView("drivers_day2")

# COMMAND ----------

display(drivers_day2_df)

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day3_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 1 AND 5 OR driverId BETWEEN 16 AND 20") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_merge (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %md Day1

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day1 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 2

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day2 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 3

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/drivers_merge")

deltaTable.alias("tgt").merge(
    drivers_day3_df.alias("upd"),
    "tgt.driverId = upd.driverId") \
  .whenMatchedUpdate(set = { "dob" : "upd.dob", "forename" : "upd.forename", "surname" : "upd.surname", "updatedDate": "current_timestamp()" } ) \
  .whenNotMatchedInsert(values =
    {
      "driverId": "upd.driverId",
      "dob": "upd.dob",
      "forename" : "upd.forename", 
      "surname" : "upd.surname", 
      "createdDate": "current_timestamp()"
    }
  ) \
  .execute()

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC 1. History & Versioning
# MAGIC 2. Time Travel
# MAGIC 3. Vaccum

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

df = spark.read.format("delta").option("timestampAsOf", '2021-06-23T15:40:33.000+0000').load("/mnt/formula1dl/demo/drivers_merge")

# COMMAND ----------

display(df)

# COMMAND ----------

# MAGIC %sql
# MAGIC VACUUM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SET spark.databricks.delta.retentionDurationCheck.enabled = false;
# MAGIC VACUUM f1_demo.drivers_merge RETAIN 0 HOURS

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.drivers_merge WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql 
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 3;

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING f1_demo.drivers_merge VERSION AS OF 3 src
# MAGIC    ON (tgt.driverId = src.driverId)
# MAGIC WHEN NOT MATCHED THEN
# MAGIC    INSERT *

# COMMAND ----------

# MAGIC %sql DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %md
# MAGIC Transaction Logs

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_txn (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM  f1_demo.drivers_txn
# MAGIC WHERE driverId = 1;

# COMMAND ----------

for driver_id in range(3, 20):
  spark.sql(f"""INSERT INTO f1_demo.drivers_txn
                SELECT * FROM f1_demo.drivers_merge
                WHERE driverId = {driver_id}""")

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Convert Parquet to Delta

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_convert_to_delta (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING PARQUET

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_convert_to_delta
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA f1_demo.drivers_convert_to_delta

# COMMAND ----------

df = spark.table("f1_demo.drivers_convert_to_delta")

# COMMAND ----------

df.write.format("parquet").save("/mnt/formula1dl/demo/drivers_convert_to_delta_new")

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA parquet.`/mnt/formula1dl/demo/drivers_convert_to_delta_new`

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/10.delta_lake_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races")

# COMMAND ----------

races_filtered_df = races_df.filter("race_year = 2019 and round <= 5")

# COMMAND ----------

races_filtered_df = races_df.where((races_df["race_year"] == 2019) & (races_df["round"] <= 5))

# COMMAND ----------

display(races_filtered_df)

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/demo/1.filter_demo
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
def add_ingestion_date(input_df):
  output_df = input_df.withColumn("ingestion_date", current_timestamp())
  return output_df

# COMMAND ----------

def re_arrange_partition_column(input_df, partition_column):
  column_list = []
  for column_name in input_df.schema.names:
    if column_name != partition_column:
      column_list.append(column_name)
  column_list.append(partition_column)
  output_df = input_df.select(column_list)
  return output_df

# COMMAND ----------

def overwrite_partition(input_df, db_name, table_name, partition_column):
  output_df = re_arrange_partition_column(input_df, partition_column)
  spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    output_df.write.mode("overwrite").insertInto(f"{db_name}.{table_name}")
  else:
    output_df.write.mode("overwrite").partitionBy(partition_column).format("parquet").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------

def df_column_to_list(input_df, column_name):
  df_row_list = input_df.select(column_name) \
                        .distinct() \
                        .collect()
  
  column_value_list = [row[column_name] for row in df_row_list]
  return column_value_list

# COMMAND ----------

def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):
  spark.conf.set("spark.databricks.optimizer.dynamicPartitionPruning","true")

  from delta.tables import DeltaTable
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    deltaTable = DeltaTable.forPath(spark, f"{folder_path}/{table_name}")
    deltaTable.alias("tgt").merge(
        input_df.alias("src"),
        merge_condition) \
      .whenMatchedUpdateAll()\
      .whenNotMatchedInsertAll()\
      .execute()
  else:
    input_df.write.mode("overwrite").partitionBy(partition_column).format("delta").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/includes/common_functions
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

raw_folder_path = "/mnt/formula1dl/raw"
processed_folder_path = "/mnt/formula1dl/processed"
presentation_folder_path = "/mnt/formula1dl/presentation"

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/includes/configuration
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

drivers_df = spark.read.format("delta").load(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.format("delta").load(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.format("delta").load(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.format("delta").load(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.format("delta").load(f"{processed_folder_path}/results") \
.filter(f"file_date = '{v_file_date}'") \
.withColumnRenamed("time", "race_time") \
.withColumnRenamed("race_id", "result_race_id") \
.withColumnRenamed("file_date", "result_file_date") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.result_race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_id", "race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position", "result_file_date") \
                          .withColumn("created_date", current_timestamp()) \
                          .withColumnRenamed("result_file_date", "file_date") 

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_presentation', 'race_results', presentation_folder_path, merge_condition, 'race_id')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.race_results;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/1.race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

spark.sql(f"""
              CREATE TABLE IF NOT EXISTS f1_presentation.calculated_race_results
              (
              race_year INT,
              team_name STRING,
              driver_id INT,
              driver_name STRING,
              race_id INT,
              position INT,
              points INT,
              calculated_points INT,
              created_date TIMESTAMP,
              updated_date TIMESTAMP
              )
              USING DELTA
""")

# COMMAND ----------

spark.sql(f"""
              CREATE OR REPLACE TEMP VIEW race_result_updated
              AS
              SELECT races.race_year,
                     constructors.name AS team_name,
                     drivers.driver_id,
                     drivers.name AS driver_name,
                     races.race_id,
                     results.position,
                     results.points,
                     11 - results.position AS calculated_points
                FROM f1_processed.results 
                JOIN f1_processed.drivers ON (results.driver_id = drivers.driver_id)
                JOIN f1_processed.constructors ON (results.constructor_id = constructors.constructor_id)
                JOIN f1_processed.races ON (results.race_id = races.race_id)
               WHERE results.position <= 10
                 AND results.file_date = '{v_file_date}'
""")

# COMMAND ----------

spark.sql(f"""
              MERGE INTO f1_presentation.calculated_race_results tgt
              USING race_result_updated upd
              ON (tgt.driver_id = upd.driver_id AND tgt.race_id = upd.race_id)
              WHEN MATCHED THEN
                UPDATE SET tgt.position = upd.position,
                           tgt.points = upd.points,
                           tgt.calculated_points = upd.calculated_points,
                           tgt.updated_date = current_timestamp
              WHEN NOT MATCHED
                THEN INSERT (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, created_date ) 
                     VALUES (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, current_timestamp)
       """)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM race_result_updated;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM f1_presentation.calculated_race_results;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/4.calculated_race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce constructor standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

merge_condition = "tgt.team = src.team AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'constructor_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.constructor_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.constructor_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/3.constructor_standings
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation"

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/0.create_presentation_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'driver_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.driver_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.driver_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/trans/2.driver_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'lap_times', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.lap = src.lap AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'lap_times', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/7.ingest_lap_times_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'pit_stops', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.stop = src.stop AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'pit_stops', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.pit_stops;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/{v_file_date}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source)) \
                                             .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.constructors")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.constructors;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/{v_file_date}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.drivers")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.drivers

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.circuits")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/1.ingest_circuits_file
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/9.create_processed_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("delta").saveAsTable("f1_processed.races")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.races;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/0.ingest_all_files
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/{v_file_date}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC De-dupe the dataframe

# COMMAND ----------

results_deduped_df = results_final_df.dropDuplicates(['race_id', 'driver_id'])

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

# MAGIC %md
# MAGIC Method 1

# COMMAND ----------

# for race_id_list in results_final_df.select("race_id").distinct().collect():
#   if (spark._jsparkSession.catalog().tableExists("f1_processed.results")):
#     spark.sql(f"ALTER TABLE f1_processed.results DROP IF EXISTS PARTITION (race_id = {race_id_list.race_id})")

# COMMAND ----------

# results_final_df.write.mode("append").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

# MAGIC %md
# MAGIC Method 2

# COMMAND ----------

# overwrite_partition(results_final_df, 'f1_processed', 'results', 'race_id')

# COMMAND ----------

merge_condition = "tgt.result_id = src.result_id AND tgt.race_id = src.race_id"
merge_delta_data(results_deduped_df, 'f1_processed', 'results', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC   FROM f1_processed.results;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_id, driver_id, COUNT(1) 
# MAGIC FROM f1_processed.results
# MAGIC GROUP BY race_id, driver_id
# MAGIC HAVING COUNT(1) > 1
# MAGIC ORDER BY race_id, driver_id DESC;

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_processed.results WHERE race_id = 540 AND driver_id = 229;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/5.ingest_results_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'qualifying', 'race_id')

# COMMAND ----------

merge_condition = "tgt.qualify_id = src.qualify_id AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'qualifying', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-20/formula1/ingestion/8.ingest_qualifying_file
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Drivers of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_drivers
AS
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) driver_rank
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/3.viz_dominant_drivers
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2011
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/2.find_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Dominant Formula 1 Teams of All Time</h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_teams
AS
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) team_rank
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT * FROM v_dominant_teams;

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/4.viz_dominant_teams
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2010
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/analysis/1.find_dominant_drivers
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

storage_account_name = "formula1dl"
client_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-id")
tenant_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-tenant-id")
client_secret        = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-secret")

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

mount_adls("presentation")

# COMMAND ----------

mount_adls("demo")

# COMMAND ----------

dbutils.fs.unmount("/mnt/formula1dl/demo")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/presentation")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/demo")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/set-up/mount_adls_storage
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/0.ingest_all_files
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'lap_times', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.lap = src.lap AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'lap_times', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/7.ingest_lap_times_file
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/9.create_processed_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/{v_file_date}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.drivers")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.drivers

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/4.ingest_drivers_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/{v_file_date}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC De-dupe the dataframe

# COMMAND ----------

results_deduped_df = results_final_df.dropDuplicates(['race_id', 'driver_id'])

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

# MAGIC %md
# MAGIC Method 1

# COMMAND ----------

# for race_id_list in results_final_df.select("race_id").distinct().collect():
#   if (spark._jsparkSession.catalog().tableExists("f1_processed.results")):
#     spark.sql(f"ALTER TABLE f1_processed.results DROP IF EXISTS PARTITION (race_id = {race_id_list.race_id})")

# COMMAND ----------

# results_final_df.write.mode("append").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

# MAGIC %md
# MAGIC Method 2

# COMMAND ----------

# overwrite_partition(results_final_df, 'f1_processed', 'results', 'race_id')

# COMMAND ----------

merge_condition = "tgt.result_id = src.result_id AND tgt.race_id = src.race_id"
merge_delta_data(results_deduped_df, 'f1_processed', 'results', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC   FROM f1_processed.results;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_id, driver_id, COUNT(1) 
# MAGIC FROM f1_processed.results
# MAGIC GROUP BY race_id, driver_id
# MAGIC HAVING COUNT(1) > 1
# MAGIC ORDER BY race_id, driver_id DESC;

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_processed.results WHERE race_id = 540 AND driver_id = 229;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/5.ingest_results_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'qualifying', 'race_id')

# COMMAND ----------

merge_condition = "tgt.qualify_id = src.qualify_id AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'qualifying', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/8.ingest_qualifying_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/{v_file_date}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source)) \
                                             .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.constructors")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.constructors;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/3.ingest_constructors_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("delta").saveAsTable("f1_processed.races")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.races;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/2.ingest_races_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

#overwrite_partition(final_df, 'f1_processed', 'pit_stops', 'race_id')

# COMMAND ----------

merge_condition = "tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.stop = src.stop AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_processed', 'pit_stops', processed_folder_path, merge_condition, 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.pit_stops;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/6.ingest_pit_stops_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("delta").saveAsTable("f1_processed.circuits")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/ingestion/1.ingest_circuits_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce constructor standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

merge_condition = "tgt.team = src.team AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'constructor_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.constructor_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.constructor_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/3.constructor_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.format("delta").load(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_year = src.race_year"
merge_delta_data(final_df, 'f1_presentation', 'driver_standings', presentation_folder_path, merge_condition, 'race_year')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.driver_standings WHERE race_year = 2021;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_year, COUNT(1)
# MAGIC   FROM f1_presentation.driver_standings
# MAGIC  GROUP BY race_year
# MAGIC  ORDER BY race_year DESC;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/2.driver_standings
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

drivers_df = spark.read.format("delta").load(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.format("delta").load(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.format("delta").load(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.format("delta").load(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.format("delta").load(f"{processed_folder_path}/results") \
.filter(f"file_date = '{v_file_date}'") \
.withColumnRenamed("time", "race_time") \
.withColumnRenamed("race_id", "result_race_id") \
.withColumnRenamed("file_date", "result_file_date") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.result_race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_id", "race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position", "result_file_date") \
                          .withColumn("created_date", current_timestamp()) \
                          .withColumnRenamed("result_file_date", "file_date") 

# COMMAND ----------

merge_condition = "tgt.driver_name = src.driver_name AND tgt.race_id = src.race_id"
merge_delta_data(final_df, 'f1_presentation', 'race_results', presentation_folder_path, merge_condition, 'race_id')

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_presentation.race_results;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/1.race_results
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

spark.sql(f"""
              CREATE TABLE IF NOT EXISTS f1_presentation.calculated_race_results
              (
              race_year INT,
              team_name STRING,
              driver_id INT,
              driver_name STRING,
              race_id INT,
              position INT,
              points INT,
              calculated_points INT,
              created_date TIMESTAMP,
              updated_date TIMESTAMP
              )
              USING DELTA
""")

# COMMAND ----------

spark.sql(f"""
              CREATE OR REPLACE TEMP VIEW race_result_updated
              AS
              SELECT races.race_year,
                     constructors.name AS team_name,
                     drivers.driver_id,
                     drivers.name AS driver_name,
                     races.race_id,
                     results.position,
                     results.points,
                     11 - results.position AS calculated_points
                FROM f1_processed.results 
                JOIN f1_processed.drivers ON (results.driver_id = drivers.driver_id)
                JOIN f1_processed.constructors ON (results.constructor_id = constructors.constructor_id)
                JOIN f1_processed.races ON (results.race_id = races.race_id)
               WHERE results.position <= 10
                 AND results.file_date = '{v_file_date}'
""")

# COMMAND ----------

spark.sql(f"""
              MERGE INTO f1_presentation.calculated_race_results tgt
              USING race_result_updated upd
              ON (tgt.driver_id = upd.driver_id AND tgt.race_id = upd.race_id)
              WHEN MATCHED THEN
                UPDATE SET tgt.position = upd.position,
                           tgt.points = upd.points,
                           tgt.calculated_points = upd.calculated_points,
                           tgt.updated_date = current_timestamp
              WHEN NOT MATCHED
                THEN INSERT (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, created_date ) 
                     VALUES (race_year, team_name, driver_id, driver_name,race_id, position, points, calculated_points, current_timestamp)
       """)

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM race_result_updated;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1) FROM f1_presentation.calculated_race_results;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/4.calculated_race_results
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation"

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/trans/0.create_presentation_database
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Access dataframes using SQL
# MAGIC ##### Objectives
# MAGIC 1. Create temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

race_results_df.createOrReplaceTempView("v_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT COUNT(1)
# MAGIC FROM v_race_results
# MAGIC WHERE race_year = 2020

# COMMAND ----------

p_race_year = 2020

# COMMAND ----------

race_results_2019_df = spark.sql(f"SELECT * FROM v_race_results WHERE race_year = {p_race_year}")

# COMMAND ----------

display(race_results_2019_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Global Temporary Views
# MAGIC 1. Create global temporary views on dataframes
# MAGIC 2. Access the view from SQL cell
# MAGIC 3. Access the view from Python cell
# MAGIC 4. Acesss the view from another notebook

# COMMAND ----------

race_results_df.createOrReplaceGlobalTempView("gv_race_results")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW TABLES IN global_temp;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------

spark.sql("SELECT * \
  FROM global_temp.gv_race_results").show()

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/4.sql_temp_view_demo
-- MAGIC %scala
-- MAGIC import scala.util.parsing.json.JSON
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC import za.co.absa.spline.agent.AgentConfig
-- MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
-- MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
-- MAGIC import org.apache.commons.configuration.Configuration
-- MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
-- MAGIC import za.co.absa.spline.harvester.HarvestingContext
-- MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
-- MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
-- MAGIC import za.co.absa.spline.producer.model.ReadOperation
-- MAGIC import za.co.absa.spline.producer.model.WriteOperation
-- MAGIC import za.co.absa.spline.producer.model.DataOperation
-- MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
-- MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
-- MAGIC
-- MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
-- MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
-- MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
-- MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
-- MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
-- MAGIC val workspaceUrl=tagMap("browserHostName")
-- MAGIC
-- MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
-- MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
-- MAGIC val user = tagMap("user")
-- MAGIC val name = notebookPath(notebookPath.size-1)
-- MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
-- MAGIC "user" -> user,
-- MAGIC "workspaceName" ->workspaceName,
-- MAGIC "workspaceUrl" -> workspaceUrl,
-- MAGIC "name" -> name,
-- MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
-- MAGIC "timestamp" -> System.currentTimeMillis)
-- MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
-- MAGIC
-- MAGIC
-- MAGIC class CustomFilter extends PostProcessingFilter {
-- MAGIC   def this(conf: Configuration) = this()
-- MAGIC
-- MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
-- MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
-- MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
-- MAGIC
-- MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC
-- MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
-- MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
-- MAGIC }
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC val myInstance = new CustomFilter()
-- MAGIC
-- MAGIC
-- MAGIC spark.enableLineageTracking(
-- MAGIC   AgentConfig.builder()
-- MAGIC     .postProcessingFilter(myInstance)
-- MAGIC     .build()
-- MAGIC )

-- COMMAND ----------

'-- Databricks notebook source
%python
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SELECT *, CONCAT(driver_ref, '-', code) AS new_driver_ref
  FROM drivers

-- COMMAND ----------

SELECT *, SPLIT(name, ' ')[0] forename, SPLIT(name, ' ')[1] surname
  FROM drivers

-- COMMAND ----------

SELECT *, current_timestamp
  FROM drivers

-- COMMAND ----------

SELECT *, date_format(dob, 'dd-MM-yyyy')
  FROM drivers

-- COMMAND ----------

SELECT *, date_add(dob, 1)
  FROM drivers

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers;

-- COMMAND ----------

SELECT MAX(dob) 
  FROM drivers;

-- COMMAND ----------

SELECT * FROM drivers WHERE dob = '2000-05-11'

-- COMMAND ----------

SELECT COUNT(*) 
  FROM drivers
 WHERE nationality = 'British' ;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, COUNT(*) 
  FROM drivers
 GROUP BY nationality
 HAVING COUNT(*) > 100
 ORDER BY nationality;

-- COMMAND ----------

SELECT nationality, name, dob, RANK() OVER(PARTITION BY nationality ORDER BY dob DESC) AS age_rank
  FROM drivers
 ORDER BY nationality, age_rank

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/8.sql_functions_demo
# MAGIC %md
# MAGIC 1. Write data to delta lake (managed table)
# MAGIC 2. Write data to delta lake (external table)
# MAGIC 3. Read data from delta lake (Table)
# MAGIC 4. Read data from delta lake (File)

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE DATABASE IF NOT EXISTS f1_demo
# MAGIC LOCATION '/mnt/formula1dl/demo'

# COMMAND ----------

results_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/results.json")

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").saveAsTable("f1_demo.results_managed")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").save("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE f1_demo.results_external
# MAGIC USING DELTA
# MAGIC LOCATION '/mnt/formula1dl/demo/results_external'

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_external

# COMMAND ----------

results_external_df = spark.read.format("delta").load("/mnt/formula1dl/demo/results_external")

# COMMAND ----------

display(results_external_df)

# COMMAND ----------

results_df.write.format("delta").mode("overwrite").partitionBy("constructorId").saveAsTable("f1_demo.results_partitioned")

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW PARTITIONS f1_demo.results_partitioned

# COMMAND ----------

# MAGIC %md
# MAGIC 1. Update Delta Table
# MAGIC 2. Delete From Delta Table

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC UPDATE f1_demo.results_managed
# MAGIC   SET points = 11 - position
# MAGIC WHERE position <= 10

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.update("position <= 10", { "points": "21 - position" } ) 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.results_managed
# MAGIC WHERE position > 10;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")

deltaTable.delete("points = 0") 

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.results_managed;

# COMMAND ----------

# MAGIC %md
# MAGIC Upsert using merge

# COMMAND ----------

drivers_day1_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId <= 10") \
.select("driverId", "dob", "name.forename", "name.surname")

# COMMAND ----------

display(drivers_day1_df)

# COMMAND ----------

drivers_day1_df.createOrReplaceTempView("drivers_day1")

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day2_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 6 AND 15") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

drivers_day2_df.createOrReplaceTempView("drivers_day2")

# COMMAND ----------

display(drivers_day2_df)

# COMMAND ----------

from pyspark.sql.functions import upper

drivers_day3_df = spark.read \
.option("inferSchema", True) \
.json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
.filter("driverId BETWEEN 1 AND 5 OR driverId BETWEEN 16 AND 20") \
.select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_merge (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %md Day1

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day1 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 2

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING drivers_day2 upd
# MAGIC ON tgt.driverId = upd.driverId
# MAGIC WHEN MATCHED THEN
# MAGIC   UPDATE SET tgt.dob = upd.dob,
# MAGIC              tgt.forename = upd.forename,
# MAGIC              tgt.surname = upd.surname,
# MAGIC              tgt.updatedDate = current_timestamp
# MAGIC WHEN NOT MATCHED
# MAGIC   THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Day 3

# COMMAND ----------

from pyspark.sql.functions import current_timestamp
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/drivers_merge")

deltaTable.alias("tgt").merge(
    drivers_day3_df.alias("upd"),
    "tgt.driverId = upd.driverId") \
  .whenMatchedUpdate(set = { "dob" : "upd.dob", "forename" : "upd.forename", "surname" : "upd.surname", "updatedDate": "current_timestamp()" } ) \
  .whenNotMatchedInsert(values =
    {
      "driverId": "upd.driverId",
      "dob": "upd.dob",
      "forename" : "upd.forename", 
      "surname" : "upd.surname", 
      "createdDate": "current_timestamp()"
    }
  ) \
  .execute()

# COMMAND ----------

# MAGIC %sql SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC 1. History & Versioning
# MAGIC 2. Time Travel
# MAGIC 3. Vaccum

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

df = spark.read.format("delta").option("timestampAsOf", '2021-06-23T15:40:33.000+0000').load("/mnt/formula1dl/demo/drivers_merge")

# COMMAND ----------

display(df)

# COMMAND ----------

# MAGIC %sql
# MAGIC VACUUM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SET spark.databricks.delta.retentionDurationCheck.enabled = false;
# MAGIC VACUUM f1_demo.drivers_merge RETAIN 0 HOURS

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM f1_demo.drivers_merge WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql 
# MAGIC SELECT * FROM f1_demo.drivers_merge VERSION AS OF 3;

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO f1_demo.drivers_merge tgt
# MAGIC USING f1_demo.drivers_merge VERSION AS OF 3 src
# MAGIC    ON (tgt.driverId = src.driverId)
# MAGIC WHEN NOT MATCHED THEN
# MAGIC    INSERT *

# COMMAND ----------

# MAGIC %sql DESC HISTORY f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %md
# MAGIC Transaction Logs

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_txn (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING DELTA

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 1;

# COMMAND ----------

# MAGIC %sql
# MAGIC DESC HISTORY f1_demo.drivers_txn

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge
# MAGIC WHERE driverId = 2;

# COMMAND ----------

# MAGIC %sql
# MAGIC DELETE FROM  f1_demo.drivers_txn
# MAGIC WHERE driverId = 1;

# COMMAND ----------

for driver_id in range(3, 20):
  spark.sql(f"""INSERT INTO f1_demo.drivers_txn
                SELECT * FROM f1_demo.drivers_merge
                WHERE driverId = {driver_id}""")

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_txn
# MAGIC SELECT * FROM f1_demo.drivers_merge;

# COMMAND ----------

# MAGIC %md
# MAGIC Convert Parquet to Delta

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS f1_demo.drivers_convert_to_delta (
# MAGIC driverId INT,
# MAGIC dob DATE,
# MAGIC forename STRING, 
# MAGIC surname STRING,
# MAGIC createdDate DATE, 
# MAGIC updatedDate DATE
# MAGIC )
# MAGIC USING PARQUET

# COMMAND ----------

# MAGIC %sql
# MAGIC INSERT INTO f1_demo.drivers_convert_to_delta
# MAGIC SELECT * FROM f1_demo.drivers_merge

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA f1_demo.drivers_convert_to_delta

# COMMAND ----------

df = spark.table("f1_demo.drivers_convert_to_delta")

# COMMAND ----------

df.write.format("parquet").save("/mnt/formula1dl/demo/drivers_convert_to_delta_new")

# COMMAND ----------

# MAGIC %sql
# MAGIC CONVERT TO DELTA parquet.`/mnt/formula1dl/demo/drivers_convert_to_delta_new`

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/10.delta_lake_demo
USE f1_presentation;

-- COMMAND ----------

DESC driver_standings

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2018
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2018

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_driver_standings_2020
AS
SELECT race_year, driver_name, team, total_points, wins, rank
  FROM driver_standings
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT * FROM v_driver_standings_2020;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Inner Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Left Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  LEFT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Right Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  RIGHT JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Full Join
-- MAGIC

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  FULL JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Semi Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  SEMI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md Anti Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  ANTI JOIN v_driver_standings_2020 d_2020
    ON (d_2018.driver_name = d_2020.driver_name)

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC Cross Join

-- COMMAND ----------

SELECT *
  FROM v_driver_standings_2018 d_2018
  CROSS JOIN v_driver_standings_2020 d_2020

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/9.sql_joins_demo
# MAGIC %run "../includes/configuration"

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races")

# COMMAND ----------

races_filtered_df = races_df.filter("race_year = 2019 and round <= 5")

# COMMAND ----------

races_filtered_df = races_df.where((races_df["race_year"] == 2019) & (races_df["round"] <= 5))

# COMMAND ----------

display(races_filtered_df)

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/1.filter_demo
# MAGIC %sql
# MAGIC SELECT * FROM v_race_results

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT *
# MAGIC   FROM global_temp.gv_race_results;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/5.sql_temp_view_demo
-- MAGIC %md
-- MAGIC ##### Lesson Objectives
-- MAGIC 1. Spark SQL documentation
-- MAGIC 1. Create Database demo
-- MAGIC 1. Data tab in the UI
-- MAGIC 1. SHOW command
-- MAGIC 1. DESCRIBE command
-- MAGIC 1. Find the current database

-- COMMAND ----------

CREATE DATABASE demo;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS demo;

-- COMMAND ----------

SHOW databases;

-- COMMAND ----------

DESCRIBE DATABASE demo; 

-- COMMAND ----------

DESCRIBE DATABASE EXTENDED demo; 

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

USE demo;

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Managed Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create managed table using Python
-- MAGIC 1. Create managed table using SQL
-- MAGIC 1. Effect of dropping a managed table
-- MAGIC 1. Describe table 

-- COMMAND ----------

-- MAGIC %run "../includes/configuration"

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").saveAsTable("demo.race_results_python")

-- COMMAND ----------

USE demo;
SHOW TABLES;

-- COMMAND ----------

DESC EXTENDED race_results_python;

-- COMMAND ----------

SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

CREATE TABLE demo.race_results_sql
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2020;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

DESC EXTENDED demo.race_results_sql;

-- COMMAND ----------

DROP TABLE demo.race_results_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### External Tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create external table using Python
-- MAGIC 1. Create external table using SQL
-- MAGIC 1. Effect of dropping an external table

-- COMMAND ----------

-- MAGIC %python
-- MAGIC race_results_df.write.format("parquet").option("path", f"{presentation_folder_path}/race_results_ext_py").saveAsTable("demo.race_results_ext_py")

-- COMMAND ----------

DESC EXTENDED demo.race_results_ext_py

-- COMMAND ----------

CREATE TABLE demo.race_results_ext_sql
(race_year INT,
race_name STRING,
race_date TIMESTAMP,
circuit_location STRING,
driver_name STRING,
driver_number INT,
driver_nationality STRING,
team STRING,
grid INT,
fastest_lap INT,
race_time STRING,
points FLOAT,
position INT,
created_date TIMESTAMP
)
USING parquet
LOCATION "/mnt/formula1dl/presentation/race_results_ext_sql"

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

INSERT INTO demo.race_results_ext_sql
SELECT * FROM demo.race_results_ext_py WHERE race_year = 2020;

-- COMMAND ----------

SELECT COUNT(1) FROM demo.race_results_ext_sql;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

DROP TABLE demo.race_results_ext_sql

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Views on tables
-- MAGIC ##### Learning Objectives
-- MAGIC 1. Create Temp View
-- MAGIC 1. Create Global Temp View
-- MAGIC 1. Create Permanent View

-- COMMAND ----------

SELECT CURRENT_DATABASE();

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2018;

-- COMMAND ----------

SELECT * FROM v_race_results;

-- COMMAND ----------

CREATE OR REPLACE GLOBAL TEMP VIEW gv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2012;

-- COMMAND ----------

SELECT * FROM global_temp.gv_race_results

-- COMMAND ----------

SHOW TABLES IN global_temp;

-- COMMAND ----------

CREATE OR REPLACE VIEW demo.pv_race_results
AS
SELECT *
  FROM demo.race_results_python
 WHERE race_year = 2000;

-- COMMAND ----------

SHOW TABLES IN demo;

-- COMMAND ----------

SELECT * FROM demo.pv_race_results;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/6.sql_objects_demo
# MAGIC %md
# MAGIC ##### Spark Join Transformation

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.filter("circuit_id < 70") \
.withColumnRenamed("name", "circuit_name")

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races").filter("race_year = 2019") \
.withColumnRenamed("name", "race_name")

# COMMAND ----------

display(circuits_df)

# COMMAND ----------

display(races_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Inner Join

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "inner") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Outer Joins

# COMMAND ----------

# Left Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "left") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# right Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "right") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# full Outer Join
race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "full") \
.select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Semi Joins

# COMMAND ----------

race_circuits_df = circuits_df.join(races_df, circuits_df.circuit_id == races_df.circuit_id, "semi") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Anti Joins

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, circuits_df.circuit_id == races_df.circuit_id, "anti") 

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Cross Joins

# COMMAND ----------

race_circuits_df = races_df.crossJoin(circuits_df)

# COMMAND ----------

display(race_circuits_df)

# COMMAND ----------

race_circuits_df.count()

# COMMAND ----------

int(races_df.count()) * int(circuits_df.count())

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/2.join_demo
# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC #### Aggregate functions demo

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Built-in Aggregate functions

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

display(race_results_df)

# COMMAND ----------

demo_df = race_results_df.filter("race_year=2020")

# COMMAND ----------

display(demo_df)

# COMMAND ----------

from pyspark.sql.functions import count, countDistinct, sum

# COMMAND ----------

demo_df.select(count("*")).show()

# COMMAND ----------

demo_df.select(count("race_name")).show()

# COMMAND ----------

demo_df.select(countDistinct("race_name")).show()

# COMMAND ----------

demo_df.select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points")).show()

# COMMAND ----------

demo_df.filter("driver_name = 'Lewis Hamilton'").select(sum("points"), countDistinct("race_name")) \
.withColumnRenamed("sum(points)", "total_points") \
.withColumnRenamed("count(DISTINCT race_name)", "number_of_races") \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### groupBy

# COMMAND ----------

demo_df\
.groupBy("driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) \
.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Window Functions

# COMMAND ----------

demo_df = race_results_df.filter("race_year in (2019, 2020)")

# COMMAND ----------

demo_grouped_df = demo_df\
.groupBy("race_year", "driver_name") \
.agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) 

# COMMAND ----------

display(demo_grouped_df)

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank

driverRankSpec = Window.partitionBy("race_year").orderBy(desc("total_points"))
demo_grouped_df.withColumn("rank", rank().over(driverRankSpec)).show(100)

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/3.aggregation_demo
SHOW DATABASES;

-- COMMAND ----------

SELECT CURRENT_DATABASE()

-- COMMAND ----------

USE f1_processed;

-- COMMAND ----------

SHOW TABLES;

-- COMMAND ----------

SELECT *
 FROM drivers;

-- COMMAND ----------

DESC drivers;

-- COMMAND ----------

SELECT *
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob AS date_of_birth
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01';

-- COMMAND ----------

SELECT name, dob 
 FROM drivers
WHERE nationality = 'British'
  AND dob >= '1990-01-01'
ORDER BY dob DESC;

-- COMMAND ----------

SELECT *
  FROM drivers
 ORDER BY nationality ASC,
          dob DESC;

-- COMMAND ----------

SELECT name, nationality,dob 
 FROM drivers
WHERE (nationality = 'British'
  AND dob >= '1990-01-01') 
   OR nationality = 'Indian'
ORDER BY dob DESC;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/demo/7.sql_basics_demo
from pyspark.sql.functions import current_timestamp
def add_ingestion_date(input_df):
  output_df = input_df.withColumn("ingestion_date", current_timestamp())
  return output_df

# COMMAND ----------

def re_arrange_partition_column(input_df, partition_column):
  column_list = []
  for column_name in input_df.schema.names:
    if column_name != partition_column:
      column_list.append(column_name)
  column_list.append(partition_column)
  output_df = input_df.select(column_list)
  return output_df

# COMMAND ----------

def overwrite_partition(input_df, db_name, table_name, partition_column):
  output_df = re_arrange_partition_column(input_df, partition_column)
  spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    output_df.write.mode("overwrite").insertInto(f"{db_name}.{table_name}")
  else:
    output_df.write.mode("overwrite").partitionBy(partition_column).format("parquet").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------

def df_column_to_list(input_df, column_name):
  df_row_list = input_df.select(column_name) \
                        .distinct() \
                        .collect()
  
  column_value_list = [row[column_name] for row in df_row_list]
  return column_value_list

# COMMAND ----------

def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):
  spark.conf.set("spark.databricks.optimizer.dynamicPartitionPruning","true")

  from delta.tables import DeltaTable
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    deltaTable = DeltaTable.forPath(spark, f"{folder_path}/{table_name}")
    deltaTable.alias("tgt").merge(
        input_df.alias("src"),
        merge_condition) \
      .whenMatchedUpdateAll()\
      .whenNotMatchedInsertAll()\
      .execute()
  else:
    input_df.write.mode("overwrite").partitionBy(partition_column).format("delta").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/includes/common_functions
raw_folder_path = "/mnt/formula1dl/raw"
processed_folder_path = "/mnt/formula1dl/processed"
presentation_folder_path = "/mnt/formula1dl/presentation"

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/includes/configuration
-- MAGIC %md
-- MAGIC ##### Drop all the tables

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_processed CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed";

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_presentation CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation";

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/utils/1.prepare_for_incremental_load
CREATE DATABASE IF NOT EXISTS f1_raw;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for CSV files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create circuits table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.circuits;
CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,
circuitRef STRING,
name STRING,
location STRING,
country STRING,
lat DOUBLE,
lng DOUBLE,
alt INT,
url STRING
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/circuits.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.circuits;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create races table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.races;
CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT,
year INT,
round INT,
circuitId INT,
name STRING,
date DATE,
time STRING,
url STRING)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/races.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.races;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for JSON files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create constructors table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.constructors;
CREATE TABLE IF NOT EXISTS f1_raw.constructors(
constructorId INT,
constructorRef STRING,
name STRING,
nationality STRING,
url STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/constructors.json")

-- COMMAND ----------

SELECT * FROM f1_raw.constructors;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create drivers table
-- MAGIC * Single Line JSON
-- MAGIC * Complex structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.drivers;
CREATE TABLE IF NOT EXISTS f1_raw.drivers(
driverId INT,
driverRef STRING,
number INT,
code STRING,
name STRUCT<forename: STRING, surname: STRING>,
dob DATE,
nationality STRING,
url STRING)
USING json
OPTIONS (path "/mnt/formula1dl/raw/drivers.json")

-- COMMAND ----------

-- MAGIC %md ##### Create results table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.results;
CREATE TABLE IF NOT EXISTS f1_raw.results(
resultId INT,
raceId INT,
driverId INT,
constructorId INT,
number INT,grid INT,
position INT,
positionText STRING,
positionOrder INT,
points INT,
laps INT,
time STRING,
milliseconds INT,
fastestLap INT,
rank INT,
fastestLapTime STRING,
fastestLapSpeed FLOAT,
statusId STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/results.json")

-- COMMAND ----------

SELECT * FROM f1_raw.results

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create pit stops table
-- MAGIC * Multi Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.pit_stops;
CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(
driverId INT,
duration STRING,
lap INT,
milliseconds INT,
raceId INT,
stop INT,
time STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/pit_stops.json", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.pit_stops;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for list of files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Lap Times Table
-- MAGIC * CSV file
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.lap_times;
CREATE TABLE IF NOT EXISTS f1_raw.lap_times(
raceId INT,
driverId INT,
lap INT,
position INT,
time STRING,
milliseconds INT
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/lap_times")

-- COMMAND ----------

SELECT * FROM f1_raw.lap_times

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Qualifying Table
-- MAGIC * JSON file
-- MAGIC * MultiLine JSON
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.qualifying;
CREATE TABLE IF NOT EXISTS f1_raw.qualifying(
constructorId INT,
driverId INT,
number INT,
position INT,
q1 STRING,
q2 STRING,
q3 STRING,
qualifyId INT,
raceId INT)
USING json
OPTIONS (path "/mnt/formula1dl/raw/qualifying", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.qualifying

-- COMMAND ----------

DESC EXTENDED f1_raw.qualifying;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Z-End of Course/formula1/raw/1.create_raw_tables
# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

display(driver_standings_df.filter("race_year = 2020"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

display(final_df.filter("race_year = 2020"))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/driver_standings")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-14/trans/2.driver_standings
# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

drivers_df = spark.read.parquet(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.parquet(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.parquet(f"{processed_folder_path}/results") \
.withColumnRenamed("time", "race_time") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position") \
                          .withColumn("created_date", current_timestamp())

# COMMAND ----------

display(final_df.filter("race_year == 2020 and race_name == 'Abu Dhabi Grand Prix'").orderBy(final_df.points.desc()))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-14/trans/1.race_results
# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

display(constructor_standings_df.filter("race_year = 2020"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

display(final_df.filter("race_year = 2020"))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/constructor_standings")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-14/trans/3.constructor_standings
dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

drivers_df = spark.read.parquet(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.parquet(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.parquet(f"{processed_folder_path}/results") \
.filter(f"file_date = '{v_file_date}'") \
.withColumnRenamed("time", "race_time") \
.withColumnRenamed("race_id", "result_race_id") \
.withColumnRenamed("file_date", "result_file_date") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.result_race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_id", "race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position", "result_file_date") \
                          .withColumn("created_date", current_timestamp()) \
                          .withColumnRenamed("result_file_date", "file_date")

# COMMAND ----------

overwrite_partition(final_df, 'f1_presentation', 'race_results', 'race_id')

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/1.race_results
USE f1_processed;

-- COMMAND ----------

CREATE TABLE f1_presentation.calculated_race_results
USING parquet
AS
SELECT races.race_year,
       constructors.name AS team_name,
       drivers.name AS driver_name,
       results.position,
       results.points,
       11 - results.position AS calculated_points
  FROM results 
  JOIN f1_processed.drivers ON (results.driver_id = drivers.driver_id)
  JOIN f1_processed.constructors ON (results.constructor_id = constructors.constructor_id)
  JOIN f1_processed.races ON (results.race_id = races.race_id)
 WHERE results.position <= 10

-- COMMAND ----------

SELECT * FROM f1_presentation.calculated_race_results

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/4.calculated_race_results
# MAGIC %md
# MAGIC ##### Produce constructor standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

overwrite_partition(final_df, 'f1_presentation', 'constructor_standings', 'race_year')

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/3.constructor_standings
CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation"

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/0.create_presentation_database
# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %md
# MAGIC Find race years for which the data is to be reprocessed

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(f"file_date = '{v_file_date}'") 

# COMMAND ----------

race_year_list = df_column_to_list(race_results_df, 'race_year')

# COMMAND ----------

from pyspark.sql.functions import col

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results") \
.filter(col("race_year").isin(race_year_list))

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

overwrite_partition(final_df, 'f1_presentation', 'driver_standings', 'race_year')

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/trans/2.driver_standings
from pyspark.sql.functions import current_timestamp
def add_ingestion_date(input_df):
  output_df = input_df.withColumn("ingestion_date", current_timestamp())
  return output_df_

# COMMAND ----------

def re_arrange_partition_column(input_df, partition_column):
  column_list = []
  for column_name in input_df.schema.names:
    if column_name != partition_column:
      column_list.append(column_name)
  column_list.append(partition_column)
  output_df = input_df.select(column_list)
  return output_df

# COMMAND ----------

def overwrite_partition(input_df, db_name, table_name, partition_column):
  output_df = re_arrange_partition_column(input_df, partition_column)
  spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")
  if (spark._jsparkSession.catalog().tableExists(f"{db_name}.{table_name}")):
    output_df.write.mode("overwrite").insertInto(f"{db_name}.{table_name}")
  else:
    output_df.write.mode("overwrite").partitionBy(partition_column).format("parquet").saveAsTable(f"{db_name}.{table_name}")

# COMMAND ----------

def df_column_to_list(input_df, column_name):
  df_row_list = input_df.select(column_name) \
                        .distinct() \
                        .collect()
  
  column_value_list = [row[column_name] for row in df_row_list]
  return column_value_list

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/includes/common_functions
raw_folder_path = "/mnt/formula1dl/raw"
processed_folder_path = "/mnt/formula1dl/processed"
presentation_folder_path = "/mnt/formula1dl/presentation"

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/includes/configuration
CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/9.create_processed_database
# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/{v_file_date}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source)) \
                                             .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.constructors")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.constructors;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/3.ingest_constructors_file
# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/{v_file_date}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.drivers")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.drivers

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/4.ingest_drivers_file
# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

overwrite_partition(final_df, 'f1_processed', 'pit_stops', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/6.ingest_pit_stops_file
# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("parquet").saveAsTable("f1_processed.races")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.races;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/2.ingest_races_file
# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/{v_file_date}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

overwrite_partition(final_df, 'f1_processed', 'qualifying', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/8.ingest_qualifying_file
# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

overwrite_partition(final_df, 'f1_processed', 'lap_times', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/7.ingest_lap_times_file
# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-21")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/{v_file_date}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source)) \
.withColumn("file_date", lit(v_file_date))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/1.ingest_circuits_file
v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API", "p_file_date": "2021-04-18"})

# COMMAND ----------

v_result

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/0.ingest_all_files
# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

dbutils.widgets.text("p_file_date", "2021-03-28")
v_file_date = dbutils.widgets.get("p_file_date")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/{v_file_date}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source)) \
                                    .withColumn("file_date", lit(v_file_date))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

# MAGIC %md
# MAGIC Method 1

# COMMAND ----------

# for race_id_list in results_final_df.select("race_id").distinct().collect():
#   if (spark._jsparkSession.catalog().tableExists("f1_processed.results")):
#     spark.sql(f"ALTER TABLE f1_processed.results DROP IF EXISTS PARTITION (race_id = {race_id_list.race_id})")

# COMMAND ----------

# results_final_df.write.mode("append").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

# MAGIC %md
# MAGIC Method 2

# COMMAND ----------

overwrite_partition(results_final_df, 'f1_processed', 'results', 'race_id')

# COMMAND ----------

dbutils.notebook.exit("Success")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT race_id, COUNT(1) 
# MAGIC FROM f1_processed.results
# MAGIC GROUP BY race_id
# MAGIC ORDER BY race_id DESC;

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-19/ingestion/5.ingest_results_file
storage_account_name = "formula1dl"
client_id            = ""
tenant_id            = ""
client_secret        = ""

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

container_name = "raw"

dbutils.fs.mount(
  source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
  mount_point = f"/mnt/{storage_account_name}/{container_name}",
  extra_configs = configs)

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.mounts()

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-6
storage_account_name = "formula1dl"
client_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-id")
tenant_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-tenant-id")
client_secret        = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-secret")

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-9
# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv("/mnt/formula1dl/raw/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = lap_times_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/lap_times")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/7.ingest_lap_times_file
# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json("/mnt/formula1dl/raw/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("ingestion_date", current_timestamp()) 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_columns_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').parquet("/mnt/formula1dl/processed/results")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/5.ingest_results_file
# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv("/mnt/formula1dl/raw/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import current_timestamp, to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("ingestion_date", current_timestamp()) \
                                  .withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_timestamp_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode('overwrite').partitionBy('race_year').parquet('/mnt/formula1dl/processed/races')

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/2.ingest_races_file
# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json("/mnt/formula1dl/raw/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

constructor_final_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/constructors")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/3.ingest_constructors_file
# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json("/mnt/formula1dl/raw/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, current_timestamp, lit

# COMMAND ----------

drivers_with_columns_df = drivers_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("ingestion_date", current_timestamp()) \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname")))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/drivers")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/4.ingest_drivers_file
# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv("/mnt/formula1dl/raw/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") 

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

circuits_final_df = circuits_renamed_df.withColumn("ingestion_date", current_timestamp()) 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/circuits")

# COMMAND ----------

display(spark.read.parquet("/mnt/formula1dl/processed/circuits"))

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/1.ingest_circuits_file
# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json("/mnt/formula1dl/raw/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = pit_stops_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/pit_stops")

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/6.ingest_pit_stops_file
# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json("/mnt/formula1dl/raw/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = qualifying_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/qualifying")

# COMMAND ----------

display(spark.read.parquet('/mnt/formula1dl/processed/qualifying'))

# COMMAND ----------


/Users/zacayd@octopai.com/Formula1-Project-Solutions/Section-09-10-11/ingestion/8.ingest_qualifying_file
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC     
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------



# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)



from pyspark.sql.functions import col
# File location and type
file_location = "/FileStore/tables/circuits-3.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

# spark.read\
#     .option("header","true")\
#     .option("inferschema","true")\
#     .csv("/FileStore/tables/circuits-2.csv")\
#     .write\
#     .mode('overwrite')\
#     .csv("/FileStore/tables/ind-2.csv")
    
df=spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")

#df.printSchema()

df=df.withColumn("CXX", col("lat"))
df.createOrReplaceTempView("Data")
df1=spark.sql("select *from Data")
df1.write.mode("overwrite").csv("mycsv.csv")





# COMMAND ----------


/Users/zacayd@octopai.com/pythonz
# MAGIC %scala
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
# MAGIC import za.co.absa.spline.producer.model.v1_1._
# MAGIC import za.co.absa.spline.producer.model._
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import scala.concurrent.duration.Duration
# MAGIC import scala.util.{Failure, Success, Try}
# MAGIC
# MAGIC val splineConf: Configuration = StandardSplineConfigurationStack(spark)
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
# MAGIC   //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
# MAGIC   //val test = dbutils.notebook.getContext.notebookPath
# MAGIC   val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC   val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC   val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC
# MAGIC   val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC   val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC   val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC
# MAGIC
# MAGIC
# MAGIC   val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC   val user = tagMap("user")
# MAGIC   val name = notebookPath(notebookPath.size-1)
# MAGIC   
# MAGIC   val notebookInfo = Map("notebookURL" -> notebookURL,  
# MAGIC                 "user" -> user, 
# MAGIC                 "workspaceName" ->workspaceName,
# MAGIC                 "name" -> name,
# MAGIC                 "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC                 "timestamp" -> System.currentTimeMillis)
# MAGIC   val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC   
# MAGIC   override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
# MAGIC     override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
# MAGIC     override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
# MAGIC     override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
# MAGIC     override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
# MAGIC     override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
# MAGIC   })
# MAGIC })

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# Databricks notebook source
# MAGIC %md
# MAGIC ###### Purpose: Notebook to process SANDVINE BILLING RECON raw data
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |Mar 05, 2021|Shagufa Akhzar| Created the notebook to process Sandvine Billing raw data | 30 min |
# MAGIC |Mar 17, 2021|Shagufa Akhzar| Added alert code in case of invalid records | 30 min |
# MAGIC |Apr 15, 2021|Shagufa Akhzar| Added Area_Code_ID column and included TDD-LTE LOB data | 30 min |
# MAGIC |May 03, 2021|Shagufa Akhzar| Added ODS_Sandvine_Billing_Hourly insertion code | 30 min |

# COMMAND ----------

# DBTITLE 1,Create widgets to extract parameters from ADF
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("LogicAppURL", "")
DataFactoryName = dbutils.widgets.get("DataFactoryName")
PipelineName = dbutils.widgets.get("PipelineName")
PipelineRunId = dbutils.widgets.get("PipelineRunId")
windowStartTime = dbutils.widgets.get("windowStartTime")
LogicAppURL = dbutils.widgets.get("LogicAppURL")

# COMMAND ----------

# DBTITLE 1,Refresh tables
# MAGIC %sql
# MAGIC REFRESH TABLE dgcdimensions_dl.ContainerName;
# MAGIC REFRESH TABLE dgcdimensions_dl.AreaCode;
# MAGIC REFRESH TABLE stgods_dl.sandvinebilling_logs

# COMMAND ----------

# DBTITLE 1,Import required packages
import pyspark
import time
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from  pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit

# COMMAND ----------

# DBTITLE 1,Create dynamic variables 
date = windowStartTime[:10].replace('-','')
date_ts = windowStartTime.replace('-','').replace(':','')
table_name = 'SandvineBilling_'+date_ts+'_tmp'
filelocation_lz = "/mnt/continuousfeeds/Sandvine_Billing_Recon/lz/"+date+"/"+date_ts+"/*/*/*/BILLING_RECON/lz/*"
filelocation_wrk = "/mnt/continuousfeeds/Sandvine_Billing_Recon/working/"+date+"/"+date_ts+"/*/*/BILLING_RECON/lz/*"

# COMMAND ----------

# DBTITLE 1,Schema Enforcement
schema = StructType([ \
     StructField("SUBSCRIBER_NUMBER",StringType(),True), \
     StructField("PLAN_NAME",StringType(),True), \
     StructField("PLAN_TYPE",StringType(),True), \
     StructField("CHARGING_CHARACTERISTICS",StringType(),True), \
     StructField("APN",StringType(),True), \
     StructField("SESSION_ID",StringType(),True), \
     StructField("SEQUENCE_NUMBER",StringType(),True), \
     StructField("START_TIME",StringType(),True), \
     StructField("END_TIME",StringType(),True), \
     StructField("PDP_TYPE",StringType(),True), \
     StructField("FRAME_DLP",StringType(),True), \
     StructField("IMSI",StringType(),True), \
     StructField("IMEISV",StringType(),True), \
     StructField("PROTOCOL",StringType(),True), \
     StructField("SGSN_MCC_MNC",StringType(),True), \
     StructField("SGSN_ADDRESS",StringType(),True), \
     StructField("GGSN_ADDRESS",StringType(),True), \
     StructField("USER_LOCATION",StringType(),True), \
     StructField("RAT_TYPE",StringType(),True), \
     StructField("QOS",StringType(),True), \
     StructField("ORIGIN_HOST",StringType(),True), \
     StructField("RATING_GROUP",StringType(),True), \
     StructField("TX_BYTES",StringType(),True), \
     StructField("RX_BYTES",StringType(),True), \
     StructField("TOTAL_BYTES",StringType(),True), \
     StructField("SERVICE_ID",StringType(),True), \
     StructField("CHARGING_ID",StringType(),True), \
     StructField("corruptrecord",StringType(),True)\
   ])                 
sbilling_withschema = spark.read.format("csv")\
  .option("header", "false")\
  .schema(schema)\
  .option("sep", "|")\
  .load(filelocation_wrk)\

svb_withheaderfooter = sbilling_withschema.withColumn("LOAD_TS",lit(windowStartTime)).withColumn("FileName", input_file_name()).withColumn("Last_Updated_by",lit("BAU")).withColumn("Data_dt",lit(date)).withColumn("Error_dt",lit(date))

# COMMAND ----------

# DBTITLE 1,Create temp view
svb_withheaderfooter.createOrReplaceTempView("temp_sandvinebilling")

# COMMAND ----------

# DBTITLE 1,Use database SandvineBilling_dl
# MAGIC %sql
# MAGIC USE SandvineBilling_dl;

# COMMAND ----------

# DBTITLE 1,Remove managed table
dbutils.fs.rm("dbfs:/user/hive/warehouse/sandvinebilling_dl/"+table_name,True)

# COMMAND ----------

# DBTITLE 1,Create main SandvineBilling temp table 
if spark._jsparkSession.catalog().tableExists('sandvinebilling_dl', table_name):
  print("Table already exists")
else:
  spark.sql('''CREATE TABLE '''+table_name+''' USING DELTA PARTITIONED BY(Container_Name) WITH CTE_SVB_SUBS
AS (
	SELECT CAST(a.SUBSCRIBER_NUMBER AS DECIMAL(16, 0)) AS SUBSCRIBER_NUMBER
		,a.PLAN_NAME
		,a.PLAN_TYPE
		,a.CHARGING_CHARACTERISTICS
		,a.APN
		,a.SESSION_ID
		,a.SEQUENCE_NUMBER
		,a.START_TIME
		,a.END_TIME
		,a.PDP_TYPE
		,a.FRAME_DLP
        ,a.IMSI
        ,a.IMEISV
		,a.PROTOCOL
		,a.SGSN_MCC_MNC
		,a.SGSN_ADDRESS
		,a.GGSN_ADDRESS
		,a.USER_LOCATION
		,a.RAT_TYPE
		,a.QOS
		,a.ORIGIN_HOST
		,a.RATING_GROUP
        ,a.TX_BYTES
        ,a.RX_BYTES
        ,a.TOTAL_BYTES
        ,a.SERVICE_ID  
        ,a.CHARGING_ID 
		,a.corruptrecord
		,a.LOAD_TS
		,CONCAT(REPLACE(SUBSTRING_INDEX(REPLACE(SUBSTRING(a.FileName, POSITION('/BILLING_RECON/lz/' IN a.FileName)), '/BILLING_RECON/lz/', ''), '/', 1), '%20', ' '),'.tgz') AS FileName
		,a.Last_Updated_by
		,a.Data_dt
		,a.Error_dt
		,CASE 
			WHEN length(CAST(a.SUBSCRIBER_NUMBER AS DECIMAL(16, 0))) BETWEEN 10
					AND 12
				AND CAST(a.SUBSCRIBER_NUMBER AS DECIMAL(16, 0)) IS NOT NULL
				AND a.corruptrecord IS NULL
				THEN 1
			ELSE 0
			END AS IsValid
	FROM temp_sandvinebilling a
	)
	,cte_dis_subs
AS (
	SELECT DISTINCT SUBSCRIBER_NUMBER
	FROM CTE_SVB_SUBS
	WHERE SUBSCRIBER_NUMBER IS NOT NULL
	)
	,cte_dis_marketcode
AS (
	SELECT DISTINCT TRIM(MARKET_ID) AS MARKET_ID,TRIM(MARKET_SH_CODE1) AS MARKET_SH_CODE1,TRIM(AREA_CODE_ID) AS AREA_CODE_ID
	FROM dgcdimensions_dl.AreaCode
	WHERE LOWER(TRIM(LINE_OF_BUSINESS)) IN ('gsm','tdd-lte')
	)
    SELECT a.SUBSCRIBER_NUMBER
	,a.PLAN_NAME
	,a.PLAN_TYPE
	,a.CHARGING_CHARACTERISTICS
	,a.APN
	,a.SESSION_ID
	,a.SEQUENCE_NUMBER
	,a.START_TIME
	,a.END_TIME
	,a.PDP_TYPE
	,a.FRAME_DLP
    ,CASE 
          WHEN a.IMSI IS NULL 
             OR a.IMSI = "" 
             OR a.IMSI = "null" 
             THEN '000000000000000' 
          ELSE a.IMSI 
          END AS IMSI
     ,CASE 
          WHEN a.IMEISV IS NULL 
             OR a.IMEISV = "" 
             OR a.IMEISV = "null" 
             THEN '0000000000000000' 
          ELSE a.IMEISV 
          END AS IMEISV
	,a.PROTOCOL
	,a.SGSN_MCC_MNC
	,a.SGSN_ADDRESS
	,a.GGSN_ADDRESS
	,a.USER_LOCATION
	,a.RAT_TYPE
	,a.QOS
	,a.ORIGIN_HOST
	,a.RATING_GROUP
    ,CASE 
          WHEN a.TX_BYTES IS NULL 
             OR a.TX_BYTES = "" 
             OR a.TX_BYTES = "null" 
             THEN '0' 
          ELSE a.TX_BYTES 
          END AS TX_BYTES
    ,CASE 
          WHEN a.RX_BYTES IS NULL 
             OR a.RX_BYTES = "" 
             OR a.RX_BYTES = "null" 
             THEN '0' 
          ELSE a.RX_BYTES 
          END AS RX_BYTES
    ,CASE 
          WHEN a.TOTAL_BYTES IS NULL 
             OR a.TOTAL_BYTES = "" 
             OR a.TOTAL_BYTES = "null" 
             THEN '0' 
          ELSE a.TOTAL_BYTES 
          END AS TOTAL_BYTES
    ,CASE 
          WHEN a.SERVICE_ID IS NULL 
             OR a.SERVICE_ID = "" 
             OR a.SERVICE_ID = "null" 
             THEN '-1' 
          ELSE a.SERVICE_ID 
          END AS SERVICE_ID  
    ,CASE 
          WHEN a.CHARGING_ID IS NULL 
             OR a.CHARGING_ID = "" 
             OR a.CHARGING_ID = "null" 
             THEN '-1' 
          ELSE a.CHARGING_ID 
          END AS CHARGING_ID 
	,a.corruptrecord
	,a.LOAD_TS
	,a.FileName
	,a.Last_Updated_by
	,a.Data_dt
	,a.Error_dt
	,a.IsValid
	,IFNULL(b.MARKET_ID, - 1) AS MARKET_ID
	,IFNULL(TRIM(c.Container_Name), 'group') AS Container_Name
    ,IFNULL(b.AREA_CODE_ID, 0) AS AREA_CODE_ID
FROM CTE_SVB_SUBS a
INNER JOIN cte_dis_subs cd ON a.SUBSCRIBER_NUMBER = cd.SUBSCRIBER_NUMBER 
LEFT JOIN cte_dis_marketcode AS b ON substring(cd.SUBSCRIBER_NUMBER, 1, 5) = b.MARKET_SH_CODE1 
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 4) = b.MARKET_SH_CODE1
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 3) = b.MARKET_SH_CODE1
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 2) = b.MARKET_SH_CODE1
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 1) = b.MARKET_SH_CODE1
LEFT JOIN dgcdimensions_dl.ContainerName c ON b.MARKET_ID = TRIM(c.MARKET_ID)''')
  spark.sql("INSERT INTO stgods_dl.sandvinebilling_logs VALUES ('"+windowStartTime+"','"+date+"','N') ")

# COMMAND ----------

# DBTITLE 1,Delete and insert invalid records to error dir of dgcdatalake01 and send an alert
import json
import requests
Invalid_MSISDNs_cnt = spark.sql("SELECT COUNT(1) as cnt_InvalidMSISDNs FROM "+table_name+" WHERE IsValid = 0 AND corruptrecord IS NULL").first()["cnt_InvalidMSISDNs"]
Schema_Mismatch_cnt = spark.sql("SELECT COUNT(1) as cnt_SchemaMismatch FROM "+table_name+" WHERE IsValid = 0 AND corruptrecord IS NOT NULL").first()["cnt_SchemaMismatch"] 
if(Invalid_MSISDNs_cnt != 0 or Schema_Mismatch_cnt != 0):
  spark.sql("DELETE FROM error_dl.reject_SandvineBilling WHERE Error_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql('''
WITH CTE_NullRecords AS (
		SELECT *
		FROM temp_sandvinebilling
		WHERE SUBSCRIBER_NUMBER IS NULL
		)
INSERT INTO TABLE error_dl.reject_SandvineBilling
SELECT SUBSCRIBER_NUMBER
	,PLAN_NAME
	,PLAN_TYPE
	,CHARGING_CHARACTERISTICS
	,APN
	,SESSION_ID
	,SEQUENCE_NUMBER
	,START_TIME
	,END_TIME
	,PDP_TYPE
	,FRAME_DLP
	,IMSI
	,IMEISV
	,PROTOCOL
	,SGSN_MCC_MNC
	,SGSN_ADDRESS
	,GGSN_ADDRESS
	,USER_LOCATION
	,RAT_TYPE
	,QOS
	,ORIGIN_HOST
	,RATING_GROUP
	,TX_BYTES
	,RX_BYTES
	,TOTAL_BYTES
	,SERVICE_ID
	,CHARGING_ID
	,corruptrecord
	,LOAD_TS
	,FileName
	,Last_Updated_by
	,Error_dt
	,CASE 
		WHEN corruptrecord IS NULL
			THEN 'Invalid_MSISDNs'
		ELSE 'Schema_Mismatch'
		END AS Error_Type
FROM '''+table_name+'''
WHERE IsValid = 0
UNION
SELECT CAST(SUBSCRIBER_NUMBER AS DECIMAL(16, 0)) AS SUBSCRIBER_NUMBER
	,PLAN_NAME
	,PLAN_TYPE
	,CHARGING_CHARACTERISTICS
	,APN
	,SESSION_ID
	,SEQUENCE_NUMBER
	,START_TIME
	,END_TIME
	,PDP_TYPE
	,FRAME_DLP
	,CASE 
		WHEN IMSI IS NULL
			OR IMSI = ""
			OR IMSI = "null"
			THEN '000000000000000'
		ELSE IMSI
		END AS IMSI
	,CASE 
		WHEN IMEISV IS NULL
			OR IMEISV = ""
			OR IMEISV = "null"
			THEN '0000000000000000'
		ELSE IMEISV
		END AS IMEISV
	,PROTOCOL
	,SGSN_MCC_MNC
	,SGSN_ADDRESS
	,GGSN_ADDRESS
	,USER_LOCATION
	,RAT_TYPE
	,QOS
	,ORIGIN_HOST
	,RATING_GROUP
	,CASE 
		WHEN TX_BYTES IS NULL
			OR TX_BYTES = ""
			OR TX_BYTES = "null"
			THEN '0'
		ELSE TX_BYTES
		END AS TX_BYTES
	,CASE 
		WHEN RX_BYTES IS NULL
			OR RX_BYTES = ""
			OR RX_BYTES = "null"
			THEN '0'
		ELSE RX_BYTES
		END AS RX_BYTES
	,CASE 
		WHEN TOTAL_BYTES IS NULL
			OR TOTAL_BYTES = ""
			OR TOTAL_BYTES = "null"
			THEN '0'
		ELSE TOTAL_BYTES
		END AS TOTAL_BYTES
	,CASE 
		WHEN SERVICE_ID IS NULL
			OR SERVICE_ID = ""
			OR SERVICE_ID = "null"
			THEN '-1'
		ELSE SERVICE_ID
		END AS SERVICE_ID
	,CASE 
		WHEN CHARGING_ID IS NULL
			OR CHARGING_ID = ""
			OR CHARGING_ID = "null"
			THEN '-1'
		ELSE CHARGING_ID
		END AS CHARGING_ID
	,corruptrecord
	,LOAD_TS
	,CONCAT (
		REPLACE(SUBSTRING_INDEX(REPLACE(SUBSTRING(FileName, POSITION('/BILLING_RECON/lz/' IN FileName)), '/BILLING_RECON/lz/', ''), '/', 1), '%20', ' ')
		,'.tgz'
		) AS FileName
	,Last_Updated_by
	,Error_dt
	,'Invalid_MSISDNs' AS Error_Type
FROM CTE_NullRecords
''')
  data = '{ "title":"Error Directory Movement !!!","color":"red","DataFactoryName":"' +str(DataFactoryName)+ '","PipelineName":"' +str(PipelineName)+ '","PipelineRunID":"' +str(PipelineRunId)+ '","time":"' +str(windowStartTime)+ '","FileName":"Sandvine Billing Recon Erroneous Records At \'dgc{xxx}datalake01\'","error":{"Message":"Invalid Records","TableName":"error_dl.reject_SandvineBilling","Path":"/error/sandvine/reject_SandvineBilling","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt)+ '","Total Records": "' +str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+ '"}}}'
  response = requests.post(LogicAppURL, data=data,headers={"Content-Type": "application/json"})

# COMMAND ----------

# DBTITLE 1,Delete and insert valid data to dgcdatalake01 container wise
dfContName = spark.sql("SELECT DISTINCT Container_Name FROM "+table_name).rdd.flatMap(lambda x: x).collect()
for c in dfContName :
  spark.sql("DELETE FROM "+c+"_dl.SandvineBilling WHERE Data_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql("INSERT INTO TABLE "+c+"_dl.SandvineBilling SELECT SUBSCRIBER_NUMBER,PLAN_NAME,PLAN_TYPE,CHARGING_CHARACTERISTICS,APN,SESSION_ID,SEQUENCE_NUMBER,START_TIME,END_TIME,PDP_TYPE,FRAME_DLP,IMSI,IMEISV,PROTOCOL,SGSN_MCC_MNC,SGSN_ADDRESS,GGSN_ADDRESS,USER_LOCATION,RAT_TYPE,QOS,ORIGIN_HOST,RATING_GROUP,TX_BYTES,RX_BYTES,TOTAL_BYTES,SERVICE_ID,CHARGING_ID,LOAD_TS,FileName,Last_Updated_by,Data_dt,MARKET_ID FROM "+table_name+" WHERE IsValid=1 AND Container_Name = '"+c+"'")

# COMMAND ----------

# DBTITLE 1,Merge processed files into staging Audit Table
for i in range(1,5,1): 
  try:
    spark.sql('''
WITH CTE_StgAudit
AS (
	SELECT LOAD_TS
		,FileName
		,SUM(CASE 
				WHEN FileName IS NOT NULL
					AND IsValid = 1
					THEN 1
				ELSE 0
				END) AS RecordCount_FN
		,SUM(CASE 
				WHEN IsValid = 0
					AND corruptrecord IS NULL
					THEN 1
				ELSE 0
				END) AS cnt_InvalidMSISDNs
		,SUM(CASE 
				WHEN IsValid = 0
					AND corruptrecord IS NOT NULL
					THEN 1
				ELSE 0
				END) AS cnt_SchemaMismatch
	FROM '''+table_name+'''
	GROUP BY LOAD_TS
		,FileName
	)
MERGE INTO stgods_dl.stg_auditcontrol AS target
USING (
	SELECT DISTINCT LOAD_TS
		,FileName
		,RecordCount_FN
		,CONCAT (
			'{"Invalid MSISDNs":"'
			,cnt_InvalidMSISDNs
			,'","Schema Mismatch":"'
			,cnt_SchemaMismatch
			,'"}'
			) AS Error
        ,'Sandvine Billing' AS FeedName
	FROM CTE_StgAudit
	) AS source
	ON source.LOAD_TS = target.Load_TS
		AND source.FileName = target.FileName
        AND source.FeedName = target.FeedName 
WHEN MATCHED
	THEN
		UPDATE
		SET *
WHEN NOT MATCHED
	THEN
		INSERT * ''')
    break;
  except Exception as e:
    if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
      time.sleep(180)
      continue

# COMMAND ----------

# DBTITLE 1,List all the files from lz dir in a temp view
import glob
from pyspark.sql import Row
path=glob.glob("/dbfs"+filelocation_lz)
rdd1 = sc.parallelize(path)
row_rdd1 = rdd1.map(lambda x: Row(x))
df_files=sqlContext.createDataFrame(row_rdd1,['FileName'])
df_files.createOrReplaceTempView("SandvineBilling_AllFiles")

# COMMAND ----------

# DBTITLE 1,Merge empty files into staging Audit Table
for i in range(1,5,1): 
  try:
    spark.sql('''WITH CTE_Files
AS (
	SELECT REPLACE(SUBSTRING(FileName, POSITION('/BILLING_RECON/lz/' IN FileName)), '/BILLING_RECON/lz/', '') AS FileName
	FROM SandvineBilling_AllFiles
	EXCEPT
    SELECT Filename
	FROM '''+table_name+'''
	)
MERGE INTO stgods_dl.stg_auditcontrol AS target
USING (
	SELECT DISTINCT '''"'"+windowStartTime+"'"''' AS LOAD_TS
		,FileName
		,0 AS RecordCount_FN
		,CONCAT (
			'{"Invalid MSISDNs":"'
			,0
			,'","Schema Mismatch":"'
			,0
			,'"}'
			) AS Error
        ,'Sandvine Billing' AS FeedName
	FROM CTE_Files
	) AS source
	ON source.LOAD_TS = target.Load_TS
		AND source.FileName = target.FileName
        AND source.FeedName = target.FeedName
WHEN MATCHED
	THEN
		UPDATE
		SET *
WHEN NOT MATCHED
	THEN
		INSERT *''')
    break;
  except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(180)
        continue

# COMMAND ----------

# DBTITLE 1,Staging ODS layer processing
dflogs = spark.sql("SELECT DISTINCT Flag FROM stgods_dl.sandvinebilling_logs WHERE Data_dt = '"+date+"' AND Load_TS = '"+windowStartTime+"' ")
TT=dflogs.select("Flag").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
#   Delete and insert valid data to staging ods
  spark.sql("DELETE FROM stgods_dl.ODS_Sandvine_Billing WHERE Data_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql("INSERT INTO TABLE stgods_dl.ODS_Sandvine_Billing SELECT SUBSCRIBER_NUMBER,PLAN_NAME,PLAN_TYPE,CHARGING_CHARACTERISTICS,APN,SESSION_ID,SEQUENCE_NUMBER,START_TIME,END_TIME,PDP_TYPE,FRAME_DLP,IMSI,IMEISV,PROTOCOL,SGSN_MCC_MNC,SGSN_ADDRESS,GGSN_ADDRESS,USER_LOCATION,RAT_TYPE,QOS,ORIGIN_HOST,RATING_GROUP,TX_BYTES,RX_BYTES,TOTAL_BYTES,SERVICE_ID,CHARGING_ID,LOAD_TS,FileName,Last_Updated_by,Data_dt,MARKET_ID,AREA_CODE_ID FROM "+table_name+" WHERE IsValid=1")
#   Delete and insert valid data to staging ods(hourly)
  spark.sql("DELETE FROM stgods_dl.ODS_Sandvine_Billing_Hourly WHERE Data_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql("INSERT INTO TABLE stgods_dl.ODS_Sandvine_Billing_Hourly SELECT SUBSCRIBER_NUMBER,PLAN_NAME,PLAN_TYPE,CHARGING_CHARACTERISTICS,APN,SESSION_ID,SEQUENCE_NUMBER,START_TIME,END_TIME,PDP_TYPE,FRAME_DLP,IMSI,IMEISV,PROTOCOL,SGSN_MCC_MNC,SGSN_ADDRESS,GGSN_ADDRESS,USER_LOCATION,RAT_TYPE,QOS,ORIGIN_HOST,RATING_GROUP,TX_BYTES,RX_BYTES,TOTAL_BYTES,SERVICE_ID,CHARGING_ID,LOAD_TS,FileName,Last_Updated_by,Data_dt,MARKET_ID,AREA_CODE_ID FROM "+table_name+" WHERE IsValid=1")
#   Logging
  spark.sql("UPDATE stgods_dl.sandvinebilling_logs SET Flag = 'Y' WHERE Data_dt = '"+date+"' AND Load_TS = '"+windowStartTime+"' ")
else:
  print("Data already exists for this trigger time")

# COMMAND ----------

# DBTITLE 1,Drop table if exists
spark.sql("DROP TABLE IF EXISTS "+table_name)

# COMMAND ----------

# DBTITLE 1,Exit the notebook 
dbutils.notebook.exit("Exited")

/Users/zacayd@octopai.com/Sandvine_BillingRecon
# MAGIC %scala
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
# MAGIC import za.co.absa.spline.producer.model.v1_1._
# MAGIC import za.co.absa.spline.producer.model._
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import scala.concurrent.duration.Duration
# MAGIC import scala.util.{Failure, Success, Try}
# MAGIC
# MAGIC val splineConf: Configuration = StandardSplineConfigurationStack(spark)
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
# MAGIC   //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
# MAGIC   //val test = dbutils.notebook.getContext.notebookPath
# MAGIC   val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC   val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC   val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC
# MAGIC   val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC   val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC   val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC
# MAGIC
# MAGIC
# MAGIC   val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC   val user = tagMap("user")
# MAGIC   val name = notebookPath(notebookPath.size-1)
# MAGIC   
# MAGIC   val notebookInfo = Map("notebookURL" -> notebookURL,  
# MAGIC                 "user" -> user, 
# MAGIC                 "workspaceName" ->workspaceName,
# MAGIC                 "name" -> name,
# MAGIC                 "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC                 "timestamp" -> System.currentTimeMillis)
# MAGIC   val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC   
# MAGIC   override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
# MAGIC     override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
# MAGIC     override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
# MAGIC     override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
# MAGIC     override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
# MAGIC     override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
# MAGIC   })
# MAGIC })

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# %sql

# insert into 
# create table  ODS_DL.market
# (Spid int)
# /*ods_total_revenue

# pbi_azure_checks
# CFW*/

# COMMAND ----------

table='CFW.pbi_azure_checks'




# COMMAND ----------

spark.sql('insert into  ODS_DL.market(Spid) select ods_total_revenue from ' +table  )

/Users/zacayd@octopai.com/SqlExample
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

from pyspark.sql.functions import col








#kafka

# sc.setSystemProperty("spline.mode","REQUIRED")
# sc.setSystemProperty("spline.lineageDispatcher", "kafka")
# sc.setSystemProperty("spline.lineageDispatcher.kafka.className","za.co.absa.spline.harvester.dispatcher.KafkaLineageDispatcher")
# # Spline Producer API version
# sc.setSystemProperty("spline.lineageDispatcher.kafka.apiVersion","1.2")
# # producer configs as defined by kafka (bootstrap.servers, key.serializer, etc) all kafka configs are supported
# sc.setSystemProperty("spline.lineageDispatcher.kafka.producer.bootstrap.servers","192.168.100.11:9092")
# sc.setSystemProperty("spline.kafka.consumer.group.id","spline-group")

# sc.setSystemProperty("spline.lineageDispatcher.kafka.producer.key.serializer","org.apache.kafka.common.serialization.StringSerializer")
# sc.setSystemProperty("spline.lineageDispatcher.kafka.producer.value.serializer","org.apache.kafka.common.serialization.StringSerializer")
# sc.setSystemProperty("spline.lineageDispatcher.kafka.producer.max.in.flight.requests.per.connection","1")
# # topic name
# sc.setSystemProperty("spline.lineageDispatcher.kafka.topic","spline-topicy")


#http
# SC.setSystemProperty("spline.mode","REQUIRED")
# SC.setSystemProperty("spline.lineageDispatcher","http")
# SC.setSystemProperty("spline.lineageDispatcher.http.producer.url","http://192.168.100.11:8080/producer")




# sc.setSystemProperty("spark.spline.lineageDispatcher.http.producer.url", "http://192.168.100.11:8080/producer")




#system.setProperty("kafka.bootstrap.servers" , "my-kafka:9092");


# x=ping('192.168.100.11')
# print(x)

# File location and type
file_location = "/FileStore/tables/circuits-2.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")\
    .write\
    .mode('overwrite')\
    .csv("/FileStore/tables/ind-2.csv")
    
df=spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")

#df.printSchema()

df=df.withColumn("CXX", col("lat"))
df.show()

df.createOrReplaceTempView("Data")

df1=spark.sql("select *from Data")



df1.write.mode("overwrite").csv("mycsv.csv")




/Users/zacayd@octopai.com/Zacay
import za.co.absa.spline.harvester.SparkLineageInitializer._

spark.enableLineageTracking()

// COMMAND ----------

case class Student(id: Int, name: String, addrId: Int)

case class Address(id: Int, address: String)

Seq(
  Student(111, "Amy Smith", 1),
  Student(222, "Bob Brown", 2)
).toDS.write.mode("overwrite").parquet("/students")

Seq(
  Address(1, "123 Park Ave, San Jose"),
  Address(2, "456 Taylor St, Cupertino")
).toDS.write.mode("overwrite").parquet("/addresses")
/Users/zacayd@octopai.com/Scala
import za.co.absa.spline.harvester.SparkLineageInitializer._

spark.enableLineageTracking()

val students = spark.read.parquet("/students")
val addresses = spark.read.parquet("/addresses")

students
  .join(addresses)
  .where(addresses("id") === students("addrId"))
  .select("name", "address")
  .write.mode("append").parquet("/student_names_with_addresses")


/Users/zacayd@octopai.com/Scala2
-- create database lineage_data;

-- CREATE TABLE IF NOT EXISTS
--   lineage_data.menu (
--     recipe_id INT,
--     app string,
--     main string,
--     dessert string
--   );

-- INSERT INTO lineage_data.menu
--     (recipe_id, app, main, dessert)
-- VALUES
--     (1,"Ceviche", "Tacos", "Flan"),
--     (2,"Tomato Soup", "Souffle", "Creme Brulee"),
--     (3,"Chips","Grilled Cheese","Cheesecake");

-- CREATE TABLE
--   lineage_data.dinner
-- AS SELECT
--   recipe_id, concat(app," + ", main," + ",dessert)
-- AS
--   full_menu
-- FROM
--   lineage_data.menu
--   ;
  
  select* from lineage_data.dinner
/Users/zacayd@octopai.com/NoteBookTest
# MAGIC %md
# MAGIC
# MAGIC ## Overview
# MAGIC
# MAGIC This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.
# MAGIC
# MAGIC This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported.

# COMMAND ----------

#  from pyspark import SparkContext
# from pyspark.sql import SQLContext
# from pyspark.sql import SparkSession
# import pandas as pd

# SC = SparkContext
# SC.setSystemProperty('spline.mode','REQUIRED')
# SC.setSystemProperty('spline.producer.url', 'http://localhost:8080/producer')
#SC._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)


                     



# File location and type
file_location = "/FileStore/tables/circuits-2.csv"
file_type = "csv"

# CSV options
infer_schema = "false"
first_row_is_header = "true"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

display(df)

# COMMAND ----------

# Create a view or table

temp_table_name = "circuits-2_csv"

df.createOrReplaceTempView(temp_table_name)

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC /* Query the created temp table in a SQL cell */
# MAGIC
# MAGIC select * from `circuits-2_csv`

# COMMAND ----------

# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.
# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.
# To do so, choose your table name and uncomment the bottom line.

permanent_table_name = "circuits-2_csv"

# df.write.format("parquet").saveAsTable(permanent_table_name)
/Users/zacayd@octopai.com/2022-10-31 - DBFS Example
import za.co.absa.spline.harvester.SparkLineageInitializer._


System.setProperty("spline.lineageDispatcher","http")
System.setProperty("spline.lineageDispatcher.http.producer.url","http://10.0.19.4:8080/producer")

spark.enableLineageTracking()


// spark.spline.lineageDispatcher http
// spark.spline.lineageDispatcher.http.producer.url http://18.116.202.35:8080/producer
// spark.spline.mode REQUIRED



// COMMAND ----------

// MAGIC %python
// MAGIC rawData = spark.read.option("inferSchema", "true").json("/databricks-datasets/structured-streaming/events/")
// MAGIC rawData.createOrReplaceTempView("rawData")
// MAGIC sql("select r1.action, count(*) as actionCount from rawData as r1 join rawData as r2 on r1.action = r2.action group by r1.action").write.mode('overwrite').csv("/tmp/pyaggaction.csv")
// MAGIC
// MAGIC rawData.show()
/Users/zacayd@octopai.com/Scala1
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC     
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------



# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)



from pyspark.sql.functions import col
# File location and type
file_location = "/FileStore/tables/circuits-3.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

# spark.read\
#     .option("header","true")\
#     .option("inferschema","true")\
#     .csv("/FileStore/tables/circuits-2.csv")\
#     .write\
#     .mode('overwrite')\
#     .csv("/FileStore/tables/ind-2.csv")
    
df=spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")

#df.printSchema()

df=df.withColumn("CXX", col("lat"))
df.createOrReplaceTempView("Data")
df1=spark.sql("select *from Data")
df1.write.mode("overwrite").csv("mycsv.csv")





# COMMAND ----------


/Users/zacayd@octopai.com/pythony
#Mus
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

from pyspark.sql.functions import *

# x=ping('192.168.100.11')
# print(x)

# File location and type
file_location = "/FileStore/tables/circuits-2.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")\
    .write\
    .mode('overwrite')\
    .csv("/FileStore/tables/ind-2.csv")
    
df=spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")

#df.printSchema()

df=df.withColumn("CXX", col("lat"))
df.show()

df.createOrReplaceTempView("Data")

df1=spark.sql("select *from Data")



df1.write.mode("overwrite").csv("mycsv.csv")




/Users/zacayd@octopai.com/py2
System.setProperty("spline.mode", "REQUIRED")
System.setProperty("spline.persistence.factory", "za.co.absa.spline.persistence.mongo.MongoPersistenceFactory")
System.setProperty("spline.mongodb.url","arangodb://root:@http://192.168.100.11:8530/spline")
System.setProperty("spline.producer.url","http://REPLACE_VM_IP_ADDRESS:9091/producer")
System.setProperty("spline.mongodb.name", "spline")
/Users/zacayd@octopai.com/TEST
// MAGIC %python
// MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
// MAGIC

// COMMAND ----------

// import org.apache.commons.configuration._

// import za.co.absa.spline.harvester.conf.{SplineConfigurer, StandardSplineConfigurationStack}
import scala.util.parsing.json.JSON
import za.co.absa.spline.harvester.SparkLineageInitializer._
import za.co.absa.spline.agent.AgentConfig


import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
import org.apache.commons.configuration.Configuration


// import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
// import za.co.absa.spline.harvester.conf.SplineConfigurer

import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
import za.co.absa.spline.harvester.HarvestingContext
import za.co.absa.spline.producer.model.ExecutionPlan
import za.co.absa.spline.producer.model.ExecutionEvent
import za.co.absa.spline.producer.model.ReadOperation
import za.co.absa.spline.producer.model.WriteOperation
import za.co.absa.spline.producer.model.DataOperation

// import za.co.absa.spline.harvester.{AgentConfig, ExtraMetadataImplicits, SparkLineageInitializer}
// import za.co.absa.spline.harvester.postprocessing.{PostProcessingFilter, AbstractPostProcessingFilter}
// import za.co.absa.spline.harvester.conf.{StandardSplineConfigurationStack, SplineConfigurer}
// import za.co.absa.spline.harvester.{HarvestingContext, DefaultSplineConfigurer}
// import za.co.absa.spline.producer.model._





import za.co.absa.spline.harvester.ExtraMetadataImplicits._
import za.co.absa.spline.harvester.SparkLineageInitializer._

val notebookInformationJson = dbutils.notebook.getContext.toJson
val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
val notebookPath = extraContextMap("notebook_path").split("/")
val workspaceUrl=tagMap("browserHostName")
    
val workspaceName=dbutils.notebook().getContext().notebookPath.get
val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
val user = tagMap("user")
val name = notebookPath(notebookPath.size-1)
val notebookInfo = Map("notebookURL" -> notebookURL,
"user" -> user,
"workspaceName" ->workspaceName,
"workspaceUrl" -> workspaceUrl,                       
"name" -> name,
"mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
"timestamp" -> System.currentTimeMillis)
val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)


class CustomFilter extends PostProcessingFilter {
  def this(conf: Configuration) = this()

  override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
    event.withAddedExtra(Map("foo" -> "bar"))

  override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
    plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))

  override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
    op.withAddedExtra(Map("foo" -> "bar"))

  override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
    op.withAddedExtra(Map("foo" -> "bar"))

  override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
    op.withAddedExtra(Map("foo" -> "bar"))
}

val myInstance = new CustomFilter()

// spark.enableLineageTracking()
// spark.enableLineageTracking()
// spark.registerPostEventProcessor(CustomFilter)
// val sparkSession: SparkSession = ???

// val config = AgentConfig.builder().build()

// spark.enableLineageTracking(config)

spark.enableLineageTracking(
  AgentConfig.builder()
    .postProcessingFilter(myInstance)
    .build()
)

//val splineConf = StandardSplineConfigurationStack(spark)
// val splineConf = StandardSplineConfigurationStack





// spark.enableLineageTracking(new DefaultSplineConfigurer(spark, splineConf) {
//   override def postProcessingFilter = new AbstractPostProcessingFilter {
//     override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext): ExecutionPlan = {
//       plan.withAddedExtra(Map(
//         "notebookInfo" -> notebookInfo
//       ))
//     }
//   }
// })



// spark.enableLineageTracking(new DefaultSplineConfigurer(spark, splineConf) {
//   override def postProcessingFilter = new AbstractPostProcessingFilter {
//     override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext): ExecutionPlan = {
//       plan.withAddedExtra(Map(
//         "notebookInfo" -> notebookInfo
//       ))
//     }
//   }
// })

// COMMAND ----------

import za.co.absa.spline.producer.model.ExecutionPlan




// COMMAND ----------

def printMessage(): Unit = {
  val message = "Hello, world!"
  println(message)
}

printMessage()

// COMMAND ----------

import za.co.absa.spline.producer.model.ExecutionPlan
import za.co.absa.spline.producer.model.ExecutionEvent
import za.co.absa.spline.producer.model.ReadOperation
import za.co.absa.spline.producer.model.WriteOperation
import za.co.absa.spline.producer.model.DataOperation




// import org.apache.commons.configuration.Configuration
// import za.co.absa.spline.harvester.HarvestingContext
// import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
// // import za.co.absa.spline.producer.model.v1_1._
// import za.co.absa.spline.harvester.ExtraMetadataImplicits._


// import org.apache.commons.configuration.Configuration
// import scala.util.matching.Regex
// import za.co.absa.commons.CaptureGroupReplacer
// import za.co.absa.commons.config.ConfigurationImplicits.ConfigurationRequiredWrapper
// import za.co.absa.spline.harvester.ExtraMetadataImplicits._
// import za.co.absa.spline.harvester.HarvestingContext
// import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter

// ///////////////////////////////////////////
// import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
// // import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
// // import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
// import za.co.absa.spline.core.conf.DefaultSplineConfigurer   
// import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
// import za.co.absa.spline.harvester.SparkLineageInitializer._



// class CustomFilter extends PostProcessingFilter {
//   def this(conf: Configuration) = this()

//   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
//     event.withAddedExtra(Map("foo" -> "bar"))

//   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
//     plan.withAddedExtra(Map("foo" -> "bar"))

//   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
//     op.withAddedExtra(Map("foo" -> "bar"))

//   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
//     op.withAddedExtra(Map("foo" -> "bar"))

//   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
//     op.withAddedExtra(Map("foo" -> "bar"))
// }


// val notebookInfo = {
//   val ctx = dbutils.notebook.getContext
//   val tags = ctx.tags
//   val notebookName = ctx.notebookPath.get.split("/").last
//   val notebookURL = s"${tags("browserHostName")}/?o=${tags("orgId")}${tags("browserHash")}"
//   val user = tags("user")
//   val mounts = dbutils.fs.ls("/mnt").map(_.path)
  
//   Map(
//     "notebookURL" -> notebookURL,
//     "user" -> user,
//     "name" -> notebookName,
//     "mounts" -> mounts,
//     "timestamp" -> System.currentTimeMillis
//   )
// }


// spark.enableLineageTracking(new DefaultSplineConfigurer(spark, splineConf) {
//   override def postProcessingFilter = new AbstractPostProcessingFilter {
//     override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext): ExecutionPlan = {
//       plan.withAddedExtra(Map(
//         "notebookInfo" -> notebookInfo
//       ))
//     }
//   }
// })


// COMMAND ----------

// %scala
// dbutils.library.install("maven://za.co.absa.spline.agent:spark-3_0-metrics_2.12:1.2.0-RC1")
import com.databricks.library.util.LibraryUtils
// LibraryUtils.listLibraries()



// COMMAND ----------

import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
import za.co.absa.spline.harvester.SparkLineageInitializer._

// create a Spline configuration object
val splineConf = Map("spline.persistence.factory" -> "za.co.absa.spline.persistence.mongo.MongoPersistenceFactory",
                     "spline.mongodb.url" -> "mongodb://localhost:27017/spline",
                     "spline.mongodb.name" -> "spline")

// enable lineage tracking
spark.enableLineageTracking(new DefaultSplineConfigurer(spark, splineConf))

// COMMAND ----------

import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
// import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
   import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer

import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
import za.co.absa.spline.harvester.HarvestingContext
// import za.co.absa.spline.producer.model.v1_1._

import za.co.absa.spline.harvester.ExtraMetadataImplicits._
import za.co.absa.spline.harvester.SparkLineageInitializer._

import za.co.absa.spline.harvester.HarvestingContext
// import za.co.absa.spline.producer.model.v1_1._

import za.co.absa.spline.harvester.ExtraMetadataImplicits._
import za.co.absa.spline.harvester.SparkLineageInitializer._

val notebookInfo = {
  val ctx = dbutils.notebook.getContext
  val tags = ctx.tags
  val notebookName = ctx.notebookPath.get.split("/").last
  val notebookURL = s"${tags("browserHostName")}/?o=${tags("orgId")}${tags("browserHash")}"
  val user = tags("user")
  val mounts = dbutils.fs.ls("/mnt").map(_.path)
  
  Map(
    "notebookURL" -> notebookURL,
    "user" -> user,
    "name" -> notebookName,
    "mounts" -> mounts,
    "timestamp" -> System.currentTimeMillis
  )
}

val splineConf = StandardSplineConfigurationStack
// val splineConf = StandardSplineConfigurationStack
// val x=new DefaultSplineConfigurer(spark, splineConf)

// spark.enableLineageTracking(new DefaultSplineConfigurer(spark, splineConf) {
//   override def postProcessingFilter = new AbstractPostProcessingFilter {
//     override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext): ExecutionPlan = {
//       plan.withAddedExtra(Map(
//         "notebookInfo" -> notebookInfo
//       ))
//     }
//   }
// })
/Users/zacayd@octopai.com/ScalaHarverster
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC     
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------


from pyspark.sql.functions import col
# File location and type
file_location = "/FileStore/tables/circuits-3.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

    
df=spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")

#df.printSchema()

df=df.withColumn("CXX", col("lat"))
df.createOrReplaceTempView("Data13")
df1=spark.sql("select *from Data13")
df1.write.mode("overwrite").csv("mycsv13.csv")
/Users/zacayd@octopai.com/MyTest
-- MAGIC %md
-- MAGIC
-- MAGIC # Load data to Delta Lake from Azure storage with COPY INTO
-- MAGIC
-- MAGIC This notebook shows you how to create and query a table or DataFrame loaded from data stored in Azure Data Lake Storage Gen2 (ADLS Gen2) and Blob Storage. 
-- MAGIC
-- MAGIC ADLS Gen2 and Blob Storage both use the ABFS driver; you can use the same patterns to connect to either of these data sources.
-- MAGIC
-- MAGIC <!-- INSERT LINKS FOR ACCESS  -->

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Create a target Delta table
-- MAGIC
-- MAGIC `COPY INTO` requires a target table created with Delta Lake. If using Databricks Runtime (DBR) 11.0 or above, you can create an empty Delta table using the command below.
-- MAGIC
-- MAGIC When using DBR below 11.0, you'll need to specify the schema for the table during creation.
-- MAGIC
-- MAGIC Delta Lake is the default format for all tables created in DBR 8.0 and above. When using DBR below 8.0, you'll need to add a `USING DELTA` clause to your create table statement.

-- COMMAND ----------

-- 11.0 and above
CREATE TABLE <database-name>.<table-name>;

-- 8.0 and above
-- CREATE TABLE <database-name>.<table-name>
-- (col_1 TYPE, col_2 TYPE, ...);

-- Below 8.0
-- CREATE TABLE <database-name>.<table-name>
-- (col_1 TYPE, col_2 TYPE, ...)
-- USING delta;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Loading data with a service principal
-- MAGIC
-- MAGIC Users with sufficient permissions can create applications in the Azure Active Directory.
-- MAGIC
-- MAGIC Databricks administrator can use these to create service principals for use in the Databricks workspace.
-- MAGIC
-- MAGIC Databricks recommends securing access Azure storage by configuring service principals for clusters.
-- MAGIC * [Databricks docs: Accessing ADLS Gen2 and Blob Storage with Azure Databricks](https://docs.microsoft.com/azure/databricks/data/data-sources/azure/azure-storage)

-- COMMAND ----------

COPY INTO <database-name>.<table-name>
FROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'
FILEFORMAT = CSV
COPY_OPTIONS ('mergeSchema' = 'true')

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## COPY INTO with temporary credentials
-- MAGIC
-- MAGIC `COPY INTO` also supports using temporary credentials to access data from Azure storage.
-- MAGIC * [Databricks docs: Use temporary credentials to load data with COPY INTO](https://docs.microsoft.com/azure/databricks/ingestion/copy-into/temporary-credentials)
-- MAGIC
-- MAGIC You can use the Azure CLI to generate SAS tokens. Note that you will need proper permissions on the Azure subscription and the storage account to create SAS tokens. (If you do not have the necessary permissions, you will need to talk to your cloud administrator).
-- MAGIC * [Azure docs: Install the Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli)
-- MAGIC * [Azure docs: Create a user delegation SAS for a container or blob with the Azure CLI](https://docs.microsoft.com/azure/storage/blobs/storage-blob-user-delegation-sas-create-cli)
-- MAGIC
-- MAGIC For more details on using SAS tokens to grant access to Azure storage, including instructions using the Azure Portal UI, see:
-- MAGIC [Azure docs: Grant limited access to Azure Storage resources using shared access signatures (SAS)](https://docs.microsoft.com/azure/storage/common/storage-sas-overview)

-- COMMAND ----------

COPY INTO <database-name>.<table-name>
FROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder' WITH (
  CREDENTIAL (AZURE_SAS_TOKEN = '<sas-token>')
)
FILEFORMAT = CSV
COPY_OPTIONS ('mergeSchema' = 'true')
/Users/zacayd@octopai.com/Azure Blob Storage
CREATE TABLE IF NOT EXISTS
  lineage_data.menu (
    recipe_id INT,
    app string,
    main string,
    dessert string
  );

/Users/zacayd@octopai.com/MySQL



from pyspark.sql.functions import *
from pyspark.sql.types import *

json_path = "/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json"
@dlt.table(
  comment="The raw wikipedia clickstream dataset, ingested from /databricks-datasets."
)
def clickstream_raw():
  return (spark.read.format("json").load(json_path))

@dlt.table(
  comment="Wikipedia clickstream data cleaned and prepared for analysis."
)
@dlt.expect("valid_current_page_title", "current_page_title IS NOT NULL")
@dlt.expect_or_fail("valid_count", "click_count > 0")
def clickstream_prepared():
  return (
    dlt.read("clickstream_raw")
      .withColumn("click_count", expr("CAST(n AS INT)"))
      .withColumnRenamed("curr_title", "current_page_title")
      .withColumnRenamed("prev_title", "previous_page_title")
      .select("current_page_title", "click_count", "previous_page_title")
  )

@dlt.table(
  comment="A table containing the top pages linking to the Apache Spark page."
)
def top_spark_referrers():
  return (
    dlt.read("clickstream_prepared")
      .filter(expr("current_page_title == 'Apache_Spark'"))
      .withColumnRenamed("previous_page_title", "referrer")
      .sort(desc("click_count"))
      .select("referrer", "click_count")
      .limit(10)
  )
/Users/zacayd@octopai.com/ExampleDLT
CREATE OR REFRESH LIVE TABLE clickstream_raw
COMMENT "The raw wikipedia clickstream dataset, ingested from /databricks-datasets."
AS SELECT * FROM json.`/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json`;

CREATE OR REFRESH LIVE TABLE clickstream_prepared(
  CONSTRAINT valid_current_page EXPECT (current_page_title IS NOT NULL),
  CONSTRAINT valid_count EXPECT (click_count > 0) ON VIOLATION FAIL UPDATE
)
COMMENT "Wikipedia clickstream data cleaned and prepared for analysis."
AS SELECT
  curr_title AS current_page_title,
  CAST(n AS INT) AS click_count,
  prev_title AS previous_page_title
FROM live.clickstream_raw;

CREATE OR REFRESH LIVE TABLE top_spark_referers
COMMENT "A table containing the top pages linking to the Apache Spark page."
AS SELECT
  previous_page_title as referrer,
  click_count
FROM live.clickstream_prepared
WHERE current_page_title = 'Apache_Spark'
ORDER BY click_count DESC
LIMIT 10;
/Users/zacayd@octopai.com/ExampleDLT_SQL

/Users/zacayd@octopai.com/Untitled Notebook 2023-06-12 15:44:30
# Replace the text in the Storage Location input box to the desired pipeline storage location. This can be found in the pipeline configuration under 'storage'.
storage_location = dbutils.widgets.get('storage')
event_log_path = storage_location + "/system/events"
 
# Read the event log into a temporary view so it's easier to query.
event_log = spark.read.format('delta').load(event_log_path)
event_log.createOrReplaceTempView("event_log_raw")

/Users/zacayd@octopai.com/MyLogs
# MAGIC %sql
# MAGIC alter table stgods_dl.BulkSMS_JAM_logs add column Flag_Audit string

# COMMAND ----------

storage_account_name = "zacaystorage"
client_id            = "0e8ac0a7-ef77-47d4-9fac-2ea9bd01b4ec"
tenant_id            = "fbb6a85c-fb76-4fe5-9893-abd1eace5774"
client_secret        = "Zte8Q~Z9F-ReAaea1z0Q~qqyQDPaLqPle4yNPdvE"


# COMMAND ----------



# dbfs:/mnt/Target_container/FeedName/working/Source_container/2023020120/20230201202020/*

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

           
storage_account_name="zacaystorage"
container_name="raw"

dbutils.fs.mount(
  source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
  mount_point = "/mnt/Target_container/FeedName/working/Source_container/2023020120/20230201202020",
  extra_configs = configs)




# COMMAND ----------



spark.sql("SHOW CREATE TABLE stgods_dl.BulkSMS_JAM_logs").show(1, False)




# COMMAND ----------

# MAGIC %sql
# MAGIC  drop table stgods_dl.BulkSMS_JAM_logs;
# MAGIC
# MAGIC
# MAGIC CREATE TABLE stgods_dl.BulkSMS_JAM_logs (
# MAGIC   Flag_reject STRING,
# MAGIC    Flag_NotebookStatus STRING,
# MAGIC    Load_TS STRING,
# MAGIC    Data_dt STRING,
# MAGIC    Source_container STRING,
# MAGIC    Trigger_Key STRING,
# MAGIC    Flag_Temp STRING,
# MAGIC    Flag_Audit STRING,
# MAGIC     Flag_DataLake STRING,
# MAGIC      Flag_Error STRING,
# MAGIC       Flag_StgODS STRING)
# MAGIC     USING delta
# MAGIC     TBLPROPERTIES (
# MAGIC    'delta.autoOptimize.optimizeWrite' = 'true',
# MAGIC    'delta.autoOptimize.autoCompact' = 'true');
# MAGIC
# MAGIC     

# COMMAND ----------

# MAGIC
# MAGIC %sql
# MAGIC
# MAGIC  CREATE TABLE stgods_dl.stgods_BulkSMS_JAM  (
# MAGIC   AreaCode STRING,
# MAGIC    Flag_NotebookStatus STRING,
# MAGIC    Load_TS STRING,
# MAGIC    Data_dt STRING,
# MAGIC    Source_container STRING,
# MAGIC    Trigger_Key STRING,
# MAGIC    Flag_Temp STRING,
# MAGIC    Flag_Audit STRING,
# MAGIC     Flag_DataLake STRING,
# MAGIC      Flag_Error STRING,
# MAGIC       Flag_StgODS STRING)
# MAGIC     USING delta
# MAGIC     TBLPROPERTIES (
# MAGIC    'delta.autoOptimize.optimizeWrite' = 'true',
# MAGIC    'delta.autoOptimize.autoCompact' = 'true');
# MAGIC     
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC  drop table jamaica_dl.BulkSMS_JAM;
# MAGIC CREATE TABLE jamaica_dl.BulkSMS_JAM (
# MAGIC   AreaCode STRING,
# MAGIC    Flag_NotebookStatus STRING,
# MAGIC    Load_TS STRING,
# MAGIC    Data_dt STRING,
# MAGIC    Source_container STRING,
# MAGIC    Trigger_Key STRING,
# MAGIC    Flag_Temp STRING,
# MAGIC    Flag_Audit STRING,
# MAGIC     Flag_DataLake STRING,
# MAGIC      Flag_Audit2 STRING,
# MAGIC       Flag_Audit3 STRING)
# MAGIC     USING delta
# MAGIC     TBLPROPERTIES (
# MAGIC    'delta.autoOptimize.optimizeWrite' = 'true',
# MAGIC    'delta.autoOptimize.autoCompact' = 'true');
# MAGIC
# MAGIC --  create table stgods_dl.BulkSMS_JAM as select * from stgods_dl.BulkSMS_JAM_logs;
# MAGIC  

# COMMAND ----------

# MAGIC %sql
# MAGIC drop table cfw.table_load_status;
# MAGIC $CREATE TABL
# MAGIC E cfw.table_load_status (
# MAGIC      AreaCode STRING,
# MAGIC    Flag_NotebookStatus STRING,
# MAGIC    Load_TS STRING,
# MAGIC    Data_dt STRING,
# MAGIC    Source_container STRING,
# MAGIC    Trigger_Key STRING,
# MAGIC    FeedName STRING,
# MAGIC    Flag_Audit STRING,
# MAGIC     Flag_DataLake STRING,
# MAGIC      Flag_Audit2 STRING,
# MAGIC       Flag_Audit3 STRING)
# MAGIC     USING delta
# MAGIC     TBLPROPERTIES (
# MAGIC    'delta.autoOptimize.optimizeWrite' = 'true',
# MAGIC    'delta.autoOptimize.autoCompact' = 'true');
# MAGIC    

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC create table  cfw.dl_file_details as select *from cfw.table_load_status;
/Users/zacayd@octopai.com/Helper
schema = StructType() \
      .add("RecordNumber",IntegerType(),True) \
      .add("Zipcode",IntegerType(),True) \
      .add("ZipCodeType",StringType(),True) \
      .add("City",StringType(),True) \
      .add("State",StringType(),True) \
      .add("LocationType",StringType(),True) \
      .add("Lat",DoubleType(),True) \
      .add("Long",DoubleType(),True) \
      .add("Xaxis",IntegerType(),True) \
      .add("Yaxis",DoubleType(),True) \
      .add("Zaxis",DoubleType(),True) \
      .add("WorldRegion",StringType(),True) \
      .add("Country",StringType(),True) \
      .add("LocationText",StringType(),True) \
      .add("Location",StringType(),True) \
      .add("Decommisioned",BooleanType(),True) \
      .add("TaxReturnsFiled",StringType(),True) \
      .add("EstimatedPopulation",IntegerType(),True) \
      .add("TotalWages",IntegerType(),True) \
      .add("Notes",StringType(),True)
      
df_with_schema = spark.read.format("csv") \
      .option("header", True) \
      .schema(schema) \
      .load("/FileStore/tables/circuits-2.csv")




df_with_schema.write.options(header='True', delimiter=',') \
 .csv("/FileStore/tables/result.csv")

df_with_schema.show()



/Users/zacayd@octopai.com/Digicel_SandvineBilling
# Databricks notebook source
# MAGIC %md
# MAGIC ###### Purpose: Notebook to process My Digicel App app_client Usage raw data
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |Feb 20, 2021|Divya Verma| Created the notebook to process My Digicel App app_client Usage raw data | 5 min |
# MAGIC |Dec 20, 2021|PARV| Duplicate File check,Audit Tables and Alert Configuration changes | 5 min |
# MAGIC |Feb 28, 2021|Divya Verma| sprint 1 backlog items changes | 5 min |

# COMMAND ----------

# DBTITLE 1,Widgets Preparation
dbutils.widgets.removeAll()
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("trigger_key", "")
dbutils.widgets.text("FeedName", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("Source_container", "")
dbutils.widgets.text("Target_container", "")
dbutils.widgets.text("File_delimiter", "")
dbutils.widgets.text("hub_id", "")
PipelineName          = dbutils.widgets.get("PipelineName")
DataFactoryName       = dbutils.widgets.get("DataFactoryName")
trigger_key           = str(dbutils.widgets.get("trigger_key"))
FeedName              = dbutils.widgets.get("FeedName")
windowStartTime       = dbutils.widgets.get("windowStartTime")
PipelineRunId         = dbutils.widgets.get("PipelineRunId")
source_container      = dbutils.widgets.get("Source_container")
target_container      = dbutils.widgets.get("Target_container")
File_delimiter        = dbutils.widgets.get("File_delimiter")
hub_id                = str(dbutils.widgets.get("hub_id"))

# COMMAND ----------

# DBTITLE 1,Optimization Hints
# MAGIC %sql
# MAGIC set spark.sql.shuffle.partitions = 400;
# MAGIC set spark.sql.files.maxPartitionBytes=1073741824;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;

# COMMAND ----------

# DBTITLE 1,Import Required Parameters
import time
import os
import json
import pyspark
import requests
import pyspark.sql.functions as F
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit,col,lower,regexp_replace,split,when,concat
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
spark.conf.set("spark.databricks.io.cache.enabled", "true")
spark.sql("set spark.sql.files.ignoreCorruptFiles=true")
spark.sql("REFRESH TABLE stgods_dl."+FeedName+"_logs")

# COMMAND ----------

# DBTITLE 1,Create Path variable to fetch file from temp adls working
dt = windowStartTime[:10].replace('-','')
dateId = windowStartTime.replace('-','').replace(':','')
file_location = "/mnt/"+target_container+"/"+FeedName+"/working/"+source_container+"/"+dt+"/"+dateId
duplicate_file_location = "/mnt/"+target_container+"/"+FeedName+"/duplicate/"+source_container+"/"+dt+"/"+dateId
table_name ='stg_cn_'+FeedName+trigger_key+'_'+source_container+'_'+dateId 
table_name = table_name.replace('-','')
path = "/mnt/stgods/"+FeedName+"/stg_cn_"+FeedName+dateId+hub_id
log_table = FeedName.lower()+"_logs"
print("log_table:",log_table)
print("path:",path)
print(table_name)
print(file_location)
print(duplicate_file_location)

# COMMAND ----------

# DBTITLE 1,Check Notebook status for current Load timestamp
NotebookStatus = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
NS_status = NotebookStatus.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
if NS_status == ['Y']:
  dbutils.notebook.exit( FeedName+" Notebook Status is already successful for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_NotebookStatus = "+str(NS_status))
else:
  print("Execute Further Notebook Commands")

# COMMAND ----------

# DBTITLE 1,Check Duplicate File
# MAGIC %run /Shared/DWH_Transformation/TOOLS/CheckDuplicateFile_v2

# COMMAND ----------

# DBTITLE 1,Duplicate File Checking
filedirectory="/*_PRODUCT_DELIVERY/*/SELFCARECLIENT/lz/*"
dflogsduplicate = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
DP          = dflogsduplicate.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if DP != ['Y']:
  dfdffileinfo=checkDuplicateFile(File_Location=str(file_location),File_Directory=str(filedirectory),FeedName=str(FeedName),Trigger_Key=str(trigger_key),WindowStartTime=str(windowStartTime),PipelineRunID=str(PipelineRunId),FileLocation_dup =str(duplicate_file_location))

# COMMAND ----------

# DBTITLE 1,Schema Enforcement
nofilefound=False
schema = StructType([ \
     StructField("ID",StringType(),True),\
     StructField("NAME",StringType(),True),\
     StructField("corruptrecord",StringType(),True), 
   ])

try:
  app_client_with_schema = spark.read.format("csv")\
  .option("header", "false")\
  .schema(schema)\
  .option("sep", File_delimiter)\
  .option("quote", "\"")\
  .option("multiline","true")\
  .load(file_location+filedirectory)
  app_client_with_schemaAllhubs= app_client_with_schema.withColumn("Load_TS",lit(windowStartTime)).\
  withColumn("FileName",input_file_name()).withColumn("Last_Updated_By",lit("BAU")).\
  withColumn("Data_dt",lit(dt)).withColumn("Trigger_Key",lit(trigger_key)).withColumn("Derived_HUB_ID",lit(hub_id))
  app_client_with_schemaAllhubs.createOrReplaceTempView("temp1_app_client_name")
  Row_count = app_client_with_schemaAllhubs.count()
  print(Row_count)
except Exception as e:
  print(file_location+filedirectory)
  print(e)
  nofilefound=True

# COMMAND ----------

# DBTITLE 1,Insert into Alert_Control for Duplicate Files
if DP != ['Y']:  
  dupfileslist=[]
  vduplicatesfiles=dfdffileinfo.filter(col('Duplicate')==1).select(concat('File_Location',lit('/'),'FileName'))
  for x in vduplicatesfiles.collect():
    dupfileslist.append(x[0])
  dupfilestr='\n'.join(dupfileslist)
  dup_data = '{"title": "' +FeedName+ ' Duplicate Files Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Duplicate Files Moved To {dwhadls{xxx}01}","error":{"Message":"Duplicate Files","TableName":"cfw.DL_File_Details","Path":"' +duplicate_file_location+ '","Duplicate Files":{"Name":"' +dupfilestr+ '"}}}'
  
  if (vduplicatesfiles.count() > 0):
    print('Duplicates Found')
    for i in range(1,5,1):
        try:
          spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key= int('"+trigger_key+"') AND Alert_Type = 'Duplicate_Files' ")
          break;
        except Exception as e1:
          if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e1)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e1)):
            time.sleep(30)
            continue
          else:
            raise Exception(e1)
            
    ##INSERT INTO cfw.etl_alert_control for duplicate Files
    spark.sql('''INSERT INTO cfw.etl_alert_control 
                   SELECT '''+trigger_key+''' AS trigger_key
                         ,'''"'"+FeedName+"'"''' AS FeedName
                         ,TO_TIMESTAMP('''"'"+windowStartTime+"'"''') AS Load_TS
                         ,'Duplicate_Files' AS Alert_Type
                         ,'''"'"+dup_data+"'"''' AS Alert_Reason
                         ,'''"'"+source_container+"'"''' AS Data_Storage
                         ,'cfw.DL_File_Details' AS Data_location
                         ,'''+str(vduplicatesfiles.count())+''' AS Data_Count 
                         ,'N' AS Alert_Sent
                         ,CURRENT_TIMESTAMP() AS Insert_Ts
                         ,NULL AS Update_ts
                         ,'''"'"+PipelineRunId+"'"''' AS Pipeline_Run_ID
              ''')
  
  else:
    print('Duplicates NOT Found in '+FeedName+' for HUB_ID ='+hub_id)

# COMMAND ----------

# DBTITLE 1,If File Not Found
if nofilefound is True:
  dbutils.notebook.exit("No Files Found")

# COMMAND ----------

# DBTITLE 1,Snapshot_Date Helper
# MAGIC %run /Shared/DWH_Transformation/TOOLS/Func_Snapshot_Date

# COMMAND ----------

# DBTITLE 1,Data Preparation: AAP Client
app_clientFinal_CN = spark.sql('''
select 
     CASE WHEN TRIM(ID) IS NULL OR TRIM(ID) = "" OR TRIM(ID) = "null" THEN -1 ELSE TRIM(ID) END AS ID
    ,TRIM(NAME) AS NAME
    ,Load_TS
    ,REPLACE(SUBSTRING(a.FileName, POSITION('lz/' IN a.FileName)),'lz/','') AS FileName
    ,Last_Updated_By
    ,nvl(a.Derived_HUB_ID,99) AS HUB_ID
    ,corruptrecord
    ,Data_dt
    ,Trigger_Key
    ,'group' AS Container_Name
    from temp1_app_client_name as a
''')
app_client_final = app_clientFinal_CN.withColumn("snapshot_dt", find_date(col("FileName")))
app_client_final.createOrReplaceTempView("temp2_app_clientFinal_CN")

# COMMAND ----------

# DBTITLE 1,Temp Table Data Load
dflogstemp = spark.sql("SELECT DISTINCT Flag_Temp FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"'  AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TF         = dflogstemp.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()
if spark._jsparkSession.catalog().tableExists('stgods_dl', table_name) and TF == ['Y']:
  print("stgods_dl."+table_name+" table exists")
else:
  dbutils.fs.rm(path,recurse=True)
  spark.sql("CREATE OR REPLACE TABLE stgods_dl."+table_name+"  USING DELTA LOCATION '"+path+"' PARTITIONED BY (Container_Name) AS SELECT * FROM temp2_app_clientFinal_CN")
  
  ##DELETE
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
        break;
      except Exception as e2:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e2)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e2)):
          time.sleep(30)
          continue
        else:
          raise Exception(e2)
          
  ##INSERT INTO LOG TABLE  
  spark.sql("INSERT INTO stgods_dl."+log_table+" VALUES ('"+windowStartTime+"','"+dt+"','"+source_container+"',int('"+trigger_key+"'),'N','N','N','N','N','N','N') ")
  
  ##Update Logging table
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Temp = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl."+table_name+" and Flag_Temp updated to Y in stgods_dl."+log_table+" for Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e3:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e3):
        time.sleep(30)
        continue
      else:
        raise Exception(e3)

# COMMAND ----------

# DBTITLE 1,Country Wise Incremental Load
dflogsDL = spark.sql("SELECT DISTINCT Flag_DataLake FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TDL      = dflogsDL.select("Flag_DataLake").distinct().rdd.flatMap(lambda x: x).collect()
if TDL == ['N']:
  MK=app_client_final.select("Container_Name").distinct().rdd.flatMap(lambda x: x).collect()
  print(MK)
  for t in MK :    
    ##DELETE
    for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM "+t+"_dl."+FeedName+" where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e4:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e4)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e4)):
          time.sleep(30)
          continue
        else:
          raise Exception(e)
             
    ##INSERT INTO COUNTRY WISE DELTA TABLE     
    spark.sql(" INSERT INTO group_dl.aap_client SELECT ID,NAME,Load_TS,FileName,Last_Updated_By,Data_dt,int(HUB_ID),REPLACE(snapshot_dt,'-','') as DATE_CODE,int(Trigger_Key) FROM stgods_dl."+table_name+" WHERE corruptrecord IS NULL OR corruptrecord == ' '")
    
  #Update logging flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Datalake = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      print("Data inserted into "+FeedName+" Data Lake tables and Flag_Datalake updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e5:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e5):
        time.sleep(180)
        continue
      else:
        raise Exception(e5)
else:
  print("Data in "+FeedName+" Data Lake tables exists for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Datalake = "+str(TDL))

# COMMAND ----------

# DBTITLE 1,For ODS Layer Processing (final ods Layer) 
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
  sdt = spark.sql('''select distinct cast(snapshot_dt as string) from stgods_dl.'''+table_name+''' WHERE corruptrecord IS NULL OR corruptrecord == "" OR corruptrecord ==" " ''').rdd.flatMap(lambda x: x).collect() 
  sdt.sort()
  for x in sdt:
    dbutils.notebook.run("/DWH_Transformation/ODS/MyDigicelApp/ODS_Digicel_App", 43200,{"table_name": table_name,"snapshot_dt":x})
                   
  ##update logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_StgODS  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into ods_dl.digicel_app and Flag_StgODS updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e7:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e7):
        time.sleep(30)
        continue
      else:
        raise Exception(e7) 
else:
  print("Data exists in ods_dl.digicel_app table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_StgODS = "+str(TT))

# COMMAND ----------

# DBTITLE 1,cfw.Table_load_status INSERT
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['Y']:
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM cfw.Table_load_status where FeedName = '"+FeedName+"' AND  Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e8:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
          time.sleep(30)
          continue
        else:
          raise Exception(e8)
          
  ##INSERT INTO cfw.Table_load_status 
  spark.sql('''
  INSERT INTO cfw.Table_load_status 
  SELECT DISTINCT int(Trigger_Key)
                 ,Load_TS
                 ,snapshot_dt
                 ,CAST(HUB_ID as int) AS HUB_ID
                 ,"'''+FeedName +'''" AS FeedName
                 ,'AAP_CLIENT' AS Source_System
                 ,'ods_dl' as target_db
                 ,'Digicel_App' as target_table_name
                 ,'Y' as load_status
                 ,count(*) as total_records
                 ,count(distinct FileName) as total_files
                 ,"'''+str(PipelineRunId)+'''" AS Pipeline_Run_Id
                 ,current_timestamp()
                 ,null FROM stgods_dl.'''+table_name+''' WHERE  corruptrecord IS NULL OR corruptrecord == "" OR corruptrecord ==" "
                 group by snapshot_dt,HUB_ID,Source_System,Trigger_Key,Load_TS ''')
  print("Data inserted into cfw.Table_load_status")

# COMMAND ----------

# DBTITLE 1,Invalid Records Segregation and alert
Invalid_MSISDNs_cnt = 0
Schema_Mismatch_cnt = spark.sql("select * from stgods_dl."+table_name+" Where (corruptrecord IS NOT NULL AND corruptrecord <> '' AND corruptrecord <>' ')").count()
dflogserror = spark.sql("SELECT DISTINCT Flag_Error FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TE          = dflogserror.select("Flag_Error").distinct().rdd.flatMap(lambda x: x).collect()
if TE == ['N']:
  if Schema_Mismatch_cnt != 0:
    for i in range(1,5,1):
      try:
        spark.sql(" DELETE FROM error_dl.reject_"+FeedName+" where Error_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_key=int('"+trigger_key+"') ")
        spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key=int('"+trigger_key+"') AND Alert_Type = 'Invalid_Data' ")
        break;
      except Exception as e9:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e9)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e9)):
          time.sleep(30)
          continue
        else:
          raise Exception(e9)
   
    ##INSERT INTO ERROR and Alert control Table
    spark.sql(" INSERT INTO TABLE error_dl.reject_"+FeedName +" SELECT ID,NAME,Load_TS,FileName,Last_Updated_By,int(HUB_ID),corruptrecord,Data_dt as Error_dt,int(Trigger_Key),snapshot_dt ,'Schema_Mismatch' AS Error_Type FROM stgods_dl."+table_name+" WHERE corruptrecord IS NOT NULL AND corruptrecord != ' ' AND corruptrecord != '' ")
    data_error = '{"title": "' +FeedName+ ' Invalid Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Erroneous Records Moved To {dgc{xxx}datalake01}","error":{"Message":"Invalid Records","TableName":"error_dl.reject_'+FeedName.lower()+'","Path":"/error/mydigicelapp/reject_aap_client/reject_'+FeedName.lower()+'","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+ '","Total Invalid Records": "' +str(Schema_Mismatch_cnt)+ '"}}}'
    
    spark.sql('''
     INSERT INTO cfw.etl_alert_control 
     select DISTINCT int("'''+trigger_key+'''") AS trigger_key
                     ,"'''+FeedName+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+windowStartTime+'''") AS Load_TS
                     ,'Invalid_Data' as Alert_Type
                     ,'"'''+str(data_error)+'''"' AS Alert_Reason
                     ,"'''+source_container+'''" as Data_Storage
                     ,LOWER("error_dl.reject_aap_client") as Data_location
                     ,"'''+str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+'''" as Data_Count
                     ,'N' as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID 
              ''')
  ##Update logging  Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Error  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into error_dl.reject_"+FeedName +" and Flag_Error updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as err:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(err):
        time.sleep(180)
        continue
      else:
        raise Exception(err)

else:
  print("Data exists in error_dl.reject_"+FeedName +" table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TE))

# COMMAND ----------

# DBTITLE 1,Audit Data Collection
df_audit = spark.sql(''' SELECT LOAD_TS
		                       ,FileName
                               ,count(*) AS total_RecordCount
		                       ,SUM(CASE 
                                        WHEN FileName IS NOT NULL AND corruptrecord IS NULL OR corruptrecord == ' '
					                    THEN 1
				                    ELSE 0
				                    END) AS RecordCount_FN
		                       ,0 AS cnt_InvalidMSISDNs
		                       ,SUM(CASE 
		                      		     WHEN corruptrecord IS NOT NULL AND corruptrecord != ' '
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_SchemaMismatch
	                     FROM stgods_dl.'''+table_name+'''
	                     GROUP BY FileName,LOAD_TS ''')
df_audit.createOrReplaceTempView("df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,For Rejected Files Audit
spark.sql("DROP TABLE IF EXISTS stgods_dl."+FeedName+"_audit_"+trigger_key)
spark.sql("CREATE TABLE stgods_dl."+FeedName+"_audit_"+trigger_key+ " AS SELECT DISTINCT Load_TS,FileName,total_RecordCount,RecordCount_FN,cnt_InvalidMSISDns,cnt_SchemaMismatch FROM df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,Audit Data details for cfw.DL_File_Details Table
dflogsaudit = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TA          = dflogsaudit.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if TA == ['N']:
  for i in range(1,5,1):
    try:
      bflag = 0
      spark.sql('''MERGE INTO cfw.DL_File_Details AS TARGET 
      USING (
      select a.*
            ,CONCAT('{"Invalid MSISDNs":"',cnt_InvalidMSISDNs,'","Schema Mismatch":"',cnt_SchemaMismatch,'"}') as error_reason from df_audit_'''+FeedName+''' a) SOURCE 
      on trim(SOURCE.FILENAME)=trim(TARGET.FILENAME) 
      and TARGET.File_load_status != 'D' 
      and TARGET.Load_TS = TO_TIMESTAMP("'''+ windowStartTime+'''") 
      and TARGET.TRIGGER_KEY= int("'''+trigger_key+'''")
      WHEN MATCHED  THEN UPDATE SET 
      TARGET.File_load_status='Y'
     ,TARGET.Total_Records=SOURCE.total_RecordCount
     ,TARGET.valid_records=SOURCE.RecordCount_FN
     ,TARGET.error_records=SOURCE.cnt_InvalidMSISDNs + SOURCE.cnt_SchemaMismatch
     ,TARGET.error_reason= SOURCE.error_reason ''')
                        
      bflag = 1
      
      #logging
      if bflag == 1:
        spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Audit  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
        print("Data inserted into cfw.DL_File_Details and Flag_Audit updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" ")     
      break;
    except Exception as e11:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e11):
        time.sleep(180)
        continue
      else:
        raise Exception(e11)
  
  ##IF Merge could not happen successfully
  if bflag == 0:
      raise Exception("Retry!")
else:
  print("Data exists in cfw.DL_File_Details table for "+FeedName +" feed for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TA))

# COMMAND ----------

# DBTITLE 1,Rejected and Empty Files Logging
db = "stgods_dl"
FeedName              = FeedName
feedaudit             = FeedName+"_audit_"+trigger_key
foldername            = "GR_PRODUCT_DELIVERY/GLOBAL/SELFCARECLIENT/lz/"     
tempaudit_rejected    = FeedName+"_rejected_"+trigger_key
file_location_reject  = "/mnt/"+target_container+"/"+FeedName+"/reject/"+source_container+"/"+dt+"/"+dateId
file_location_working = file_location+filedirectory

#Rejected Files Flow
dflogsreject = spark.sql("SELECT DISTINCT Flag_reject FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TR          = dflogsreject.select("Flag_reject").distinct().rdd.flatMap(lambda x: x).collect()
if TR  == ['N']:
  dbutils.notebook.run("/Shared/DWH_Transformation/DATALAKE/Generic/RejectedFiles", 43200,{"windowStartTime": windowStartTime, "PipelineRunId": PipelineRunId, "DataFactoryName":DataFactoryName,"PipelineName":PipelineName,"trigger_key":trigger_key,"db":db,"feedaudit":feedaudit,"foldername":foldername,"file_location":file_location,"file_location_reject":file_location_reject,"file_location_working":file_location_working,"tempaudit_rejected":tempaudit_rejected,"FeedName":FeedName,"source_container":source_container})
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Reject  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      print("Data Moved to rejected folder for rejected files and to cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND source_container = "+source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e13:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e13):
        time.sleep(180)
        continue
      else:
        raise Exception(e13)
else:
  print("Data exists in cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND source_container = "+source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TR))

# COMMAND ----------

# DBTITLE 1, Notebook status Logging
for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_NotebookStatus  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Flag_Temp = 'Y' AND Flag_Datalake = 'Y' AND Flag_StgODS = 'Y' AND Flag_Error = 'Y' AND Flag_Audit = 'Y' AND Flag_reject = 'Y' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      break;
    except Exception as e14:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e14):
        time.sleep(30)
        continue
      else:
        raise Exception(e14)     

# COMMAND ----------

# DBTITLE 1,Clean up and Exit
dflogs_final = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
NFS =dflogs_final.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
print(NFS)
if NFS == ['Y']:
  spark.sql(" DROP TABLE IF EXISTS stgods_dl."+table_name+" ")
  dbutils.fs.rm(path,recurse=True)
  dbutils.notebook.exit("Successful")

/Users/zacayd@octopai.com/Digicel

import za.co.absa.spline.harvester.SparkLineageInitializer._


spark.enableLineageTracking()

case class Student(id: Int, name: String, addrId: Int)

case class Address(id: Int, address: String)

Seq(
  Student(111, "Amy Smith", 1),
  Student(222, "Bob Brown", 2)
).toDS.write.mode("overwrite").parquet("/students")

Seq(
  Address(1, "123 Park Ave, San Jose"),
  Address(2, "456 Taylor St, Cupertino")
).toDS.write.mode("overwrite").parquet("/addresses")

val students = spark.read.parquet("/students")
val addresses = spark.read.parquet("/addresses")

students
  .join(addresses)
  .where(addresses("id") === students("addrId"))
  .select("name", "address")
  .write.mode("append").parquet("/student_names_with_addresses")
/Users/zacayd@octopai.com/Scalal
# MAGIC %md
# MAGIC
# MAGIC ## Overview
# MAGIC
# MAGIC This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.
# MAGIC
# MAGIC This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported.

# COMMAND ----------

# MAGIC %fs
# MAGIC ls

# COMMAND ----------

# Create a view or table



# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC /* Query the created temp table in a SQL cell */
# MAGIC
# MAGIC select * from `circuits-1_csv`

# COMMAND ----------

# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.
# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.
# To do so, choose your table name and uncomment the bottom line.

permanent_table_name = "circuits-1_csv"

# df.write.format("parquet").saveAsTable(permanent_table_name)
/Users/zacayd@octopai.com/2022-10-25 - DBFS Example
# MAGIC %scala
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
# MAGIC import za.co.absa.spline.producer.model.v1_1._
# MAGIC import za.co.absa.spline.producer.model._
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import scala.concurrent.duration.Duration
# MAGIC import scala.util.{Failure, Success, Try}
# MAGIC
# MAGIC val splineConf: Configuration = StandardSplineConfigurationStack(spark)
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
# MAGIC   //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
# MAGIC   //val test = dbutils.notebook.getContext.notebookPath
# MAGIC   val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC   val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC   val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC
# MAGIC   val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC   val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC   val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC
# MAGIC
# MAGIC
# MAGIC   //val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC   val user = tagMap("user")
# MAGIC   val name = notebookPath(notebookPath.size-1)
# MAGIC   
# MAGIC   val notebookInfo = Map(
# MAGIC      //"notebookURL" -> notebookURL,  
# MAGIC                 "user" -> user, 
# MAGIC                 "workspaceName" ->workspaceName,
# MAGIC                 "name" -> name,
# MAGIC                 "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC                 "timestamp" -> System.currentTimeMillis)
# MAGIC   val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC   
# MAGIC   override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
# MAGIC     override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
# MAGIC     override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
# MAGIC     override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
# MAGIC     override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
# MAGIC     override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
# MAGIC   })
# MAGIC })

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------



# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)



from pyspark.sql.functions import col
# File location and type
file_location = "/FileStore/tables/circuits-3.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

# spark.read\
#     .option("header","true")\
#     .option("inferschema","true")\
#     .csv("/FileStore/tables/circuits-2.csv")\
#     .write\
#     .mode('overwrite')\
#     .csv("/FileStore/tables/ind-2.csv")
    
df=spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")

#df.printSchema()

df=df.withColumn("CXX", col("lat"))
df.createOrReplaceTempView("Data")
df1=spark.sql("select *from Data")
df1.write.mode("overwrite").csv("mycsv.csv")





# COMMAND ----------


/Users/zacayd@octopai.com/pythonX
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

import pyspark
from pyspark.sql import SparkSession



spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data = [("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  ]
columns = ["firstname","lastname","country","state"]
df = spark.createDataFrame(data = data, schema = columns)
df.show(truncate=False)

df.createOrReplaceTempView("df_tbl")

spark.sql('drop table  tt')
spark.sql('Create table tt as select *from df_tbl')
          

/Users/zacayd@octopai.com/Sql_PY
# MAGIC %scala
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
# MAGIC import za.co.absa.spline.producer.model.v1_1._
# MAGIC import za.co.absa.spline.producer.model._
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import scala.concurrent.duration.Duration
# MAGIC import scala.util.{Failure, Success, Try}
# MAGIC
# MAGIC val splineConf: Configuration = StandardSplineConfigurationStack(spark)
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
# MAGIC   //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
# MAGIC   //val test = dbutils.notebook.getContext.notebookPath
# MAGIC   val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC   val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC   val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC
# MAGIC   val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC   val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC   val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC
# MAGIC
# MAGIC
# MAGIC   val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC   val user = tagMap("user")
# MAGIC   val name = notebookPath(notebookPath.size-1)
# MAGIC   
# MAGIC   val notebookInfo = Map("notebookURL" -> notebookURL,  
# MAGIC                 "user" -> user, 
# MAGIC                 "workspaceName" ->workspaceName,
# MAGIC                 "name" -> name,
# MAGIC                 "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC                 "timestamp" -> System.currentTimeMillis)
# MAGIC   val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC   
# MAGIC   override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
# MAGIC     override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
# MAGIC     override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
# MAGIC     override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
# MAGIC     override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
# MAGIC     override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
# MAGIC   })
# MAGIC })

# COMMAND ----------

# MAGIC %scala
# MAGIC
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC spark.enableLineageTracking()
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %scala
# MAGIC case class Student(id: Int, name: String, addrId: Int)
# MAGIC
# MAGIC case class Address(id: Int, address: String)
# MAGIC
# MAGIC Seq(
# MAGIC   Student(111, "Amy Smith", 1),
# MAGIC   Student(222, "Bob Brown", 2)
# MAGIC ).toDS.write.mode("overwrite").parquet("/students")
# MAGIC
# MAGIC Seq(
# MAGIC   Address(1, "123 Park Ave, San Jose"),
# MAGIC   Address(2, "456 Taylor St, Cupertino")
# MAGIC ).toDS.write.mode("overwrite").parquet("/addresses")

# COMMAND ----------

# MAGIC %scala
# MAGIC val students = spark.read.parquet("/students")
# MAGIC val addresses = spark.read.parquet("/addresses")
# MAGIC
# MAGIC students
# MAGIC   .join(addresses)
# MAGIC   .where(addresses("id") === students("addrId"))
# MAGIC   .select("name", "address")
# MAGIC   .write.mode("append").parquet("/student_names_with_addresses")
/Users/zacayd@octopai.com/ScalaPython
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC     
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# Databricks notebook source
# MAGIC %md
# MAGIC ###### Purpose: Notebook for EDR Feed Raw Data Processing
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |JUL 18, 2022|Shyam Prasad| Created the notebook to insert data into datalake and ods staging tables|

# COMMAND ----------

# MAGIC %scala
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
# MAGIC import za.co.absa.spline.producer.model.v1_1._
# MAGIC import za.co.absa.spline.producer.model._
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import scala.concurrent.duration.Duration
# MAGIC import scala.util.{Failure, Success, Try}
# MAGIC val splineConf: Configuration = StandardSplineConfigurationStack(spark)
# MAGIC spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
# MAGIC //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
# MAGIC //val test = dbutils.notebook.getContext.notebookPath
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
# MAGIC override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
# MAGIC override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
# MAGIC override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
# MAGIC override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
# MAGIC override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
# MAGIC })
# MAGIC })

# COMMAND ----------

# DBTITLE 1,Get Variables from adf pipeline
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dbutils.widgets.removeAll()
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("trigger_key", "")
dbutils.widgets.text("FeedName", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("Source_container", "")
dbutils.widgets.text("Target_container", "")
dbutils.widgets.text("File_delimiter", "")
dbutils.widgets.text("File_Extension", "")
dbutils.widgets.text("hub_id", "")
PipelineName          = dbutils.widgets.get("PipelineName")
DataFactoryName       = dbutils.widgets.get("DataFactoryName")
trigger_key           = str(dbutils.widgets.get("trigger_key"))
FeedName              = dbutils.widgets.get("FeedName")
windowStartTime       = dbutils.widgets.get("windowStartTime")
PipelineRunId         = dbutils.widgets.get("PipelineRunId")
Source_container      = dbutils.widgets.get("Source_container")
Target_container      = dbutils.widgets.get("Target_container")
File_delimiter        = dbutils.widgets.get("File_delimiter")
File_Extension        = dbutils.widgets.get("File_Extension")
hub_id                = dbutils.widgets.get("hub_id")

# COMMAND ----------

# DBTITLE 1,Optimization
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

%sql
set spark.sql.shuffle.partitions = 400;
set spark.sql.files.maxPartitionBytes=1073741824;
set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;
set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;

# COMMAND ----------

# DBTITLE 1,Refresh Dimension Tables
# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC %sql
# MAGIC REFRESH TABLE dgcdimensions_dl.ContainerName;
# MAGIC REFRESH TABLE dgcdimensions_dl.AreaCode;
# MAGIC REFRESH TABLE stgods_dl.BulkSMS_JAM_logs;

# COMMAND ----------

# DBTITLE 1,Import Required Packages
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

import time
import os
import json
import pyspark
import requests
import pyspark.sql.functions as F
import pyspark.sql.types as T
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit,col,lower,regexp_replace,split,when,concat
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")

# COMMAND ----------

# DBTITLE 1,Create Path variable to fetch file from temp adls working
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dt = windowStartTime[:10].replace('-','')
dateId = windowStartTime.replace('-','').replace(':','')
file_location = "/mnt/"+Target_container+"/"+FeedName+"/working/"+Source_container+"/"+dt+"/"+dateId
duplicate_file_location = "/mnt/"+Target_container+"/"+FeedName+"/duplicate/"+Source_container+"/"+dt+"/"+dateId
table_name ='stg_BulkSMS_JAM_'+trigger_key+'_'+Source_container+'_'+dateId 
table_name = table_name.replace('-','')
path = "/mnt/stgods/BulkSMS_JAM/stg_BulkSMS_JAM"+dateId
log_table = "BulkSMS_JAM_logs"
print(table_name)
print(file_location)

# COMMAND ----------

# DBTITLE 1,Check Notebook status for current Load timestamp
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

NotebookStatus = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
NS_BulkSMS_JAM = NotebookStatus.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
if NS_BulkSMS_JAM == ['Y']:
  dbutils.notebook.exit( FeedName+" Notebook Status is already successful for Load_TS = "+windowStartTime+" and Source_container = "+Source_container+" and  Trigger_Key = "+trigger_key+" and Flag_NotebookStatus = "+str(NS_BulkSMS_JAM))
else:
  print("Execute Further Notebook Commands")

# COMMAND ----------

# DBTITLE 1,Duplicate File Function Check
# MAGIC %run /Shared/DWH_Transformation/TOOLS/CheckDuplicateFile

# COMMAND ----------

# DBTITLE 1,Duplicate File Check
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

#filedirectory="/*/*/*/lz/*"
filedirectory="/*"
dflogsduplicate = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
DP          = dflogsduplicate.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if DP != ['Y']:  
  dfdffileinfo=checkDuplicateFile(File_Location=str(file_location),File_Directory=str(filedirectory),FeedName=str(FeedName),Trigger_Key=str(trigger_key),WindowStartTime=str(windowStartTime),PipelineRunID=str(PipelineRunId),Source_Container=Source_container,Target_Container=('Target_container'))

# COMMAND ----------

# DBTITLE 1,Schema Enforcement
# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC nofilefound=False
# MAGIC schemaBulkSMS_JAM = StructType([ \
# MAGIC StructField("TIMESTAMP",StringType(),True),\
# MAGIC StructField("MESSAGE_ID",StringType(),True),\
# MAGIC StructField("SYSTEM_ID",StringType(),True),\
# MAGIC StructField("IP_ADDRESS",StringType(),True),\
# MAGIC StructField("SOURCE_ADDRESS",StringType(),True),\
# MAGIC StructField("MSISDN",StringType(),True),\
# MAGIC StructField("IMSI",StringType(),True),\
# MAGIC StructField("MSC",StringType(),True),\
# MAGIC StructField("RESULT",StringType(),True),\
# MAGIC StructField("ERRORCODE",StringType(),True),\
# MAGIC StructField("ERRORCAUSE",StringType(),True),\
# MAGIC StructField("FILLER_A_registered",StringType(),True),\
# MAGIC StructField("FILLER_B_errorhandlertable",StringType(),True),\
# MAGIC StructField("corruptrecord",StringType(),True) \
# MAGIC ])
# MAGIC 
# MAGIC spark.sql("set spark.sql.files.ignoreCorruptFiles=true")
# MAGIC 
# MAGIC try:
# MAGIC   BulkSMS_JAM_with_schema = spark.read.option("header", "false")\
# MAGIC   .schema(schemaBulkSMS_JAM)\
# MAGIC   .option("sep", File_delimiter)\
# MAGIC   .option("quote", "\"")\
# MAGIC   .option("multiline","true")\
# MAGIC   .option("encoding", "US-ASCII")\
# MAGIC   .csv(file_location+filedirectory)
# MAGIC 
# MAGIC   BulkSMS_JAM_with_schemaAllhubs=      BulkSMS_JAM_with_schema.withColumn("Load_TS",lit(windowStartTime)).withColumn("FileName",input_file_name()).\
# MAGIC   withColumn("Last_Updated_By",lit("BAU")).withColumn("Data_dt",lit(dt)).\
# MAGIC   withColumn("Trigger_Key",lit(trigger_key))
# MAGIC   BulkSMS_JAM_with_schemaAllhubs.createOrReplaceTempView("temp_BulkSMS_JAM")
# MAGIC   Row_count = BulkSMS_JAM_with_schemaAllhubs.count()
# MAGIC   print(Row_count)
# MAGIC except Exception as e:
# MAGIC   print(file_location+filedirectory)
# MAGIC   print(e)
# MAGIC   nofilefound=True

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC %sql
# MAGIC select * from temp_BulkSMS_JAM

# COMMAND ----------

# DBTITLE 1,If File Not Found
if nofilefound is True:
  dbutils.notebook.exit("No Files Found")

# COMMAND ----------

# DBTITLE 1,Snapshot_Date Helper
# MAGIC %run /Shared/DWH_Transformation/TOOLS/Func_Snapshot_Date

# COMMAND ----------

# DBTITLE 1,Area_Code Helper
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dfcode=spark.sql('''SELECT MARKET_ID, CAST(MARKET_SH_CODE1 AS STRING) AS MARKET_SH_CODE1, LINE_OF_BUSINESS FROM dgcdimensions_dl.areacode 
where UPPER(LINE_OF_BUSINESS)='GSM' ''')
dfcode.createOrReplaceTempView("temp_AreaCode")

# COMMAND ----------

# DBTITLE 1,Market_Code Helper
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dfHUBID=spark.sql("SELECT DISTINCT Market_Key AS MARKET_ID,HUB_ID FROM ods_dl.market")
dfHUBID.createOrReplaceTempView("temp_MarketID")

# COMMAND ----------

# DBTITLE 1,Data Preparation
# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC BulkSMS_JAM = spark.sql('''
# MAGIC SELECT
# MAGIC CASE WHEN TRIM(b.MARKET_ID) IS NULL THEN -1 ELSE TRIM(b.MARKET_ID) END AS MARKET_ID
# MAGIC ,TRIM(a.TIMESTAMP) as TIMESTAMP
# MAGIC ,TRIM(a.MESSAGE_ID)		AS	MESSAGE_ID
# MAGIC ,TRIM(a.SYSTEM_ID) 				AS	SYSTEM_ID
# MAGIC ,TRIM(a.IP_ADDRESS)				AS	IP_ADDRESS
# MAGIC ,TRIM(a.SOURCE_ADDRESS)			AS	SOURCE_ADDRESS
# MAGIC ,TRIM(a.MSISDN)			AS	MSISDN
# MAGIC ,TRIM(a.IMSI)		AS	IMSI
# MAGIC ,TRIM(a.MSC)						AS	MSC
# MAGIC ,TRIM(a.RESULT)						AS	RESULT
# MAGIC ,TRIM(a.ERRORCODE)						AS	ERRORCODE
# MAGIC ,TRIM(a.ERRORCAUSE)				AS	ERRORCAUSE
# MAGIC ,TRIM(a.FILLER_A_registered)						AS	FILLER_A_registered
# MAGIC ,TRIM(a.FILLER_B_errorhandlertable)			AS	FILLER_B_errorhandlertable
# MAGIC ,TRIM(a.corruptrecord) as CORRUPTRECORD,
# MAGIC        load_ts,
# MAGIC        substring_index(a.FileName,'/',-1) AS FileName,
# MAGIC        last_updated_by,
# MAGIC        data_dt,
# MAGIC        trigger_key,
# MAGIC        d.hub_id,
# MAGIC       CASE 
# MAGIC 			WHEN (LENGTH(TRIM(a.MSISDN)) BETWEEN 10
# MAGIC 					AND 12
# MAGIC                 AND TRIM(a.MSISDN) RLIKE '^[0-9]*$'
# MAGIC                 AND (TRIM(a.MSISDN) IS NOT NULL) )        
# MAGIC 				THEN 1
# MAGIC 			ELSE 0
# MAGIC 			END AS IsValid_MSISDN
# MAGIC        ,CASE WHEN c.Container_Name IS NULL THEN "group" ELSE c.Container_Name END AS Container_Name
# MAGIC        from temp_BulkSMS_JAM a
# MAGIC LEFT JOIN temp_AreaCode AS b ON SUBSTRING(TRIM(a.MSISDN), 1, 5) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 4) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 3) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 2) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 1) = b.MARKET_SH_CODE1
# MAGIC LEFT JOIN dgcdimensions_dl.ContainerName c ON b.MARKET_ID = TRIM(c.MARKET_ID)
# MAGIC LEFT JOIN temp_MarketId d ON b.MARKET_ID = TRIM(d.MARKET_ID)
# MAGIC ''')
# MAGIC 
# MAGIC BulkSMS_JAM_allhubs_final = BulkSMS_JAM.withColumn("snapshot_dt",find_date_v2(col("FileName"))).withColumn("Load_TS",lit(windowStartTime)).\
# MAGIC withColumn("Last_Updated_By",lit("BAU")).withColumn("Data_dt",lit(dt)).withColumn("trigger_key",lit(trigger_key))
# MAGIC BulkSMS_JAM_allhubs_final.createOrReplaceTempView("temp_BulkSMS_JAM_allhubs_final")

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC %sql 
# MAGIC select * from temp_BulkSMS_JAM_allhubs_final

# COMMAND ----------

# DBTITLE 1,Temp Table Data Load
# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC dflogstemp = spark.sql("SELECT DISTINCT Flag_Temp FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
# MAGIC TF         = dflogstemp.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()
# MAGIC if spark._jsparkSession.catalog().tableExists('stgods_dl', table_name) and TF == ['Y']:
# MAGIC   print("stgods_dl."+table_name+" table exists")
# MAGIC else:
# MAGIC   spark.sql("CREATE OR REPLACE TABLE stgods_dl."+table_name+"  USING DELTA LOCATION '"+path+"' PARTITIONED BY (Container_Name) AS SELECT * FROM temp_BulkSMS_JAM_allhubs_final")
# MAGIC   for i in range(1,5,1):
# MAGIC     try:
# MAGIC       spark.sql("DELETE FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
# MAGIC       break;
# MAGIC     except Exception as e2:
# MAGIC       if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e2)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e2)):
# MAGIC         time.sleep(30)
# MAGIC         continue
# MAGIC       else:
# MAGIC         raise Exception(e2)
# MAGIC    
# MAGIC   ##INSERT INTO LOG TABLE 
# MAGIC   spark.sql("INSERT INTO stgods_dl."+log_table+" VALUES ('"+windowStartTime+"','"+dt+"','"+Source_container+"',int('"+trigger_key+"'),'N','N','N','N','N','N','N') ")
# MAGIC   
# MAGIC   ##Update Logging Flag
# MAGIC   for i in range(1,5,1):
# MAGIC     try:
# MAGIC       spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Temp = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
# MAGIC       print("Data inserted into stgods_dl."+table_name+" and Flag_Temp updated to Y in stgods_dl."+log_table+" for  Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
# MAGIC       break;
# MAGIC     except Exception as e3:
# MAGIC       if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e3):
# MAGIC         time.sleep(30)
# MAGIC         continue
# MAGIC       else:
# MAGIC         raise Exception(e3)

# COMMAND ----------

# DBTITLE 1,For Datalake Layer Processing
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogsDL = spark.sql("SELECT DISTINCT Flag_DataLake FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TDL      = dflogsDL.select("Flag_DataLake").distinct().rdd.flatMap(lambda x: x).collect()
if TDL == ['N']:
  MK= BulkSMS_JAM_allhubs_final.select("Container_Name").distinct().rdd.flatMap(lambda x: x).collect()
  print(MK)
  for t in MK :
    
    ##DELETE
    for i in range(1,5,1):
      try:
        #spark.sql("truncate table "+t+"_dl.BulkSMS_JAM")
        spark.sql(" DELETE FROM "+t+"_dl.BulkSMS_JAM where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e4:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e4)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e4)):
          time.sleep(30)
          continue
        else:
          raise Exception(e4)
   
    ##INSERT INTO COUNTRY WISE DELTA TABLE
    spark.sql(" INSERT INTO TABLE "+t+"_dl.BulkSMS_JAM select TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord,int(MARKET_ID),Load_TS,FileName,Last_Updated_By,Data_dt,int(Trigger_Key),int(HUB_ID),IsValid_MSISDN,snapshot_dt FROM stgods_dl."+table_name+" WHERE CORRUPTRECORD is null and IsValid_MSISDN in (1,2) and Container_Name = '"+t+"'")
  
  ##Update logging flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Datalake = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into BulkSMS_JAM Data Lake tables and Flag_Datalake updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+"  and Source_Container = "+Source_container+" and  Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      time.sleep(30)
      continue
else:
  print("Data in BulkSMS_JAM Data Lake tables exists for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Datalake = "+str(TDL))

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC %sql
# MAGIC select * from jamaica_dl.BulkSMS_JAM

# COMMAND ----------

# DBTITLE 1,Invalid Records Segregation and alertitled
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

Schema_Mismatch_cnt = spark.sql('''select * from stgods_dl.'''+table_name+''' Where IsValid_MSISDN = 0  
                                   AND ((corruptrecord IS NOT NULL AND corruptrecord <> " " AND corruptrecord <> "")
                                   OR  ((corruptrecord IS NULL OR corruptrecord = " " OR corruptrecord = "")
                                   AND ((TRIM(MSISDN) NOT RLIKE '^[0-9]*$' AND TRIM(MSISDN) IS NOT NULL 
                                         AND TRIM(MSISDN) <> "" AND TRIM(MSISDN) <> " " ))))''').count()

Invalid_MSISDNs_cnt = spark.sql("select * from stgods_dl."+table_name+" Where IsValid_MSISDN = 0 AND (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ') AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL OR TRIM(MSISDN) <> ' '  OR TRIM(MSISDN) <> '') ").count()

dflogserror = spark.sql("SELECT DISTINCT Flag_Error FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TE          = dflogserror.select("Flag_Error").distinct().rdd.flatMap(lambda x: x).collect()
                                
if TE == ['N']:
  if Schema_Mismatch_cnt!=0 or Invalid_MSISDNs_cnt !=0:
    ##DELETE
    for i in range(1,5,1):
      try:
        spark.sql(" DELETE FROM error_dl.reject_BulkSMS_JAM where Error_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND Trigger_Key = int('"+trigger_key+"') AND Alert_Type = 'Invalid_Data' ")
        break;
      except Exception as e9:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e9)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e9)):
          time.sleep(30)
          continue
        else:
          raise Exception(e9)
        
    ##INSERT INTO ERROR and Alert control Table  
    spark.sql('''INSERT INTO TABLE error_dl.reject_BulkSMS_JAM SELECT TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord ,MARKET_ID ,Load_TS ,FileName ,Last_Updated_By ,Data_dt as Error_dt,Trigger_Key,IsValid_MSISDN ,HUB_ID, snapshot_dt
                                           ,CASE WHEN IsValid_MSISDN = 0
                                           AND (corruptrecord IS NULL OR corruptrecord = "" OR corruptrecord = " ") 
                                           AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL 
                                                 OR TRIM(MSISDN) <> " "  OR TRIM(MSISDN) <> "")
                                           THEN 'Invalid_MSISDNs' 
                                           ELSE 'Schema_Mismatch' END AS Error_Type FROM stgods_dl.'''+table_name+''' as a where IsValid_MSISDN = 0 ''')
		
		
			
    
    data_error = '{"title": "' +FeedName+ ' Invalid Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Erroneous Records Moved To {dgc{xxx}datalake01}","error":{"Message":"Invalid Records","TableName":"error_dl.reject_BulkSMS_JAM","Path":"/error/BulkSMS_JAM/reject_BulkSMS_JAM","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt)+ '","Total Invalid Records": "' +str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+ '"}}}'
    
    spark.sql('''
     INSERT INTO cfw.etl_alert_control 
     select DISTINCT int("'''+str(trigger_key)+'''") AS trigger_key
                     ,"'''+str(FeedName)+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+str(windowStartTime)+'''") AS Load_TS
                     ,"Invalid_Data" as Alert_Type
                     ,'"'''+str(data_error)+'''"' AS Alert_Reason
                     ,"'''+str(Source_container)+'''" as Data_Storage
                     ,"error_dl.reject_BulkSMS_JAM" as Data_location
                     ,int("'''+str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+'''") as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID  ''')
    
  #Update logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Error  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'")
      print("Data inserted into error_dl.reject_BulkSMS_JAM and Flag_Error updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(180)
        continue
      else:
        raise Exception(e)

else:
  print("Data exists in error_dl.reject_BulkSMS_JAM table for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TE))

# COMMAND ----------

# DBTITLE 1,For ODS Layer Processing
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
  ##DELETE
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM stgods_dl.stgods_BulkSMS_JAM where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"')")
      break;
    except Exception as e6:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e6)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e6)):
        time.sleep(30)
        continue
      else:
        raise Exception(e6)
   
  ##INSERT INTO stgods_dl.stgods_BulkSMS_JAM
  spark.sql("INSERT INTO TABLE stgods_dl.stgods_BulkSMS_JAM select TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,CASE WHEN MSISDN not like '5%' THEN '5'||substr(MSISDN,3,2)||substr(MSISDN,2,10) ELSE MSISDN  END as MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord,MARKET_ID,Load_TS,FileName,Last_Updated_By,Data_dt,Trigger_Key,HUB_ID, IsValid_MSISDN,snapshot_dt FROM temp_BulkSMS_JAM_allhubs_final ")

            
  ##Update Logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_StgODS  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl.stgods_BulkSMS_JAM and Flag_StgODS updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e) 
else:
  print("Data exists in stgods_dl.stgods_BulkSMS_JAM table for Load_TS = "+windowStartTime+" AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'and  Flag_StgODS = "+str(TT))

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC %sql
# MAGIC select * from stgods_dl.stgods_BulkSMS_JAM 

# COMMAND ----------

# DBTITLE 1,cfw.Table_load_status INSERT
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs = spark.sql("SELECT DISTINCT Flag_Stgods FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"'  AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TT=dflogs.select("Flag_Stgods").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['Y']:
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM cfw.Table_load_status WHERE Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key='"+trigger_key+"'  AND Hub_id='"+hub_id+"' ")
      break;
    except Exception as e8:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
        time.sleep(30)
        continue
      else:
        raise Exception(e8)
## INSERT INTO cfw.Table_load_status 
  spark.sql('''
  INSERT INTO cfw.Table_load_status 
  SELECT DISTINCT CAST(Trigger_Key as int) AS Trigger_Key
                 ,CAST(Load_TS AS TIMESTAMP) AS Load_TS
                 ,snapshot_dt
                 ,CAST(hub_id as int) AS HUB_ID
                 ,"'''+FeedName +'''" AS FeedName
                 ,"BulkSMS" AS Source_System
                 ,"stgods_dl" as target_db
                 ,"stgods_BulkSMS_JAM" as target_table_name
                 ,'Y' as load_status
                 ,count(*) as total_records
                 ,count(distinct FileName) as total_files
                 ,"'''+PipelineRunId+'''" AS Pipeline_Run_Id
                 ,current_timestamp()
                 ,null FROM stgods_dl.'''+table_name+''' group by snapshot_dt,hub_id,Trigger_Key,Load_TS ''')
  print("Data inserted into cfw.Table_load_status")

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC %sql
# MAGIC select * from cfw.table_load_status where FeedName = 'BulkSMS_JAM'

# COMMAND ----------

# DBTITLE 1,Audit Data Collection
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

df_audit = spark.sql(''' SELECT LOAD_TS
		                       ,FileName
                               ,count(*) AS total_RecordCount
		                       ,SUM(CASE 
                                        WHEN FileName IS NOT NULL AND IsValid_MSISDN in (1,2)
					                    THEN 1
				                    ELSE 0
				                    END) AS RecordCount_FN
		                       ,SUM(
                                    CASE
                                    WHEN IsValid_MSISDN  = 0 
                                    and (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ') 
                                   AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL OR TRIM(MSISDN) <> ' ' 
                                    OR TRIM(MSISDN) <> '')
                                    THEN 1 
                                  
                                  ELSE 0
                                  END ) AS cnt_InvalidMSISDNs
                              ,SUM(CASE 
		                      		     WHEN IsValid_MSISDN in (1,2)
					                     AND ((corruptrecord IS NOT NULL AND corruptrecord <> " " AND corruptrecord <> ""))
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_SchemaMismatch
	                     FROM stgods_dl.'''+table_name+'''
	                     GROUP BY FileName,LOAD_TS ''')
df_audit.createOrReplaceTempView("df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,For Rejected Files Audit
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

spark.sql("DROP TABLE IF EXISTS stgods_dl.BulkSMS_JAM_audit_"+trigger_key)
spark.sql("CREATE TABLE stgods_dl.BulkSMS_JAM_audit_"+trigger_key+" AS SELECT DISTINCT Load_TS,FileName AS FileName ,total_RecordCount,RecordCount_FN,cnt_InvalidMSISDNs,cnt_SchemaMismatch FROM df_audit_BulkSMS_JAM ")

# COMMAND ----------

# DBTITLE 1,cfw.DL_File_Details Table :Valid and Non-Empty Files Audit Data 
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

spark.sql("set spark.databricks.delta.checkLatestSchemaOnRead=FALSE")
dflogsaudit = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TA          = dflogsaudit.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if TA == ['N']:
  for i in range(1,5,1):
    try:
      spark.sql('''MERGE INTO cfw.DL_File_Details AS TARGET 
      USING (
      select a.*
            ,CONCAT('{"Invalid MESSAGEIDs":"',cnt_InvalidMSISDNs,'","Schema Mismatch":"',cnt_SchemaMismatch,'"}') as error_reason from df_audit_BulkSMS_JAM a) SOURCE 
      on trim(SOURCE.FileName)=trim(TARGET.FileName) 
      and TARGET.File_load_status != 'D' 
      and TARGET.Load_TS = "'''+ windowStartTime+'''" 
      and TARGET.TRIGGER_KEY="'''+trigger_key+'''" 
      WHEN MATCHED  THEN UPDATE SET 
      TARGET.File_load_status='Y'
     ,TARGET.Total_Records=SOURCE.total_RecordCount
     ,TARGET.valid_records=SOURCE.RecordCount_FN
     ,TARGET.error_records=SOURCE.cnt_InvalidMSISDNs + SOURCE.cnt_SchemaMismatch
     ,TARGET.error_reason= SOURCE.error_reason ''')
      
      #logging
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Audit  = 'Y',Flag_reject = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'  ")
      print("Data inserted into cfw.DL_File_Details and Flag_Audit updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e)  
else:
  print("Data exists in cfw.DL_File_Details table for Load_TS = "+windowStartTime+"  AND Source_Container = "+Source_container+" AND Trigger_Key = '"+trigger_key+"' and  Flag_Error = "+str(TA))

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

db = "stgods_dl"
#FeedName             = "BulkSMS_JAM"
feedaudit             = "BulkSMS_JAM_audit_"+trigger_key
foldername            = "BulkSMS_JAM/"            
tempaudit_rejected    = "BulkSMS_JAM_rejected_"+trigger_key
file_location_reject  = "/mnt/"+Target_container+"/"+FeedName+"/reject/"+Source_container+"/"+dt+"/"+dateId
file_location_working = file_location+filedirectory

#Rejected Files Flow
dflogsreject = spark.sql("SELECT DISTINCT Flag_reject FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TR           = dflogsreject.select("Flag_reject").distinct().rdd.flatMap(lambda x: x).collect()
if TR  == ['N']:
  dbutils.notebook.run("/Shared/DWH_Transformation/DATALAKE/Generic/RejectedFiles", 43200,{"windowStartTime": windowStartTime, "PipelineRunId": PipelineRunId, "DataFactoryName":DataFactoryName,"PipelineName":PipelineName,"trigger_key":trigger_key,"db":db,"feedaudit":feedaudit,"foldername":foldername,"file_location":file_location,"file_location_reject":file_location_reject,"file_location_working":file_location_working,"tempaudit_rejected":tempaudit_rejected,"FeedName":FeedName,"Source_container":Source_container})
  
  ##Update Logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Reject  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      print("Data Moved to rejected folder for rejected files and to cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+"  and Source_Container = "+Source_container+" and  Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e)
else:
  print("Data exists in cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TR))

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC 
# MAGIC %sql
# MAGIC select * from cfw.dl_file_details where feedname = 'BulkSMS_JAM'

# COMMAND ----------

# DBTITLE 1,Alert details capture for non true duplicate rows for composite PK
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dfPK_Alert = spark.sql(''' WITH CTE_PK(SELECT TIMESTAMP,MESSAGE_ID,SYSTEM_ID,count(*) FROM stgods_dl.'''+table_name+''' group by TIMESTAMP,MESSAGE_ID,SYSTEM_ID HAVING count(*)>1) SELECT * FROM CTE_PK ''').count()
print(dfPK_Alert)

if dfPK_Alert != 0:
  data_PK = '{"title": "' +FeedName+ ' Duplicate Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Duplicate Records Found After Primary Key Validation","error":{"Message":"Duplicate Records","TableName":"stgods_dl.ods_'+FeedName.lower()+',"Requirement":"One record per {CDRID} column/s must be present in '+FeedName+'","Total Duplicate Records":"' +str(dfPK_Alert)+ '"}}'  
  
  for i in range(1,5,1):
    try:
      spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key = int('"+trigger_key+"') AND Alert_Type = 'Duplicate_Records' ")
      break;
    except Exception as e10:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e10)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e10)):
        time.sleep(30)
        continue
      else:
        raise Exception(e10)
        
  ##INSERT INTO cfw.etl_alert_control 
  spark.sql('''
  INSERT INTO cfw.etl_alert_control 
  select DISTINCT int("'''+trigger_key+'''") AS trigger_key
                     ,"'''+FeedName+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+windowStartTime+'''") AS Load_TS
                     ,"Duplicate_Records" as Alert_Type
                     ,'"'''+data_PK+'''"' AS Alert_Reason
                     ,"'''+Source_container+'''" as Data_Storage
                     ,LOWER(CONCAT("stgods_dl.ods_","'''+FeedName+'''")) as Data_location
                     ,"'''+str(dfPK_Alert)+'''" as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID 
           ''')
  print("Duplicate number of rows :",dfPK_Alert)
  print("Data inserted into cfw.etl_alert_control for stgods_dl.ods_"+FeedName+" for PK Duplicate Records")

# COMMAND ----------

# DBTITLE 1, Notebook status Logging
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_NotebookStatus  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Flag_Temp = 'Y' AND Flag_Datalake = 'Y' AND Flag_StgODS = 'Y' AND Flag_Error = 'Y' AND Flag_Audit = 'Y' AND Flag_reject = 'Y' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      break;
    except Exception as e14:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e14):
        time.sleep(30)
        continue
      else:
        raise Exception(e14)

# COMMAND ----------

# DBTITLE 1, Clean up and Exit
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs_final = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
NFS =dflogs_final.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
print(NFS)
if NFS == ['Y']:
  spark.sql(" DROP TABLE IF EXISTS stgods_dl."+table_name+" ")
  dbutils.fs.rm(path,recurse=True)
  dbutils.notebook.exit("Successful")

/Shared/Untitled Notebook 2023-05-23 09:28:27
# MAGIC %md
# MAGIC ###### Purpose: Notebook for EDR Feed Raw Data Processing
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |JUL 18, 2022|Shyam Prasad| Created the notebook to insert data into datalake and ods staging tables|

# COMMAND ----------

# MAGIC %sql
# MAGIC select *from  stgods_dl.stg_bulksms_jam_18_jamicasrc_2023012923000000
# MAGIC

# COMMAND ----------

# MAGIC %scala
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
# MAGIC import za.co.absa.spline.producer.model.v1_1._
# MAGIC import za.co.absa.spline.producer.model._
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import scala.concurrent.duration.Duration
# MAGIC import scala.util.{Failure, Success, Try}
# MAGIC val splineConf: Configuration = StandardSplineConfigurationStack(spark)
# MAGIC spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
# MAGIC //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
# MAGIC //val test = dbutils.notebook.getContext.notebookPath
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
# MAGIC override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
# MAGIC override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
# MAGIC override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
# MAGIC override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
# MAGIC override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
# MAGIC })
# MAGIC })

# COMMAND ----------

# DBTITLE 1,Get Variables from adf pipeline
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dbutils.widgets.removeAll()
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("trigger_key", "")
dbutils.widgets.text("FeedName", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("Source_container", "")
dbutils.widgets.text("Target_container", "")
dbutils.widgets.text("File_delimiter", "")
dbutils.widgets.text("File_Extension", "")
dbutils.widgets.text("hub_id", "")
PipelineName          = dbutils.widgets.get("PipelineName")
DataFactoryName       = dbutils.widgets.get("DataFactoryName")
trigger_key           = str(dbutils.widgets.get("trigger_key"))
FeedName              = dbutils.widgets.get("FeedName")
windowStartTime       = dbutils.widgets.get("windowStartTime")
PipelineRunId         = dbutils.widgets.get("PipelineRunId")
Source_container      = dbutils.widgets.get("Source_container")
Target_container      = dbutils.widgets.get("Target_container")
File_delimiter        = dbutils.widgets.get("File_delimiter")
File_Extension        = dbutils.widgets.get("File_Extension")
hub_id                = dbutils.widgets.get("hub_id")

# COMMAND ----------

# DBTITLE 1,Optimization
# MAGIC
# MAGIC %sql
# MAGIC set spark.sql.shuffle.partitions = 400;
# MAGIC set spark.sql.files.maxPartitionBytes=1073741824;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- create Database stgods_dl;
# MAGIC --create Database dgcdimensions_dl;
# MAGIC -- create table dgcdimensions_dl.ContainerName 
# MAGIC -- ( ContainerName string,
# MAGIC --   MARKET_ID string);
# MAGIC   
# MAGIC  -- alter table dgcdimensions_dl.ContainerName  add column MARKET_ID string;
# MAGIC   alter table dgcdimensions_dl.ContainerName  add column Container_Name string;
# MAGIC   
# MAGIC
# MAGIC -- create table dgcdimensions_dl.AreaCode 
# MAGIC -- ( AreaCode string );
# MAGIC
# MAGIC -- drop table dgcdimensions_dl.areacode ;
# MAGIC -- CREATE TABLE  dgcdimensions_dl.areacode 
# MAGIC -- ( MARKET_ID STRING ,
# MAGIC -- MARKET_SH_CODE1 int,
# MAGIC -- LINE_OF_BUSINESS STRING
# MAGIC
# MAGIC -- )
# MAGIC
# MAGIC -- drop table ods_dl.market;
# MAGIC -- CREATE TABLE  ods_dl.market 
# MAGIC -- ( Market_Key STRING ,
# MAGIC -- HUB_ID STRING
# MAGIC -- );
# MAGIC --SELECT DISTINCT Market_Key AS MARKET_ID,HUB_ID FROM ods_dl.market
# MAGIC
# MAGIC ;
# MAGIC

# COMMAND ----------

# DBTITLE 1,Refresh Dimension Tables
# MAGIC %sql
# MAGIC REFRESH TABLE dgcdimensions_dl.ContainerName;
# MAGIC REFRESH TABLE dgcdimensions_dl.AreaCode;
# MAGIC REFRESH TABLE stgods_dl.BulkSMS_JAM_logs;

# COMMAND ----------

# DBTITLE 1,Import Required Packages

import time
import os
import json
import pyspark
import requests
import pyspark.sql.functions as F
import pyspark.sql.types as T
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit,col,lower,regexp_replace,split,when,concat
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")

# COMMAND ----------

#dbutils.fs.unmount(mount_point = f"/mnt/{storage_account_name}/{container_name}")
# storage_account_name='zacaystorage'
# container_name = "raw"
# mount_point = f"/mnt/{storage_account_name}/{container_name}"

# # dbutils.fs.unmount(mount_point)

# dbutils.fs.mount(
#   source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
#   mount_point = f"mnt/stgods/BulkSMS_JAM"+date,
#   extra_configs = configs)

# COMMAND ----------

# DBTITLE 1,Create Path variable to fetch file from temp adls working


dt = windowStartTime[:10].replace('-','')
dateId = windowStartTime.replace('-','').replace(':','')
# file_location = "/mnt/"+Target_container+"/"+FeedName+"/working/"+Source_container+"/"+dt+"/"+dateId
file_location="/FileStore/tables"
duplicate_file_location = "/mnt/"+Target_container+"/"+FeedName+"/duplicate/"+Source_container+"/"+dt+"/"+dateId
table_name ='stg_BulkSMS_JAM_'+trigger_key+'_'+Source_container+'_'+dateId 
table_name = table_name.replace('-','')
path = "/mnt/stgods/BulkSMS_JAM/stg_BulkSMS_JAM"+dateId
log_table = "BulkSMS_JAM_logs"
print(table_name)
print(file_location)

# COMMAND ----------

# DBTITLE 1,Check Notebook status for current Load timestamp

NotebookStatus = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
NS_BulkSMS_JAM = NotebookStatus.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
if NS_BulkSMS_JAM == ['Y']:
  dbutils.notebook.exit( FeedName+" Notebook Status is already successful for Load_TS = "+windowStartTime+" and Source_container = "+Source_container+" and  Trigger_Key = "+trigger_key+" and Flag_NotebookStatus = "+str(NS_BulkSMS_JAM))
else:
  print("Execute Further Notebook Commands")

# COMMAND ----------

# DBTITLE 1,Duplicate File Function Check
#%run /Shared/DWH_Transformation/TOOLS/CheckDuplicateFile

# COMMAND ----------

# DBTITLE 1,Duplicate File Check

#filedirectory="/*/*/*/lz/*"
filedirectory="/*"
# dflogsduplicate = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
# DP = dflogsduplicate.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
# if DP != ['Y']:  dfdffileinfo=checkDuplicateFile(File_Location=str(file_location),File_Directory=str(filedirectory),FeedName=str(FeedName),Trigger_Key=str(trigger_key),WindowStartTime=str(windowStartTime),PipelineRunID=str(PipelineRunId),Source_Container=
#                                                  Source_container,Target_Container=('Target_container'))


# COMMAND ----------

# DBTITLE 1,Schema Enforcement

nofilefound=False
schemaBulkSMS_JAM = StructType([ \
StructField("TIMESTAMP",StringType(),True),\
StructField("MESSAGE_ID",StringType(),True),\
StructField("SYSTEM_ID",StringType(),True),\
StructField("IP_ADDRESS",StringType(),True),\
StructField("SOURCE_ADDRESS",StringType(),True),\
StructField("MSISDN",StringType(),True),\
StructField("IMSI",StringType(),True),\
StructField("MSC",StringType(),True),\
StructField("RESULT",StringType(),True),\
StructField("ERRORCODE",StringType(),True),\
StructField("ERRORCAUSE",StringType(),True),\
StructField("FILLER_A_registered",StringType(),True),\
StructField("FILLER_B_errorhandlertable",StringType(),True),\
StructField("corruptrecord",StringType(),True) \
])

spark.sql("set spark.sql.files.ignoreCorruptFiles=true")

try:
   BulkSMS_JAM_with_schema = spark.read.option("header", "false")\
  .schema(schemaBulkSMS_JAM)\
  .option("sep", File_delimiter)\
  .option("quote", "\"")\
  .option("multiline","true")\
  .option("encoding", "US-ASCII")\
  .csv(file_location+filedirectory)

   BulkSMS_JAM_with_schemaAllhubs=BulkSMS_JAM_with_schema.withColumn("Load_TS",lit(windowStartTime)).withColumn("FileName",input_file_name()).\
   withColumn("Last_Updated_By",lit("BAU")).withColumn("Data_dt",lit(dt)).\
   withColumn("Trigger_Key",lit(trigger_key))
   BulkSMS_JAM_with_schemaAllhubs.createOrReplaceTempView("temp_BulkSMS_JAM")
   Row_count = BulkSMS_JAM_with_schemaAllhubs.count()
   print(Row_count)
except Exception as e:
  print(file_location+filedirectory)
  print(e)
  nofilefound=True

# COMMAND ----------

# MAGIC
# MAGIC %sql
# MAGIC select * from temp_BulkSMS_JAM

# COMMAND ----------

# DBTITLE 1,If File Not Found
if nofilefound is True:
  dbutils.notebook.exit("No Files Found")

# COMMAND ----------

# DBTITLE 1,Snapshot_Date Helper
# MAGIC %run /Shared/DWH_Transformation/TOOLS/Func_Snapshot_Date

# COMMAND ----------

# DBTITLE 1,Area_Code Helper
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dfcode=spark.sql('''SELECT MARKET_ID, CAST(MARKET_SH_CODE1 AS STRING) AS MARKET_SH_CODE1, LINE_OF_BUSINESS FROM dgcdimensions_dl.areacode 
where UPPER(LINE_OF_BUSINESS)='GSM' ''')
dfcode.createOrReplaceTempView("temp_AreaCode")

# COMMAND ----------

# DBTITLE 1,Market_Code Helper
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dfHUBID=spark.sql("SELECT DISTINCT Market_Key AS MARKET_ID,HUB_ID FROM ods_dl.market")
dfHUBID.createOrReplaceTempView("temp_MarketID")

# COMMAND ----------

# DBTITLE 1,Data Preparation
# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC
# MAGIC BulkSMS_JAM = spark.sql('''
# MAGIC SELECT
# MAGIC CASE WHEN TRIM(b.MARKET_ID) IS NULL THEN -1 ELSE TRIM(b.MARKET_ID) END AS MARKET_ID
# MAGIC ,TRIM(a.TIMESTAMP) as TIMESTAMP
# MAGIC ,TRIM(a.MESSAGE_ID)		AS	MESSAGE_ID
# MAGIC ,TRIM(a.SYSTEM_ID) 				AS	SYSTEM_ID
# MAGIC ,TRIM(a.IP_ADDRESS)				AS	IP_ADDRESS
# MAGIC ,TRIM(a.SOURCE_ADDRESS)			AS	SOURCE_ADDRESS
# MAGIC ,TRIM(a.MSISDN)			AS	MSISDN
# MAGIC ,TRIM(a.IMSI)		AS	IMSI
# MAGIC ,TRIM(a.MSC)						AS	MSC
# MAGIC ,TRIM(a.RESULT)						AS	RESULT
# MAGIC ,TRIM(a.ERRORCODE)						AS	ERRORCODE
# MAGIC ,TRIM(a.ERRORCAUSE)				AS	ERRORCAUSE
# MAGIC ,TRIM(a.FILLER_A_registered)						AS	FILLER_A_registered
# MAGIC ,TRIM(a.FILLER_B_errorhandlertable)			AS	FILLER_B_errorhandlertable
# MAGIC ,TRIM(a.corruptrecord) as CORRUPTRECORD,
# MAGIC        load_ts,
# MAGIC        substring_index(a.FileName,'/',-1) AS FileName,
# MAGIC        last_updated_by,
# MAGIC        data_dt,
# MAGIC        trigger_key,
# MAGIC        d.hub_id,
# MAGIC       CASE 
# MAGIC 			WHEN (LENGTH(TRIM(a.MSISDN)) BETWEEN 10
# MAGIC 					AND 12
# MAGIC                 AND TRIM(a.MSISDN) RLIKE '^[0-9]*$'
# MAGIC                 AND (TRIM(a.MSISDN) IS NOT NULL) )        
# MAGIC 				THEN 1
# MAGIC 			ELSE 0
# MAGIC 			END AS IsValid_MSISDN
# MAGIC        ,CASE WHEN c.Container_Name IS NULL THEN "group" ELSE c.Container_Name END AS Container_Name
# MAGIC        from temp_BulkSMS_JAM a
# MAGIC LEFT JOIN temp_AreaCode AS b ON SUBSTRING(TRIM(a.MSISDN), 1, 5) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 4) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 3) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 2) = b.MARKET_SH_CODE1
# MAGIC 	OR SUBSTRING(TRIM(a.MSISDN), 1, 1) = b.MARKET_SH_CODE1
# MAGIC LEFT JOIN dgcdimensions_dl.ContainerName c ON b.MARKET_ID = TRIM(c.MARKET_ID)
# MAGIC LEFT JOIN temp_MarketId d ON b.MARKET_ID = TRIM(d.MARKET_ID)
# MAGIC ''')
# MAGIC
# MAGIC #BulkSMS_JAM_allhubs_final = BulkSMS_JAM.withColumn("snapshot_dt",find_date_v2(col("FileName"))).withColumn("Load_TS",lit(windowStartTime)).\
# MAGIC BulkSMS_JAM_allhubs_final=BulkSMS_JAM.withColumn("Last_Updated_By",lit("BAU")).withColumn("Data_dt",lit(dt)).withColumn("trigger_key",lit(trigger_key))
# MAGIC BulkSMS_JAM_allhubs_final.createOrReplaceTempView("temp_BulkSMS_JAM_allhubs_final")

# COMMAND ----------

# MAGIC
# MAGIC
# MAGIC %sql 
# MAGIC select * from temp_BulkSMS_JAM_allhubs_final

# COMMAND ----------

# MAGIC %sql
# MAGIC select *from stgods_dl.BulkSMS_JAM_logs;
# MAGIC
# MAGIC alter table stgods_dl.BulkSMS_JAM_logs add column Flag_Temp string;

# COMMAND ----------

# DBTITLE 1,Temp Table Data Load
# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC
# MAGIC dflogstemp = spark.sql("SELECT DISTINCT Flag_Temp FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
# MAGIC TF         = dflogstemp.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()
# MAGIC if spark._jsparkSession.catalog().tableExists('stgods_dl', table_name) and TF == ['Y']:
# MAGIC   print("stgods_dl."+table_name+" table exists")
# MAGIC else:
# MAGIC   print(path)
# MAGIC   spark.sql("CREATE OR REPLACE TABLE stgods_dl."+table_name+"  USING DELTA LOCATION '"+path+"' PARTITIONED BY (Container_Name) AS SELECT * FROM temp_BulkSMS_JAM_allhubs_final")
# MAGIC   for i in range(1,5,1):
# MAGIC     try:
# MAGIC       spark.sql("DELETE FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
# MAGIC       break;
# MAGIC     except Exception as e2:
# MAGIC       if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e2)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e2)):
# MAGIC         time.sleep(30)
# MAGIC         continue
# MAGIC       else:
# MAGIC         raise Exception(e2)
# MAGIC   ##INSERT INTO LOG TABLE 
# MAGIC   spark.sql("INSERT INTO stgods_dl.stg_bulksms_jam_18_jamicasrc_2023012923000000(MARKET_ID,TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS) VALUES ('"+windowStartTime+"','"+dt+"','"+Source_container+"',int('"+trigger_key+",'N')) ")
# MAGIC   
# MAGIC   ##Update Logging Flag
# MAGIC   for i in range(1,5,1):
# MAGIC     try:
# MAGIC       spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Temp = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
# MAGIC       print("Data inserted into stgods_dl."+table_name+" and Flag_Temp updated to Y in stgods_dl."+log_table+" for  Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
# MAGIC       break;
# MAGIC     except Exception as e3:
# MAGIC       if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e3):
# MAGIC         time.sleep(30)
# MAGIC         continue
# MAGIC       else:
# MAGIC         raise Exception(e3)

# COMMAND ----------

# DBTITLE 1,For Datalake Layer Processing
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogsDL = spark.sql("SELECT DISTINCT Flag_DataLake FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TDL      = dflogsDL.select("Flag_DataLake").distinct().rdd.flatMap(lambda x: x).collect()
if TDL == ['N']:
  MK= BulkSMS_JAM_allhubs_final.select("Container_Name").distinct().rdd.flatMap(lambda x: x).collect()
  print(MK)
  for t in MK :
    
    ##DELETE
    for i in range(1,5,1):
      try:
        #spark.sql("truncate table "+t+"_dl.BulkSMS_JAM")
        spark.sql(" DELETE FROM "+t+"_dl.BulkSMS_JAM where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e4:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e4)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e4)):
          time.sleep(30)
          continue
        else:
          raise Exception(e4)
   
    ##INSERT INTO COUNTRY WISE DELTA TABLE
    spark.sql(" INSERT INTO TABLE "+t+"_dl.BulkSMS_JAM select TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord,int(MARKET_ID),Load_TS,FileName,Last_Updated_By,Data_dt,int(Trigger_Key),int(HUB_ID),IsValid_MSISDN,snapshot_dt FROM stgods_dl."+table_name+" WHERE CORRUPTRECORD is null and IsValid_MSISDN in (1,2) and Container_Name = '"+t+"'")
  
  ##Update logging flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Datalake = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into BulkSMS_JAM Data Lake tables and Flag_Datalake updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+"  and Source_Container = "+Source_container+" and  Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      time.sleep(30)
      continue
else:
  print("Data in BulkSMS_JAM Data Lake tables exists for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Datalake = "+str(TDL))

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC
# MAGIC %sql
# MAGIC select * from jamaica_dl.BulkSMS_JAM

# COMMAND ----------

# DBTITLE 1,Invalid Records Segregation and alertitled
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

Schema_Mismatch_cnt = spark.sql('''select * from stgods_dl.'''+table_name+''' Where IsValid_MSISDN = 0  
                                   AND ((corruptrecord IS NOT NULL AND corruptrecord <> " " AND corruptrecord <> "")
                                   OR  ((corruptrecord IS NULL OR corruptrecord = " " OR corruptrecord = "")
                                   AND ((TRIM(MSISDN) NOT RLIKE '^[0-9]*$' AND TRIM(MSISDN) IS NOT NULL 
                                         AND TRIM(MSISDN) <> "" AND TRIM(MSISDN) <> " " ))))''').count()

Invalid_MSISDNs_cnt = spark.sql("select * from stgods_dl."+table_name+" Where IsValid_MSISDN = 0 AND (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ') AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL OR TRIM(MSISDN) <> ' '  OR TRIM(MSISDN) <> '') ").count()

dflogserror = spark.sql("SELECT DISTINCT Flag_Error FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TE          = dflogserror.select("Flag_Error").distinct().rdd.flatMap(lambda x: x).collect()
                                
if TE == ['N']:
  if Schema_Mismatch_cnt!=0 or Invalid_MSISDNs_cnt !=0:
    ##DELETE
    for i in range(1,5,1):
      try:
        spark.sql(" DELETE FROM error_dl.reject_BulkSMS_JAM where Error_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND Trigger_Key = int('"+trigger_key+"') AND Alert_Type = 'Invalid_Data' ")
        break;
      except Exception as e9:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e9)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e9)):
          time.sleep(30)
          continue
        else:
          raise Exception(e9)
        
    ##INSERT INTO ERROR and Alert control Table  
    spark.sql('''INSERT INTO TABLE error_dl.reject_BulkSMS_JAM SELECT TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord ,MARKET_ID ,Load_TS ,FileName ,Last_Updated_By ,Data_dt as Error_dt,Trigger_Key,IsValid_MSISDN ,HUB_ID, snapshot_dt
                                           ,CASE WHEN IsValid_MSISDN = 0
                                           AND (corruptrecord IS NULL OR corruptrecord = "" OR corruptrecord = " ") 
                                           AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL 
                                                 OR TRIM(MSISDN) <> " "  OR TRIM(MSISDN) <> "")
                                           THEN 'Invalid_MSISDNs' 
                                           ELSE 'Schema_Mismatch' END AS Error_Type FROM stgods_dl.'''+table_name+''' as a where IsValid_MSISDN = 0 ''')
		
		
			
    
    data_error = '{"title": "' +FeedName+ ' Invalid Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Erroneous Records Moved To {dgc{xxx}datalake01}","error":{"Message":"Invalid Records","TableName":"error_dl.reject_BulkSMS_JAM","Path":"/error/BulkSMS_JAM/reject_BulkSMS_JAM","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt)+ '","Total Invalid Records": "' +str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+ '"}}}'
    
    spark.sql('''
     INSERT INTO cfw.etl_alert_control 
     select DISTINCT int("'''+str(trigger_key)+'''") AS trigger_key
                     ,"'''+str(FeedName)+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+str(windowStartTime)+'''") AS Load_TS
                     ,"Invalid_Data" as Alert_Type
                     ,'"'''+str(data_error)+'''"' AS Alert_Reason
                     ,"'''+str(Source_container)+'''" as Data_Storage
                     ,"error_dl.reject_BulkSMS_JAM" as Data_location
                     ,int("'''+str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+'''") as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID  ''')
    
  #Update logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Error  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'")
      print("Data inserted into error_dl.reject_BulkSMS_JAM and Flag_Error updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(180)
        continue
      else:
        raise Exception(e)

else:
  print("Data exists in error_dl.reject_BulkSMS_JAM table for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TE))

# COMMAND ----------

# DBTITLE 1,For ODS Layer Processing
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
  ##DELETE
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM stgods_dl.stgods_BulkSMS_JAM where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"')")
      break;
    except Exception as e6:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e6)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e6)):
        time.sleep(30)
        continue
      else:
        raise Exception(e6)
   
  ##INSERT INTO stgods_dl.stgods_BulkSMS_JAM
  spark.sql("INSERT INTO TABLE stgods_dl.stgods_BulkSMS_JAM select TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,CASE WHEN MSISDN not like '5%' THEN '5'||substr(MSISDN,3,2)||substr(MSISDN,2,10) ELSE MSISDN  END as MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord,MARKET_ID,Load_TS,FileName,Last_Updated_By,Data_dt,Trigger_Key,HUB_ID, IsValid_MSISDN,snapshot_dt FROM temp_BulkSMS_JAM_allhubs_final ")

            
  ##Update Logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_StgODS  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl.stgods_BulkSMS_JAM and Flag_StgODS updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e) 
else:
  print("Data exists in stgods_dl.stgods_BulkSMS_JAM table for Load_TS = "+windowStartTime+" AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'and  Flag_StgODS = "+str(TT))

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC
# MAGIC %sql
# MAGIC select * from stgods_dl.stgods_BulkSMS_JAM 

# COMMAND ----------

# DBTITLE 1,cfw.Table_load_status INSERT
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs = spark.sql("SELECT DISTINCT Flag_Stgods FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"'  AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TT=dflogs.select("Flag_Stgods").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['Y']:
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM cfw.Table_load_status WHERE Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key='"+trigger_key+"'  AND Hub_id='"+hub_id+"' ")
      break;
    except Exception as e8:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
        time.sleep(30)
        continue
      else:
        raise Exception(e8)
## INSERT INTO cfw.Table_load_status 
  spark.sql('''
  INSERT INTO cfw.Table_load_status 
  SELECT DISTINCT CAST(Trigger_Key as int) AS Trigger_Key
                 ,CAST(Load_TS AS TIMESTAMP) AS Load_TS
                 ,snapshot_dt
                 ,CAST(hub_id as int) AS HUB_ID
                 ,"'''+FeedName +'''" AS FeedName
                 ,"BulkSMS" AS Source_System
                 ,"stgods_dl" as target_db
                 ,"stgods_BulkSMS_JAM" as target_table_name
                 ,'Y' as load_status
                 ,count(*) as total_records
                 ,count(distinct FileName) as total_files
                 ,"'''+PipelineRunId+'''" AS Pipeline_Run_Id
                 ,current_timestamp()
                 ,null FROM stgods_dl.'''+table_name+''' group by snapshot_dt,hub_id,Trigger_Key,Load_TS ''')
  print("Data inserted into cfw.Table_load_status")

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC
# MAGIC %sql
# MAGIC select * from cfw.table_load_status where FeedName = 'BulkSMS_JAM'

# COMMAND ----------

# DBTITLE 1,Audit Data Collection
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
FeedName='BulkSMS_JAM'
df_audit = spark.sql(''' SELECT LOAD_TS
		                       ,FileName
                               ,count(*) AS total_RecordCount
		                       ,SUM(CASE 
                                        WHEN FileName IS NOT NULL AND IsValid_MSISDN in (1,2)
					                    THEN 1
				                    ELSE 0
				                    END) AS RecordCount_FN
		                       ,SUM(
                                    CASE
                                    WHEN IsValid_MSISDN  = 0 
                                    and (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ') 
                                   AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL OR TRIM(MSISDN) <> ' ' 
                                    OR TRIM(MSISDN) <> '')
                                    THEN 1 
                                  
                                  ELSE 0
                                  END ) AS cnt_InvalidMSISDNs
                              ,SUM(CASE 
		                      		     WHEN IsValid_MSISDN in (1,2)
					                     AND ((corruptrecord IS NOT NULL AND corruptrecord <> " " AND corruptrecord <> ""))
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_SchemaMismatch
	                     FROM stgods_dl.'''+table_name+'''
	                     GROUP BY FileName,LOAD_TS ''')
df_audit.createOrReplaceTempView("df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,For Rejected Files Audit
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

spark.sql("DROP TABLE IF EXISTS stgods_dl.BulkSMS_JAM_audit_"+trigger_key)
spark.sql("CREATE TABLE stgods_dl.BulkSMS_JAM_audit_"+trigger_key+" AS SELECT DISTINCT Load_TS,FileName AS FileName ,total_RecordCount,RecordCount_FN,cnt_InvalidMSISDNs,cnt_SchemaMismatch FROM df_audit_BulkSMS_JAM ")

# COMMAND ----------

# DBTITLE 1,cfw.DL_File_Details Table :Valid and Non-Empty Files Audit Data 
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

spark.sql("set spark.databricks.delta.checkLatestSchemaOnRead=FALSE")
dflogsaudit = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TA          = dflogsaudit.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if TA == ['N']:
  for i in range(1,5,1):
    try:
      spark.sql('''MERGE INTO cfw.DL_File_Details AS TARGET 
      USING (
      select a.*
            ,CONCAT('{"Invalid MESSAGEIDs":"',cnt_InvalidMSISDNs,'","Schema Mismatch":"',cnt_SchemaMismatch,'"}') as error_reason from df_audit_BulkSMS_JAM a) SOURCE 
      on trim(SOURCE.FileName)=trim(TARGET.FileName) 
      and TARGET.File_load_status != 'D' 
      and TARGET.Load_TS = "'''+ windowStartTime+'''" 
      and TARGET.TRIGGER_KEY="'''+trigger_key+'''" 
      WHEN MATCHED  THEN UPDATE SET 
      TARGET.File_load_status='Y'
     ,TARGET.Total_Records=SOURCE.total_RecordCount
     ,TARGET.valid_records=SOURCE.RecordCount_FN
     ,TARGET.error_records=SOURCE.cnt_InvalidMSISDNs + SOURCE.cnt_SchemaMismatch
     ,TARGET.error_reason= SOURCE.error_reason ''')
      
      #logging
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Audit  = 'Y',Flag_reject = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'  ")
      print("Data inserted into cfw.DL_File_Details and Flag_Audit updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e)  
else:
  print("Data exists in cfw.DL_File_Details table for Load_TS = "+windowStartTime+"  AND Source_Container = "+Source_container+" AND Trigger_Key = '"+trigger_key+"' and  Flag_Error = "+str(TA))

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

db = "stgods_dl"
#FeedName             = "BulkSMS_JAM"
feedaudit             = "BulkSMS_JAM_audit_"+trigger_key
foldername            = "BulkSMS_JAM/"            
tempaudit_rejected    = "BulkSMS_JAM_rejected_"+trigger_key
file_location_reject  = "/mnt/"+Target_container+"/"+FeedName+"/reject/"+Source_container+"/"+dt+"/"+dateId
file_location_working = file_location+filedirectory

#Rejected Files Flow
dflogsreject = spark.sql("SELECT DISTINCT Flag_reject FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TR           = dflogsreject.select("Flag_reject").distinct().rdd.flatMap(lambda x: x).collect()
if TR  == ['N']:
  dbutils.notebook.run("/Shared/DWH_Transformation/DATALAKE/Generic/RejectedFiles", 43200,{"windowStartTime": windowStartTime, "PipelineRunId": PipelineRunId, "DataFactoryName":DataFactoryName,"PipelineName":PipelineName,"trigger_key":trigger_key,"db":db,"feedaudit":feedaudit,"foldername":foldername,"file_location":file_location,"file_location_reject":file_location_reject,"file_location_working":file_location_working,"tempaudit_rejected":tempaudit_rejected,"FeedName":FeedName,"Source_container":Source_container})
  
  ##Update Logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Reject  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      print("Data Moved to rejected folder for rejected files and to cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+"  and Source_Container = "+Source_container+" and  Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e)
else:
  print("Data exists in cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TR))

# COMMAND ----------

# MAGIC %python
# MAGIC sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)
# MAGIC
# MAGIC %sql
# MAGIC select * from cfw.dl_file_details where feedname = 'BulkSMS_JAM'

# COMMAND ----------

# DBTITLE 1,Alert details capture for non true duplicate rows for composite PK
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dfPK_Alert = spark.sql(''' WITH CTE_PK(SELECT TIMESTAMP,MESSAGE_ID,SYSTEM_ID,count(*) FROM stgods_dl.'''+table_name+''' group by TIMESTAMP,MESSAGE_ID,SYSTEM_ID HAVING count(*)>1) SELECT * FROM CTE_PK ''').count()
print(dfPK_Alert)

if dfPK_Alert != 0:
  data_PK = '{"title": "' +FeedName+ ' Duplicate Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Duplicate Records Found After Primary Key Validation","error":{"Message":"Duplicate Records","TableName":"stgods_dl.ods_'+FeedName.lower()+',"Requirement":"One record per {CDRID} column/s must be present in '+FeedName+'","Total Duplicate Records":"' +str(dfPK_Alert)+ '"}}'  
  
  for i in range(1,5,1):
    try:
      spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key = int('"+trigger_key+"') AND Alert_Type = 'Duplicate_Records' ")
      break;
    except Exception as e10:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e10)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e10)):
        time.sleep(30)
        continue
      else:
        raise Exception(e10)
        
  ##INSERT INTO cfw.etl_alert_control 
  spark.sql('''
  INSERT INTO cfw.etl_alert_control 
  select DISTINCT int("'''+trigger_key+'''") AS trigger_key
                     ,"'''+FeedName+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+windowStartTime+'''") AS Load_TS
                     ,"Duplicate_Records" as Alert_Type
                     ,'"'''+data_PK+'''"' AS Alert_Reason
                     ,"'''+Source_container+'''" as Data_Storage
                     ,LOWER(CONCAT("stgods_dl.ods_","'''+FeedName+'''")) as Data_location
                     ,"'''+str(dfPK_Alert)+'''" as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID 
           ''')
  print("Duplicate number of rows :",dfPK_Alert)
  print("Data inserted into cfw.etl_alert_control for stgods_dl.ods_"+FeedName+" for PK Duplicate Records")

# COMMAND ----------

# DBTITLE 1, Notebook status Logging
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_NotebookStatus  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Flag_Temp = 'Y' AND Flag_Datalake = 'Y' AND Flag_StgODS = 'Y' AND Flag_Error = 'Y' AND Flag_Audit = 'Y' AND Flag_reject = 'Y' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      break;
    except Exception as e14:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e14):
        time.sleep(30)
        continue
      else:
        raise Exception(e14)

# COMMAND ----------

# DBTITLE 1, Clean up and Exit
sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs_final = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
NFS =dflogs_final.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
print(NFS)
if NFS == ['Y']:
  spark.sql(" DROP TABLE IF EXISTS stgods_dl."+table_name+" ")
  dbutils.fs.rm(path,recurse=True)
  dbutils.notebook.exit("Successful")
/Shared/DL_BULKSMS_JAM
# MAGIC %md
# MAGIC ###### Purpose: Notebook to process My Digicel App app_client Usage raw data
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |Feb 20, 2021|Divya Verma| Created the notebook to process My Digicel App app_client Usage raw data | 5 min |
# MAGIC |Dec 20, 2021|PARV| Duplicate File check,Audit Tables and Alert Configuration changes | 5 min |
# MAGIC |Feb 28, 2021|Divya Verma| sprint 1 backlog items changes | 5 min |

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.Try
# MAGIC
# MAGIC import com.databricks.dbutils_v1.DBUtilsHolder.dbutils
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraMetadataProvider
# MAGIC import za.co.absa.spline.harvester.extra.UserExtraAppendingPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.harvester.conf.DefaultSplineConfigurer
# MAGIC import za.co.absa.spline.producer.model.v1_1._
# MAGIC import za.co.absa.spline.producer.model._
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import scala.concurrent.duration.Duration
# MAGIC import scala.util.{Failure, Success, Try}
# MAGIC val splineConf: Configuration = StandardSplineConfigurationStack(spark)
# MAGIC spark.enableLineageTracking(new DefaultSplineConfigurer(spark,splineConf) {
# MAGIC //override protected def userExtraMetadataProvider = new UserExtraMetaDataProvider {
# MAGIC //val test = dbutils.notebook.getContext.notebookPath
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC   
# MAGIC
# MAGIC   
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC override protected def maybeUserExtraMetadataProvider: Option[UserExtraMetadataProvider] = Some(new UserExtraMetadataProvider() {
# MAGIC override def forExecEvent(event: ExecutionEvent, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar1")
# MAGIC override def forExecPlan(plan: ExecutionPlan, ctx: HarvestingContext): Map[String, Any] = Map("notebookInfo" -> notebookInfoJson)
# MAGIC override def forOperation(op: ReadOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar3")
# MAGIC override def forOperation(op: WriteOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar4")
# MAGIC override def forOperation(op: DataOperation, ctx: HarvestingContext): Map[String, Any] = Map("foo" -> "bar5")
# MAGIC })
# MAGIC })
# MAGIC
# MAGIC
# MAGIC

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

# DBTITLE 1,Widgets Preparation
dbutils.widgets.removeAll()
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("trigger_key", "")
dbutils.widgets.text("FeedName", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("Source_container", "")
dbutils.widgets.text("Target_container", "")
dbutils.widgets.text("File_delimiter", "")
dbutils.widgets.text("hub_id", "")
PipelineName          = dbutils.widgets.get("PipelineName")
DataFactoryName       = dbutils.widgets.get("DataFactoryName")
trigger_key           = str(dbutils.widgets.get("trigger_key"))
FeedName              = dbutils.widgets.get("FeedName")
windowStartTime       = dbutils.widgets.get("windowStartTime")
PipelineRunId         = dbutils.widgets.get("PipelineRunId")
source_container      = dbutils.widgets.get("Source_container")
target_container      = dbutils.widgets.get("Target_container")
File_delimiter        = dbutils.widgets.get("File_delimiter")
hub_id                = str(dbutils.widgets.get("hub_id"))

# COMMAND ----------

# DBTITLE 1,Optimization Hints
# MAGIC %sql
# MAGIC set spark.sql.shuffle.partitions = 400;
# MAGIC set spark.sql.files.maxPartitionBytes=1073741824;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;

# COMMAND ----------

# DBTITLE 1,Import Required Parameters
import time
import os
import json
import pyspark
import requests
import pyspark.sql.functions as F
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit,col,lower,regexp_replace,split,when,concat
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
spark.conf.set("spark.databricks.io.cache.enabled", "true")
spark.sql("set spark.sql.files.ignoreCorruptFiles=true")
spark.sql("REFRESH TABLE stgods_dl."+FeedName+"_logs")

# COMMAND ----------

# DBTITLE 1,Create Path variable to fetch file from temp adls working
dt = windowStartTime[:10].replace('-','')
dateId = windowStartTime.replace('-','').replace(':','')
file_location = "/mnt/"+target_container+"/"+FeedName+"/working/"+source_container+"/"+dt+"/"+dateId
duplicate_file_location = "/mnt/"+target_container+"/"+FeedName+"/duplicate/"+source_container+"/"+dt+"/"+dateId
table_name ='stg_cn_'+FeedName+trigger_key+'_'+source_container+'_'+dateId 
table_name = table_name.replace('-','')
path = "/mnt/stgods/"+FeedName+"/stg_cn_"+FeedName+dateId+hub_id
log_table = FeedName.lower()+"_logs"
print("log_table:",log_table)
print("path:",path)
print(table_name)
print(file_location)
print(duplicate_file_location)

# COMMAND ----------

# DBTITLE 1,Check Notebook status for current Load timestamp
NotebookStatus = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
NS_status = NotebookStatus.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
if NS_status == ['Y']:
  dbutils.notebook.exit( FeedName+" Notebook Status is already successful for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_NotebookStatus = "+str(NS_status))
else:
  print("Execute Further Notebook Commands")

# COMMAND ----------

# DBTITLE 1,Check Duplicate File
#%run /Shared/DWH_Transformation/TOOLS/CheckDuplicateFile_v2

# COMMAND ----------

# DBTITLE 1,Duplicate File Checking
# filedirectory="/*_PRODUCT_DELIVERY/*/SELFCARECLIENT/lz/*"
filedirectory="/*"
dflogsduplicate = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
DP          = dflogsduplicate.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if DP != ['Y']:
    print('dd')
#   dfdffileinfo=checkDuplicateFile(File_Location=str(file_location),File_Directory=str(filedirectory),FeedName=str(FeedName),Trigger_Key=str(trigger_key),WindowStartTime=str(windowStartTime),PipelineRunID=str(PipelineRunId),FileLocation_dup =str(duplicate_file_location))

# COMMAND ----------

# DBTITLE 1,Schema Enforcement
nofilefound=False
schema = StructType([ \
     StructField("ID",StringType(),True),\
     StructField("NAME",StringType(),True),\
     StructField("corruptrecord",StringType(),True), 
   ])

try:
  app_client_with_schema = spark.read.format("csv")\
  .option("header", "false")\
  .schema(schema)\
  .option("sep", File_delimiter)\
  .option("quote", "\"")\
  .option("multiline","true")\
  .load(file_location+filedirectory)
  app_client_with_schemaAllhubs= app_client_with_schema.withColumn("Load_TS",lit(windowStartTime)).\
  withColumn("FileName",input_file_name()).withColumn("Last_Updated_By",lit("BAU")).\
  withColumn("Data_dt",lit(dt)).withColumn("Trigger_Key",lit(trigger_key)).withColumn("Derived_HUB_ID",lit(hub_id))
  app_client_with_schemaAllhubs.createOrReplaceTempView("temp1_app_client_name")
  Row_count = app_client_with_schemaAllhubs.count()
  print(Row_count)
except Exception as e:
  print(file_location+filedirectory)
  print(e)
  nofilefound=True

# COMMAND ----------

# DBTITLE 1,Insert into Alert_Control for Duplicate Files
if DP != ['Y']:  
  dupfileslist=[]
  vduplicatesfiles=dfdffileinfo.filter(col('Duplicate')==1).select(concat('File_Location',lit('/'),'FileName'))
  for x in vduplicatesfiles.collect():
    dupfileslist.append(x[0])
  dupfilestr='\n'.join(dupfileslist)
  dup_data = '{"title": "' +FeedName+ ' Duplicate Files Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Duplicate Files Moved To {dwhadls{xxx}01}","error":{"Message":"Duplicate Files","TableName":"cfw.DL_File_Details","Path":"' +duplicate_file_location+ '","Duplicate Files":{"Name":"' +dupfilestr+ '"}}}'
  
  if (vduplicatesfiles.count() > 0):
    print('Duplicates Found')
    for i in range(1,5,1):
        try:
          spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key= int('"+trigger_key+"') AND Alert_Type = 'Duplicate_Files' ")
          break;
        except Exception as e1:
          if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e1)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e1)):
            time.sleep(30)
            continue
          else:
            raise Exception(e1)
            
    ##INSERT INTO cfw.etl_alert_control for duplicate Files
    spark.sql('''INSERT INTO cfw.etl_alert_control 
                   SELECT '''+trigger_key+''' AS trigger_key
                         ,'''"'"+FeedName+"'"''' AS FeedName
                         ,TO_TIMESTAMP('''"'"+windowStartTime+"'"''') AS Load_TS
                         ,'Duplicate_Files' AS Alert_Type
                         ,'''"'"+dup_data+"'"''' AS Alert_Reason
                         ,'''"'"+source_container+"'"''' AS Data_Storage
                         ,'cfw.DL_File_Details' AS Data_location
                         ,'''+str(vduplicatesfiles.count())+''' AS Data_Count 
                         ,'N' AS Alert_Sent
                         ,CURRENT_TIMESTAMP() AS Insert_Ts
                         ,NULL AS Update_ts
                         ,'''"'"+PipelineRunId+"'"''' AS Pipeline_Run_ID
              ''')
  
  else:
    print('Duplicates NOT Found in '+FeedName+' for HUB_ID ='+hub_id)

# COMMAND ----------

# DBTITLE 1,If File Not Found
if nofilefound is True:
  dbutils.notebook.exit("No Files Found")

# COMMAND ----------

# DBTITLE 1,Snapshot_Date Helper
# MAGIC %run /Shared/DWH_Transformation/TOOLS/Func_Snapshot_Date

# COMMAND ----------

# DBTITLE 1,Data Preparation: AAP Client
app_clientFinal_CN = spark.sql('''
select 
     CASE WHEN TRIM(ID) IS NULL OR TRIM(ID) = "" OR TRIM(ID) = "null" THEN -1 ELSE TRIM(ID) END AS ID
    ,TRIM(NAME) AS NAME
    ,Load_TS
    ,REPLACE(SUBSTRING(a.FileName, POSITION('lz/' IN a.FileName)),'lz/','') AS FileName
    ,Last_Updated_By
    ,nvl(a.Derived_HUB_ID,99) AS HUB_ID
    ,corruptrecord
    ,Data_dt
    ,Trigger_Key
    ,'group' AS Container_Name
    from temp1_app_client_name as a
''')
app_client_final = app_clientFinal_CN.withColumn("snapshot_dt", find_date(col("FileName")))
app_client_final.createOrReplaceTempView("temp2_app_clientFinal_CN")

# COMMAND ----------

# DBTITLE 1,Temp Table Data Load
dflogstemp = spark.sql("SELECT DISTINCT Flag_Temp FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"'  AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TF         = dflogstemp.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()
if spark._jsparkSession.catalog().tableExists('stgods_dl', table_name) and TF == ['Y']:
  print("stgods_dl."+table_name+" table exists")
else:
  dbutils.fs.rm(path,recurse=True)
  spark.sql("CREATE OR REPLACE TABLE stgods_dl."+table_name+"  USING DELTA LOCATION '"+path+"' PARTITIONED BY (Container_Name) AS SELECT * FROM temp2_app_clientFinal_CN")
  
  ##DELETE
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
        break;
      except Exception as e2:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e2)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e2)):
          time.sleep(30)
          continue
        else:
          raise Exception(e2)
          
  ##INSERT INTO LOG TABLE  
  spark.sql("INSERT INTO stgods_dl."+log_table+" VALUES ('"+windowStartTime+"','"+dt+"','"+source_container+"',int('"+trigger_key+"'),'N','N','N','N','N','N','N') ")
  
  ##Update Logging table
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Temp = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl."+table_name+" and Flag_Temp updated to Y in stgods_dl."+log_table+" for Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e3:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e3):
        time.sleep(30)
        continue
      else:
        raise Exception(e3)

# COMMAND ----------

# DBTITLE 1,Country Wise Incremental Load
dflogsDL = spark.sql("SELECT DISTINCT Flag_DataLake FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TDL      = dflogsDL.select("Flag_DataLake").distinct().rdd.flatMap(lambda x: x).collect()
if TDL == ['N']:
  MK=app_client_final.select("Container_Name").distinct().rdd.flatMap(lambda x: x).collect()
  print(MK)
  for t in MK :    
    ##DELETE
    for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM "+t+"_dl."+FeedName+" where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e4:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e4)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e4)):
          time.sleep(30)
          continue
        else:
          raise Exception(e)
             
    ##INSERT INTO COUNTRY WISE DELTA TABLE     
    spark.sql(" INSERT INTO group_dl.aap_client SELECT ID,NAME,Load_TS,FileName,Last_Updated_By,Data_dt,int(HUB_ID),REPLACE(snapshot_dt,'-','') as DATE_CODE,int(Trigger_Key) FROM stgods_dl."+table_name+" WHERE corruptrecord IS NULL OR corruptrecord == ' '")
    
  #Update logging flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Datalake = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      print("Data inserted into "+FeedName+" Data Lake tables and Flag_Datalake updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e5:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e5):
        time.sleep(180)
        continue
      else:
        raise Exception(e5)
else:
  print("Data in "+FeedName+" Data Lake tables exists for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Datalake = "+str(TDL))

# COMMAND ----------

# DBTITLE 1,For ODS Layer Processing (final ods Layer) 
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
  sdt = spark.sql('''select distinct cast(snapshot_dt as string) from stgods_dl.'''+table_name+''' WHERE corruptrecord IS NULL OR corruptrecord == "" OR corruptrecord ==" " ''').rdd.flatMap(lambda x: x).collect() 
  sdt.sort()
  for x in sdt:
    dbutils.notebook.run("/DWH_Transformation/ODS/MyDigicelApp/ODS_Digicel_App", 43200,{"table_name": table_name,"snapshot_dt":x})
                   
  ##update logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_StgODS  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into ods_dl.digicel_app and Flag_StgODS updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e7:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e7):
        time.sleep(30)
        continue
      else:
        raise Exception(e7) 
else:
  print("Data exists in ods_dl.digicel_app table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_StgODS = "+str(TT))

# COMMAND ----------

# DBTITLE 1,cfw.Table_load_status INSERT
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['Y']:
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM cfw.Table_load_status where FeedName = '"+FeedName+"' AND  Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e8:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
          time.sleep(30)
          continue
        else:
          raise Exception(e8)
          
  ##INSERT INTO cfw.Table_load_status 
  spark.sql('''
  INSERT INTO cfw.Table_load_status 
  SELECT DISTINCT int(Trigger_Key)
                 ,Load_TS
                 ,snapshot_dt
                 ,CAST(HUB_ID as int) AS HUB_ID
                 ,"'''+FeedName +'''" AS FeedName
                 ,'AAP_CLIENT' AS Source_System
                 ,'ods_dl' as target_db
                 ,'Digicel_App' as target_table_name
                 ,'Y' as load_status
                 ,count(*) as total_records
                 ,count(distinct FileName) as total_files
                 ,"'''+str(PipelineRunId)+'''" AS Pipeline_Run_Id
                 ,current_timestamp()
                 ,null FROM stgods_dl.'''+table_name+''' WHERE  corruptrecord IS NULL OR corruptrecord == "" OR corruptrecord ==" "
                 group by snapshot_dt,HUB_ID,Source_System,Trigger_Key,Load_TS ''')
  print("Data inserted into cfw.Table_load_status")

# COMMAND ----------

# DBTITLE 1,Invalid Records Segregation and alert
Invalid_MSISDNs_cnt = 0
Schema_Mismatch_cnt = spark.sql("select * from stgods_dl."+table_name+" Where (corruptrecord IS NOT NULL AND corruptrecord <> '' AND corruptrecord <>' ')").count()
dflogserror = spark.sql("SELECT DISTINCT Flag_Error FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TE          = dflogserror.select("Flag_Error").distinct().rdd.flatMap(lambda x: x).collect()
if TE == ['N']:
  if Schema_Mismatch_cnt != 0:
    for i in range(1,5,1):
      try:
        spark.sql(" DELETE FROM error_dl.reject_"+FeedName+" where Error_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_key=int('"+trigger_key+"') ")
        spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key=int('"+trigger_key+"') AND Alert_Type = 'Invalid_Data' ")
        break;
      except Exception as e9:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e9)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e9)):
          time.sleep(30)
          continue
        else:
          raise Exception(e9)
   
    ##INSERT INTO ERROR and Alert control Table
    spark.sql(" INSERT INTO TABLE error_dl.reject_"+FeedName +" SELECT ID,NAME,Load_TS,FileName,Last_Updated_By,int(HUB_ID),corruptrecord,Data_dt as Error_dt,int(Trigger_Key),snapshot_dt ,'Schema_Mismatch' AS Error_Type FROM stgods_dl."+table_name+" WHERE corruptrecord IS NOT NULL AND corruptrecord != ' ' AND corruptrecord != '' ")
    data_error = '{"title": "' +FeedName+ ' Invalid Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Erroneous Records Moved To {dgc{xxx}datalake01}","error":{"Message":"Invalid Records","TableName":"error_dl.reject_'+FeedName.lower()+'","Path":"/error/mydigicelapp/reject_aap_client/reject_'+FeedName.lower()+'","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+ '","Total Invalid Records": "' +str(Schema_Mismatch_cnt)+ '"}}}'
    
    spark.sql('''
     INSERT INTO cfw.etl_alert_control 
     select DISTINCT int("'''+trigger_key+'''") AS trigger_key
                     ,"'''+FeedName+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+windowStartTime+'''") AS Load_TS
                     ,'Invalid_Data' as Alert_Type
                     ,'"'''+str(data_error)+'''"' AS Alert_Reason
                     ,"'''+source_container+'''" as Data_Storage
                     ,LOWER("error_dl.reject_aap_client") as Data_location
                     ,"'''+str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+'''" as Data_Count
                     ,'N' as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID 
              ''')
  ##Update logging  Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Error  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into error_dl.reject_"+FeedName +" and Flag_Error updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as err:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(err):
        time.sleep(180)
        continue
      else:
        raise Exception(err)

else:
  print("Data exists in error_dl.reject_"+FeedName +" table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TE))

# COMMAND ----------

# DBTITLE 1,Audit Data Collection
df_audit = spark.sql(''' SELECT LOAD_TS
		                       ,FileName
                               ,count(*) AS total_RecordCount
		                       ,SUM(CASE 
                                        WHEN FileName IS NOT NULL AND corruptrecord IS NULL OR corruptrecord == ' '
					                    THEN 1
				                    ELSE 0
				                    END) AS RecordCount_FN
		                       ,0 AS cnt_InvalidMSISDNs
		                       ,SUM(CASE 
		                      		     WHEN corruptrecord IS NOT NULL AND corruptrecord != ' '
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_SchemaMismatch
	                     FROM stgods_dl.'''+table_name+'''
	                     GROUP BY FileName,LOAD_TS ''')
df_audit.createOrReplaceTempView("df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,For Rejected Files Audit
spark.sql("DROP TABLE IF EXISTS stgods_dl."+FeedName+"_audit_"+trigger_key)
spark.sql("CREATE TABLE stgods_dl."+FeedName+"_audit_"+trigger_key+ " AS SELECT DISTINCT Load_TS,FileName,total_RecordCount,RecordCount_FN,cnt_InvalidMSISDns,cnt_SchemaMismatch FROM df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,Audit Data details for cfw.DL_File_Details Table
dflogsaudit = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TA          = dflogsaudit.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if TA == ['N']:
  for i in range(1,5,1):
    try:
      bflag = 0
      spark.sql('''MERGE INTO cfw.DL_File_Details AS TARGET 
      USING (
      select a.*
            ,CONCAT('{"Invalid MSISDNs":"',cnt_InvalidMSISDNs,'","Schema Mismatch":"',cnt_SchemaMismatch,'"}') as error_reason from df_audit_'''+FeedName+''' a) SOURCE 
      on trim(SOURCE.FILENAME)=trim(TARGET.FILENAME) 
      and TARGET.File_load_status != 'D' 
      and TARGET.Load_TS = TO_TIMESTAMP("'''+ windowStartTime+'''") 
      and TARGET.TRIGGER_KEY= int("'''+trigger_key+'''")
      WHEN MATCHED  THEN UPDATE SET 
      TARGET.File_load_status='Y'
     ,TARGET.Total_Records=SOURCE.total_RecordCount
     ,TARGET.valid_records=SOURCE.RecordCount_FN
     ,TARGET.error_records=SOURCE.cnt_InvalidMSISDNs + SOURCE.cnt_SchemaMismatch
     ,TARGET.error_reason= SOURCE.error_reason ''')
                        
      bflag = 1
      
      #logging
      if bflag == 1:
        spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Audit  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
        print("Data inserted into cfw.DL_File_Details and Flag_Audit updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" ")     
      break;
    except Exception as e11:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e11):
        time.sleep(180)
        continue
      else:
        raise Exception(e11)
  
  ##IF Merge could not happen successfully
  if bflag == 0:
      raise Exception("Retry!")
else:
  print("Data exists in cfw.DL_File_Details table for "+FeedName +" feed for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TA))

# COMMAND ----------

# DBTITLE 1,Rejected and Empty Files Logging
db = "stgods_dl"
FeedName              = FeedName
feedaudit             = FeedName+"_audit_"+trigger_key
foldername            = "GR_PRODUCT_DELIVERY/GLOBAL/SELFCARECLIENT/lz/"     
tempaudit_rejected    = FeedName+"_rejected_"+trigger_key
file_location_reject  = "/mnt/"+target_container+"/"+FeedName+"/reject/"+source_container+"/"+dt+"/"+dateId
file_location_working = file_location+filedirectory

#Rejected Files Flow
dflogsreject = spark.sql("SELECT DISTINCT Flag_reject FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TR          = dflogsreject.select("Flag_reject").distinct().rdd.flatMap(lambda x: x).collect()
if TR  == ['N']:
  dbutils.notebook.run("/Shared/DWH_Transformation/DATALAKE/Generic/RejectedFiles", 43200,{"windowStartTime": windowStartTime, "PipelineRunId": PipelineRunId, "DataFactoryName":DataFactoryName,"PipelineName":PipelineName,"trigger_key":trigger_key,"db":db,"feedaudit":feedaudit,"foldername":foldername,"file_location":file_location,"file_location_reject":file_location_reject,"file_location_working":file_location_working,"tempaudit_rejected":tempaudit_rejected,"FeedName":FeedName,"source_container":source_container})
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Reject  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      print("Data Moved to rejected folder for rejected files and to cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND source_container = "+source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e13:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e13):
        time.sleep(180)
        continue
      else:
        raise Exception(e13)
else:
  print("Data exists in cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND source_container = "+source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TR))

# COMMAND ----------

# DBTITLE 1, Notebook status Logging
for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_NotebookStatus  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Flag_Temp = 'Y' AND Flag_Datalake = 'Y' AND Flag_StgODS = 'Y' AND Flag_Error = 'Y' AND Flag_Audit = 'Y' AND Flag_reject = 'Y' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      break;
    except Exception as e14:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e14):
        time.sleep(30)
        continue
      else:
        raise Exception(e14)     

# COMMAND ----------

# DBTITLE 1,Clean up and Exit
dflogs_final = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
NFS =dflogs_final.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
print(NFS)
if NFS == ['Y']:
  spark.sql(" DROP TABLE IF EXISTS stgods_dl."+table_name+" ")
  dbutils.fs.rm(path,recurse=True)
  dbutils.notebook.exit("Successful")
/Shared/AAP_CLIENT
# MAGIC %md
# MAGIC ###### Purpose: Notebook to process SANDVINE BILLING RECON raw data
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |Mar 05, 2021|Shagufa Akhzar| Created the notebook to process Sandvine Billing raw data | 30 min |
# MAGIC |Mar 17, 2021|Shagufa Akhzar| Added alert code in case of invalid records | 30 min |
# MAGIC |Apr 15, 2021|Shagufa Akhzar| Added Area_Code_ID column and included TDD-LTE LOB data | 30 min |
# MAGIC |May 03, 2021|Shagufa Akhzar| Added ODS_Sandvine_Billing_Hourly insertion code | 30 min |

# COMMAND ----------

# DBTITLE 1,Create widgets to extract parameters from ADF
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("LogicAppURL", "")
DataFactoryName = dbutils.widgets.get("DataFactoryName")
PipelineName = dbutils.widgets.get("PipelineName")
PipelineRunId = dbutils.widgets.get("PipelineRunId")
windowStartTime = dbutils.widgets.get("windowStartTime")
LogicAppURL = dbutils.widgets.get("LogicAppURL")

# COMMAND ----------

# DBTITLE 1,Refresh tables
# MAGIC %sql
# MAGIC REFRESH TABLE dgcdimensions_dl.ContainerName;
# MAGIC REFRESH TABLE dgcdimensions_dl.AreaCode;
# MAGIC REFRESH TABLE stgods_dl.sandvinebilling_logs

# COMMAND ----------

# DBTITLE 1,Import required packages
import pyspark
import time
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from  pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit

# COMMAND ----------

# DBTITLE 1,Create dynamic variables 
date = windowStartTime[:10].replace('-','')
date_ts = windowStartTime.replace('-','').replace(':','')
table_name = 'SandvineBilling_'+date_ts+'_tmp'
filelocation_lz = "/mnt/continuousfeeds/Sandvine_Billing_Recon/lz/"+date+"/"+date_ts+"/*/*/*/BILLING_RECON/lz/*"
filelocation_wrk = "/mnt/continuousfeeds/Sandvine_Billing_Recon/working/"+date+"/"+date_ts+"/*/*/BILLING_RECON/lz/*"

# COMMAND ----------

# DBTITLE 1,Schema Enforcement
schema = StructType([ \
     StructField("SUBSCRIBER_NUMBER",StringType(),True), \
     StructField("PLAN_NAME",StringType(),True), \
     StructField("PLAN_TYPE",StringType(),True), \
     StructField("CHARGING_CHARACTERISTICS",StringType(),True), \
     StructField("APN",StringType(),True), \
     StructField("SESSION_ID",StringType(),True), \
     StructField("SEQUENCE_NUMBER",StringType(),True), \
     StructField("START_TIME",StringType(),True), \
     StructField("END_TIME",StringType(),True), \
     StructField("PDP_TYPE",StringType(),True), \
     StructField("FRAME_DLP",StringType(),True), \
     StructField("IMSI",StringType(),True), \
     StructField("IMEISV",StringType(),True), \
     StructField("PROTOCOL",StringType(),True), \
     StructField("SGSN_MCC_MNC",StringType(),True), \
     StructField("SGSN_ADDRESS",StringType(),True), \
     StructField("GGSN_ADDRESS",StringType(),True), \
     StructField("USER_LOCATION",StringType(),True), \
     StructField("RAT_TYPE",StringType(),True), \
     StructField("QOS",StringType(),True), \
     StructField("ORIGIN_HOST",StringType(),True), \
     StructField("RATING_GROUP",StringType(),True), \
     StructField("TX_BYTES",StringType(),True), \
     StructField("RX_BYTES",StringType(),True), \
     StructField("TOTAL_BYTES",StringType(),True), \
     StructField("SERVICE_ID",StringType(),True), \
     StructField("CHARGING_ID",StringType(),True), \
     StructField("corruptrecord",StringType(),True)\
   ])                 
sbilling_withschema = spark.read.format("csv")\
  .option("header", "false")\
  .schema(schema)\
  .option("sep", "|")\
  .load(filelocation_wrk)\

svb_withheaderfooter = sbilling_withschema.withColumn("LOAD_TS",lit(windowStartTime)).withColumn("FileName", input_file_name()).withColumn("Last_Updated_by",lit("BAU")).withColumn("Data_dt",lit(date)).withColumn("Error_dt",lit(date))

# COMMAND ----------

# DBTITLE 1,Create temp view
svb_withheaderfooter.createOrReplaceTempView("temp_sandvinebilling")

# COMMAND ----------

# DBTITLE 1,Use database SandvineBilling_dl
# MAGIC %sql
# MAGIC USE SandvineBilling_dl;

# COMMAND ----------

# DBTITLE 1,Remove managed table
dbutils.fs.rm("dbfs:/user/hive/warehouse/sandvinebilling_dl/"+table_name,True)

# COMMAND ----------

# DBTITLE 1,Create main SandvineBilling temp table 
if spark._jsparkSession.catalog().tableExists('sandvinebilling_dl', table_name):
  print("Table already exists")
else:
  spark.sql('''CREATE TABLE '''+table_name+''' USING DELTA PARTITIONED BY(Container_Name) WITH CTE_SVB_SUBS
AS (
	SELECT CAST(a.SUBSCRIBER_NUMBER AS DECIMAL(16, 0)) AS SUBSCRIBER_NUMBER
		,a.PLAN_NAME
		,a.PLAN_TYPE
		,a.CHARGING_CHARACTERISTICS
		,a.APN
		,a.SESSION_ID
		,a.SEQUENCE_NUMBER
		,a.START_TIME
		,a.END_TIME
		,a.PDP_TYPE
		,a.FRAME_DLP
        ,a.IMSI
        ,a.IMEISV
		,a.PROTOCOL
		,a.SGSN_MCC_MNC
		,a.SGSN_ADDRESS
		,a.GGSN_ADDRESS
		,a.USER_LOCATION
		,a.RAT_TYPE
		,a.QOS
		,a.ORIGIN_HOST
		,a.RATING_GROUP
        ,a.TX_BYTES
        ,a.RX_BYTES
        ,a.TOTAL_BYTES
        ,a.SERVICE_ID  
        ,a.CHARGING_ID 
		,a.corruptrecord
		,a.LOAD_TS
		,CONCAT(REPLACE(SUBSTRING_INDEX(REPLACE(SUBSTRING(a.FileName, POSITION('/BILLING_RECON/lz/' IN a.FileName)), '/BILLING_RECON/lz/', ''), '/', 1), '%20', ' '),'.tgz') AS FileName
		,a.Last_Updated_by
		,a.Data_dt
		,a.Error_dt
		,CASE 
			WHEN length(CAST(a.SUBSCRIBER_NUMBER AS DECIMAL(16, 0))) BETWEEN 10
					AND 12
				AND CAST(a.SUBSCRIBER_NUMBER AS DECIMAL(16, 0)) IS NOT NULL
				AND a.corruptrecord IS NULL
				THEN 1
			ELSE 0
			END AS IsValid
	FROM temp_sandvinebilling a
	)
	,cte_dis_subs
AS (
	SELECT DISTINCT SUBSCRIBER_NUMBER
	FROM CTE_SVB_SUBS
	WHERE SUBSCRIBER_NUMBER IS NOT NULL
	)
	,cte_dis_marketcode
AS (
	SELECT DISTINCT TRIM(MARKET_ID) AS MARKET_ID,TRIM(MARKET_SH_CODE1) AS MARKET_SH_CODE1,TRIM(AREA_CODE_ID) AS AREA_CODE_ID
	FROM dgcdimensions_dl.AreaCode
	WHERE LOWER(TRIM(LINE_OF_BUSINESS)) IN ('gsm','tdd-lte')
	)
    SELECT a.SUBSCRIBER_NUMBER
	,a.PLAN_NAME
	,a.PLAN_TYPE
	,a.CHARGING_CHARACTERISTICS
	,a.APN
	,a.SESSION_ID
	,a.SEQUENCE_NUMBER
	,a.START_TIME
	,a.END_TIME
	,a.PDP_TYPE
	,a.FRAME_DLP
    ,CASE 
          WHEN a.IMSI IS NULL 
             OR a.IMSI = "" 
             OR a.IMSI = "null" 
             THEN '000000000000000' 
          ELSE a.IMSI 
          END AS IMSI
     ,CASE 
          WHEN a.IMEISV IS NULL 
             OR a.IMEISV = "" 
             OR a.IMEISV = "null" 
             THEN '0000000000000000' 
          ELSE a.IMEISV 
          END AS IMEISV
	,a.PROTOCOL
	,a.SGSN_MCC_MNC
	,a.SGSN_ADDRESS
	,a.GGSN_ADDRESS
	,a.USER_LOCATION
	,a.RAT_TYPE
	,a.QOS
	,a.ORIGIN_HOST
	,a.RATING_GROUP
    ,CASE 
          WHEN a.TX_BYTES IS NULL 
             OR a.TX_BYTES = "" 
             OR a.TX_BYTES = "null" 
             THEN '0' 
          ELSE a.TX_BYTES 
          END AS TX_BYTES
    ,CASE 
          WHEN a.RX_BYTES IS NULL 
             OR a.RX_BYTES = "" 
             OR a.RX_BYTES = "null" 
             THEN '0' 
          ELSE a.RX_BYTES 
          END AS RX_BYTES
    ,CASE 
          WHEN a.TOTAL_BYTES IS NULL 
             OR a.TOTAL_BYTES = "" 
             OR a.TOTAL_BYTES = "null" 
             THEN '0' 
          ELSE a.TOTAL_BYTES 
          END AS TOTAL_BYTES
    ,CASE 
          WHEN a.SERVICE_ID IS NULL 
             OR a.SERVICE_ID = "" 
             OR a.SERVICE_ID = "null" 
             THEN '-1' 
          ELSE a.SERVICE_ID 
          END AS SERVICE_ID  
    ,CASE 
          WHEN a.CHARGING_ID IS NULL 
             OR a.CHARGING_ID = "" 
             OR a.CHARGING_ID = "null" 
             THEN '-1' 
          ELSE a.CHARGING_ID 
          END AS CHARGING_ID 
	,a.corruptrecord
	,a.LOAD_TS
	,a.FileName
	,a.Last_Updated_by
	,a.Data_dt
	,a.Error_dt
	,a.IsValid
	,IFNULL(b.MARKET_ID, - 1) AS MARKET_ID
	,IFNULL(TRIM(c.Container_Name), 'group') AS Container_Name
    ,IFNULL(b.AREA_CODE_ID, 0) AS AREA_CODE_ID
FROM CTE_SVB_SUBS a
INNER JOIN cte_dis_subs cd ON a.SUBSCRIBER_NUMBER = cd.SUBSCRIBER_NUMBER 
LEFT JOIN cte_dis_marketcode AS b ON substring(cd.SUBSCRIBER_NUMBER, 1, 5) = b.MARKET_SH_CODE1 
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 4) = b.MARKET_SH_CODE1
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 3) = b.MARKET_SH_CODE1
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 2) = b.MARKET_SH_CODE1
	OR substring(cd.SUBSCRIBER_NUMBER, 1, 1) = b.MARKET_SH_CODE1
LEFT JOIN dgcdimensions_dl.ContainerName c ON b.MARKET_ID = TRIM(c.MARKET_ID)''')
  spark.sql("INSERT INTO stgods_dl.sandvinebilling_logs VALUES ('"+windowStartTime+"','"+date+"','N') ")

# COMMAND ----------

# DBTITLE 1,Delete and insert invalid records to error dir of dgcdatalake01 and send an alert
import json
import requests
Invalid_MSISDNs_cnt = spark.sql("SELECT COUNT(1) as cnt_InvalidMSISDNs FROM "+table_name+" WHERE IsValid = 0 AND corruptrecord IS NULL").first()["cnt_InvalidMSISDNs"]
Schema_Mismatch_cnt = spark.sql("SELECT COUNT(1) as cnt_SchemaMismatch FROM "+table_name+" WHERE IsValid = 0 AND corruptrecord IS NOT NULL").first()["cnt_SchemaMismatch"] 
if(Invalid_MSISDNs_cnt != 0 or Schema_Mismatch_cnt != 0):
  spark.sql("DELETE FROM error_dl.reject_SandvineBilling WHERE Error_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql('''
WITH CTE_NullRecords AS (
		SELECT *
		FROM temp_sandvinebilling
		WHERE SUBSCRIBER_NUMBER IS NULL
		)
INSERT INTO TABLE error_dl.reject_SandvineBilling
SELECT SUBSCRIBER_NUMBER
	,PLAN_NAME
	,PLAN_TYPE
	,CHARGING_CHARACTERISTICS
	,APN
	,SESSION_ID
	,SEQUENCE_NUMBER
	,START_TIME
	,END_TIME
	,PDP_TYPE
	,FRAME_DLP
	,IMSI
	,IMEISV
	,PROTOCOL
	,SGSN_MCC_MNC
	,SGSN_ADDRESS
	,GGSN_ADDRESS
	,USER_LOCATION
	,RAT_TYPE
	,QOS
	,ORIGIN_HOST
	,RATING_GROUP
	,TX_BYTES
	,RX_BYTES
	,TOTAL_BYTES
	,SERVICE_ID
	,CHARGING_ID
	,corruptrecord
	,LOAD_TS
	,FileName
	,Last_Updated_by
	,Error_dt
	,CASE 
		WHEN corruptrecord IS NULL
			THEN 'Invalid_MSISDNs'
		ELSE 'Schema_Mismatch'
		END AS Error_Type
FROM '''+table_name+'''
WHERE IsValid = 0
UNION
SELECT CAST(SUBSCRIBER_NUMBER AS DECIMAL(16, 0)) AS SUBSCRIBER_NUMBER
	,PLAN_NAME
	,PLAN_TYPE
	,CHARGING_CHARACTERISTICS
	,APN
	,SESSION_ID
	,SEQUENCE_NUMBER
	,START_TIME
	,END_TIME
	,PDP_TYPE
	,FRAME_DLP
	,CASE 
		WHEN IMSI IS NULL
			OR IMSI = ""
			OR IMSI = "null"
			THEN '000000000000000'
		ELSE IMSI
		END AS IMSI
	,CASE 
		WHEN IMEISV IS NULL
			OR IMEISV = ""
			OR IMEISV = "null"
			THEN '0000000000000000'
		ELSE IMEISV
		END AS IMEISV
	,PROTOCOL
	,SGSN_MCC_MNC
	,SGSN_ADDRESS
	,GGSN_ADDRESS
	,USER_LOCATION
	,RAT_TYPE
	,QOS
	,ORIGIN_HOST
	,RATING_GROUP
	,CASE 
		WHEN TX_BYTES IS NULL
			OR TX_BYTES = ""
			OR TX_BYTES = "null"
			THEN '0'
		ELSE TX_BYTES
		END AS TX_BYTES
	,CASE 
		WHEN RX_BYTES IS NULL
			OR RX_BYTES = ""
			OR RX_BYTES = "null"
			THEN '0'
		ELSE RX_BYTES
		END AS RX_BYTES
	,CASE 
		WHEN TOTAL_BYTES IS NULL
			OR TOTAL_BYTES = ""
			OR TOTAL_BYTES = "null"
			THEN '0'
		ELSE TOTAL_BYTES
		END AS TOTAL_BYTES
	,CASE 
		WHEN SERVICE_ID IS NULL
			OR SERVICE_ID = ""
			OR SERVICE_ID = "null"
			THEN '-1'
		ELSE SERVICE_ID
		END AS SERVICE_ID
	,CASE 
		WHEN CHARGING_ID IS NULL
			OR CHARGING_ID = ""
			OR CHARGING_ID = "null"
			THEN '-1'
		ELSE CHARGING_ID
		END AS CHARGING_ID
	,corruptrecord
	,LOAD_TS
	,CONCAT (
		REPLACE(SUBSTRING_INDEX(REPLACE(SUBSTRING(FileName, POSITION('/BILLING_RECON/lz/' IN FileName)), '/BILLING_RECON/lz/', ''), '/', 1), '%20', ' ')
		,'.tgz'
		) AS FileName
	,Last_Updated_by
	,Error_dt
	,'Invalid_MSISDNs' AS Error_Type
FROM CTE_NullRecords
''')
  data = '{ "title":"Error Directory Movement !!!","color":"red","DataFactoryName":"' +str(DataFactoryName)+ '","PipelineName":"' +str(PipelineName)+ '","PipelineRunID":"' +str(PipelineRunId)+ '","time":"' +str(windowStartTime)+ '","FileName":"Sandvine Billing Recon Erroneous Records At \'dgc{xxx}datalake01\'","error":{"Message":"Invalid Records","TableName":"error_dl.reject_SandvineBilling","Path":"/error/sandvine/reject_SandvineBilling","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt)+ '","Total Records": "' +str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+ '"}}}'
  response = requests.post(LogicAppURL, data=data,headers={"Content-Type": "application/json"})

# COMMAND ----------

# DBTITLE 1,Delete and insert valid data to dgcdatalake01 container wise
dfContName = spark.sql("SELECT DISTINCT Container_Name FROM "+table_name).rdd.flatMap(lambda x: x).collect()
for c in dfContName :
  spark.sql("DELETE FROM "+c+"_dl.SandvineBilling WHERE Data_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql("INSERT INTO TABLE "+c+"_dl.SandvineBilling SELECT SUBSCRIBER_NUMBER,PLAN_NAME,PLAN_TYPE,CHARGING_CHARACTERISTICS,APN,SESSION_ID,SEQUENCE_NUMBER,START_TIME,END_TIME,PDP_TYPE,FRAME_DLP,IMSI,IMEISV,PROTOCOL,SGSN_MCC_MNC,SGSN_ADDRESS,GGSN_ADDRESS,USER_LOCATION,RAT_TYPE,QOS,ORIGIN_HOST,RATING_GROUP,TX_BYTES,RX_BYTES,TOTAL_BYTES,SERVICE_ID,CHARGING_ID,LOAD_TS,FileName,Last_Updated_by,Data_dt,MARKET_ID FROM "+table_name+" WHERE IsValid=1 AND Container_Name = '"+c+"'")

# COMMAND ----------

# DBTITLE 1,Merge processed files into staging Audit Table
for i in range(1,5,1): 
  try:
    spark.sql('''
WITH CTE_StgAudit
AS (
	SELECT LOAD_TS
		,FileName
		,SUM(CASE 
				WHEN FileName IS NOT NULL
					AND IsValid = 1
					THEN 1
				ELSE 0
				END) AS RecordCount_FN
		,SUM(CASE 
				WHEN IsValid = 0
					AND corruptrecord IS NULL
					THEN 1
				ELSE 0
				END) AS cnt_InvalidMSISDNs
		,SUM(CASE 
				WHEN IsValid = 0
					AND corruptrecord IS NOT NULL
					THEN 1
				ELSE 0
				END) AS cnt_SchemaMismatch
	FROM '''+table_name+'''
	GROUP BY LOAD_TS
		,FileName
	)
MERGE INTO stgods_dl.stg_auditcontrol AS target
USING (
	SELECT DISTINCT LOAD_TS
		,FileName
		,RecordCount_FN
		,CONCAT (
			'{"Invalid MSISDNs":"'
			,cnt_InvalidMSISDNs
			,'","Schema Mismatch":"'
			,cnt_SchemaMismatch
			,'"}'
			) AS Error
        ,'Sandvine Billing' AS FeedName
	FROM CTE_StgAudit
	) AS source
	ON source.LOAD_TS = target.Load_TS
		AND source.FileName = target.FileName
        AND source.FeedName = target.FeedName 
WHEN MATCHED
	THEN
		UPDATE
		SET *
WHEN NOT MATCHED
	THEN
		INSERT * ''')
    break;
  except Exception as e:
    if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
      time.sleep(180)
      continue

# COMMAND ----------

# DBTITLE 1,List all the files from lz dir in a temp view
import glob
from pyspark.sql import Row
path=glob.glob("/dbfs"+filelocation_lz)
rdd1 = sc.parallelize(path)
row_rdd1 = rdd1.map(lambda x: Row(x))
df_files=sqlContext.createDataFrame(row_rdd1,['FileName'])
df_files.createOrReplaceTempView("SandvineBilling_AllFiles")

# COMMAND ----------

# DBTITLE 1,Merge empty files into staging Audit Table
for i in range(1,5,1): 
  try:
    spark.sql('''WITH CTE_Files
AS (
	SELECT REPLACE(SUBSTRING(FileName, POSITION('/BILLING_RECON/lz/' IN FileName)), '/BILLING_RECON/lz/', '') AS FileName
	FROM SandvineBilling_AllFiles
	EXCEPT
    SELECT Filename
	FROM '''+table_name+'''
	)
MERGE INTO stgods_dl.stg_auditcontrol AS target
USING (
	SELECT DISTINCT '''"'"+windowStartTime+"'"''' AS LOAD_TS
		,FileName
		,0 AS RecordCount_FN
		,CONCAT (
			'{"Invalid MSISDNs":"'
			,0
			,'","Schema Mismatch":"'
			,0
			,'"}'
			) AS Error
        ,'Sandvine Billing' AS FeedName
	FROM CTE_Files
	) AS source
	ON source.LOAD_TS = target.Load_TS
		AND source.FileName = target.FileName
        AND source.FeedName = target.FeedName
WHEN MATCHED
	THEN
		UPDATE
		SET *
WHEN NOT MATCHED
	THEN
		INSERT *''')
    break;
  except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(180)
        continue

# COMMAND ----------

# DBTITLE 1,Staging ODS layer processing
dflogs = spark.sql("SELECT DISTINCT Flag FROM stgods_dl.sandvinebilling_logs WHERE Data_dt = '"+date+"' AND Load_TS = '"+windowStartTime+"' ")
TT=dflogs.select("Flag").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
#   Delete and insert valid data to staging ods
  spark.sql("DELETE FROM stgods_dl.ODS_Sandvine_Billing WHERE Data_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql("INSERT INTO TABLE stgods_dl.ODS_Sandvine_Billing SELECT SUBSCRIBER_NUMBER,PLAN_NAME,PLAN_TYPE,CHARGING_CHARACTERISTICS,APN,SESSION_ID,SEQUENCE_NUMBER,START_TIME,END_TIME,PDP_TYPE,FRAME_DLP,IMSI,IMEISV,PROTOCOL,SGSN_MCC_MNC,SGSN_ADDRESS,GGSN_ADDRESS,USER_LOCATION,RAT_TYPE,QOS,ORIGIN_HOST,RATING_GROUP,TX_BYTES,RX_BYTES,TOTAL_BYTES,SERVICE_ID,CHARGING_ID,LOAD_TS,FileName,Last_Updated_by,Data_dt,MARKET_ID,AREA_CODE_ID FROM "+table_name+" WHERE IsValid=1")
#   Delete and insert valid data to staging ods(hourly)
  spark.sql("DELETE FROM stgods_dl.ODS_Sandvine_Billing_Hourly WHERE Data_dt = '"+date+"' AND LOAD_TS = '"+windowStartTime+"'")
  spark.sql("INSERT INTO TABLE stgods_dl.ODS_Sandvine_Billing_Hourly SELECT SUBSCRIBER_NUMBER,PLAN_NAME,PLAN_TYPE,CHARGING_CHARACTERISTICS,APN,SESSION_ID,SEQUENCE_NUMBER,START_TIME,END_TIME,PDP_TYPE,FRAME_DLP,IMSI,IMEISV,PROTOCOL,SGSN_MCC_MNC,SGSN_ADDRESS,GGSN_ADDRESS,USER_LOCATION,RAT_TYPE,QOS,ORIGIN_HOST,RATING_GROUP,TX_BYTES,RX_BYTES,TOTAL_BYTES,SERVICE_ID,CHARGING_ID,LOAD_TS,FileName,Last_Updated_by,Data_dt,MARKET_ID,AREA_CODE_ID FROM "+table_name+" WHERE IsValid=1")
#   Logging
  spark.sql("UPDATE stgods_dl.sandvinebilling_logs SET Flag = 'Y' WHERE Data_dt = '"+date+"' AND Load_TS = '"+windowStartTime+"' ")
else:
  print("Data already exists for this trigger time")

# COMMAND ----------

# DBTITLE 1,Drop table if exists
spark.sql("DROP TABLE IF EXISTS "+table_name)

# COMMAND ----------

# DBTITLE 1,Exit the notebook 
dbutils.notebook.exit("Exited")
/Shared/Sandvine_BillingRecon
# MAGIC %md
# MAGIC ###### Purpose: Notebook to process My Digicel App Selfcare Usage raw data
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |Feb 20, 2021|Divya Verma| Created the notebook to process My Digicel App Selfcare Usage raw data | 15 min |
# MAGIC |Dec 20, 2021|Divya Verma| Duplicate File check,Audit Tables and Alert Configuration changes | 15 min |

# COMMAND ----------

# DBTITLE 1,Widgets Preparation
dbutils.widgets.removeAll()
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("trigger_key", "")
dbutils.widgets.text("FeedName", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("Source_container", "")
dbutils.widgets.text("Target_container", "")
dbutils.widgets.text("File_delimiter", "")
dbutils.widgets.text("hub_id", "")
PipelineName          = dbutils.widgets.get("PipelineName")
DataFactoryName       = dbutils.widgets.get("DataFactoryName")
trigger_key           = str(dbutils.widgets.get("trigger_key"))
FeedName              = dbutils.widgets.get("FeedName")
windowStartTime       = dbutils.widgets.get("windowStartTime")
PipelineRunId         = dbutils.widgets.get("PipelineRunId")
source_container      = dbutils.widgets.get("Source_container")
target_container      = dbutils.widgets.get("Target_container")
File_delimiter        = dbutils.widgets.get("File_delimiter")
hub_id                = str(dbutils.widgets.get("hub_id"))

# COMMAND ----------

# DBTITLE 1,Optimization Hints
# MAGIC %sql
# MAGIC set spark.sql.shuffle.partitions = 400;
# MAGIC set spark.sql.files.maxPartitionBytes=1073741824;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;

# COMMAND ----------

# DBTITLE 1,Import Required Parameters
import time
import os
import json
import pyspark
import requests
import pyspark.sql.functions as F
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit,col,lower,regexp_replace,split,when,concat
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
spark.conf.set("spark.databricks.io.cache.enabled", "true")
spark.sql("set spark.sql.files.ignoreCorruptFiles=true")
spark.sql("REFRESH TABLE stgods_dl."+FeedName+"_logs")

# COMMAND ----------

# DBTITLE 1,Create Path variable to fetch file from temp adls working
dt = windowStartTime[:10].replace('-','')
dateId = windowStartTime.replace('-','').replace(':','')
file_location = "/mnt/"+target_container+"/"+FeedName+"/working/"+source_container+"/"+dt+"/"+dateId
duplicate_file_location = "/mnt/"+target_container+"/"+FeedName+"/duplicate/"+source_container+"/"+dt+"/"+dateId
table_name ='stg_cn_'+FeedName+trigger_key+'_'+source_container+'_'+dateId 
table_name = table_name.replace('-','')
path = "/mnt/stgods/"+FeedName+"/stg_cn_"+FeedName+dateId+hub_id
log_table = FeedName.lower()+"_logs"
print("log_table:",log_table)
print("path:",path)
print(table_name)
print(file_location)
print(duplicate_file_location)

# COMMAND ----------

# DBTITLE 1,Check Notebook status for current Load timestamp
NotebookStatus = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
NS_status = NotebookStatus.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
if NS_status == ['Y']:
  dbutils.notebook.exit( FeedName+" Notebook Status is already successful for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_NotebookStatus = "+str(NS_status))
else:
  print("Execute Further Notebook Commands")

# COMMAND ----------

# DBTITLE 1,Dimensions
# MAGIC %sql
# MAGIC REFRESH TABLE dgcdimensions_dl.countrycode;
# MAGIC REFRESH TABLE dgcdimensions_dl.containername;
# MAGIC REFRESH TABLE dgcdimensions_dl.areacode;

# COMMAND ----------

# DBTITLE 1,For Market_ID Derivation
dfcode=spark.sql("SELECT distinct MARKET_ID, CAST(MARKET_SH_CODE1 AS STRING) AS MARKET_SH_CODE1 FROM dgcdimensions_dl.areacode  where LINE_OF_BUSINESS = 'GSM' ")
dfcode.createOrReplaceTempView("temp_MarketCode")
dfHUBID=spark.sql("SELECT DISTINCT Market_Key AS MARKET_ID,HUB_ID FROM ods_dl.market")
dfHUBID.createOrReplaceTempView("temp1_HUBID")

# COMMAND ----------

# DBTITLE 1,Check Duplicate File
# MAGIC %run /Shared/DWH_Transformation/TOOLS/CheckDuplicateFile_v2

# COMMAND ----------

# DBTITLE 1,Duplicate File Checking
filedirectory="/*_PRODUCT_DELIVERY/*/SELFCAREUSAGE/lz/*"
dflogsduplicate = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
DP          = dflogsduplicate.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if DP != ['Y']:
  dfdffileinfo=checkDuplicateFile(File_Location=str(file_location),File_Directory=str(filedirectory),FeedName=str(FeedName),Trigger_Key=str(trigger_key),WindowStartTime=str(windowStartTime),PipelineRunID=str(PipelineRunId),FileLocation_dup =str(duplicate_file_location))

# COMMAND ----------

# DBTITLE 1,Schema Enforcement
nofilefound=False
schema = StructType([ \
     StructField("DATETIMESTAMP",StringType(),True), \
     StructField("USER_AGENT",StringType(),True), \
     StructField("IP_REQUESTER",StringType(),True), \
     StructField("SELFCARE_METHOD",StringType(),True), \
     StructField("SELFCARE_METHOD_ALIAS",StringType(),True), \
     StructField("EXECUTION_TIME",StringType(),True), \
     StructField("USERID",StringType(),True), \
     StructField("CLIENT_ID",StringType(),True), \
     StructField("MSISDN",StringType(),True), \
     StructField("ANDROID_VERSION",StringType(),True), \
     StructField("LANGUAGE",StringType(),True), \
     StructField("SOURCE",StringType(),True), \
     StructField("AUTHENTICATION_TYPE",StringType(),True), \
     StructField("ENTERPRISE_USER",StringType(),True), \
     StructField("RESULT",StringType(),True), \
     StructField("METHOD_PARAMETERS",StringType(),True), \
     StructField("NULL_COLUMN2",StringType(),True), \
     StructField("NULL_COLUMN3",StringType(),True), \
     StructField("corruptrecord",StringType(),True), 
   ])

try:
  selfcare_with_schema = spark.read.format("csv")\
  .option("header", "false")\
  .schema(schema)\
  .option("sep", File_delimiter)\
  .option("quote", "\"")\
  .option("multiline","true")\
  .load(file_location+filedirectory)
  selfcare_with_schemaAllhubs= selfcare_with_schema.withColumn("Load_TS",lit(windowStartTime)).\
  withColumn("FileName",input_file_name()).withColumn("Last_Updated_By",lit("BAU")).\
  withColumn("Data_dt",lit(dt)).withColumn("Trigger_Key",lit(trigger_key)).withColumn("Derived_HUB_ID",lit(hub_id)).\
  withColumn("MSISDN",regexp_replace(col("MSISDN"), "\t", "")).withColumn("MSISDN",regexp_replace(col("MSISDN")," ", ""))
  selfcare_with_schemaAllhubs.createOrReplaceTempView("temp1_selfcare_name")
  Row_count = selfcare_with_schemaAllhubs.count()
  print(Row_count)
except Exception as e:
  print(file_location+filedirectory)
  print(e)
  nofilefound=True

# COMMAND ----------

# DBTITLE 1,Insert into Alert_Control for Duplicate Files
if DP != ['Y']:  
  dupfileslist=[]
  vduplicatesfiles=dfdffileinfo.filter(col('Duplicate')==1).select(concat('File_Location',lit('/'),'FileName'))
  for x in vduplicatesfiles.collect():
    dupfileslist.append(x[0])
  dupfilestr='\n'.join(dupfileslist)
  dup_data = '{"title": "' +FeedName+ ' Duplicate Files Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Duplicate Files Moved To {dwhadls{xxx}01}","error":{"Message":"Duplicate Files","TableName":"cfw.DL_File_Details","Path":"' +duplicate_file_location+ '","Duplicate Files":{"Name":"' +dupfilestr+ '"}}}'
  
  if (vduplicatesfiles.count() > 0):
    print('Duplicates Found')
    for i in range(1,5,1):
        try:
          spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key= int('"+trigger_key+"') AND Alert_Type = 'Duplicate_Files' ")
          break;
        except Exception as e1:
          if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e1)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e1)):
            time.sleep(30)
            continue
          else:
            raise Exception(e1)
            
    ##INSERT INTO cfw.etl_alert_control for duplicate Files
    spark.sql('''INSERT INTO cfw.etl_alert_control 
                   SELECT '''+trigger_key+''' AS trigger_key
                         ,'''"'"+FeedName+"'"''' AS FeedName
                         ,TO_TIMESTAMP('''"'"+windowStartTime+"'"''') AS Load_TS
                         ,'Duplicate_Files' AS Alert_Type
                         ,'''"'"+dup_data+"'"''' AS Alert_Reason
                         ,'''"'"+source_container+"'"''' AS Data_Storage
                         ,'cfw.DL_File_Details' AS Data_location
                         ,'''+str(vduplicatesfiles.count())+''' AS Data_Count 
                         ,'N' AS Alert_Sent
                         ,CURRENT_TIMESTAMP() AS Insert_Ts
                         ,NULL AS Update_ts
                         ,'''"'"+PipelineRunId+"'"''' AS Pipeline_Run_ID
              ''')
  
  else:
    print('Duplicates NOT Found in '+FeedName+' for HUB_ID ='+hub_id)

# COMMAND ----------

# DBTITLE 1,If File Not Found
if nofilefound is True:
  dbutils.notebook.exit("No Files Found")

# COMMAND ----------

# DBTITLE 1,Snapshot_Date Helper
# MAGIC %run /Shared/DWH_Transformation/TOOLS/Func_Snapshot_Date

# COMMAND ----------

# DBTITLE 1,Data Preparation: Selfcare
SelfcareFinal_CN = spark.sql('''
select 
     TRIM(DATETIMESTAMP)         AS DATETIMESTAMP
    ,TRIM(USER_AGENT)            AS USER_AGENT
    ,TRIM(IP_REQUESTER)          AS IP_REQUESTER
    ,TRIM(SELFCARE_METHOD)       AS SELFCARE_METHOD
    ,TRIM(SELFCARE_METHOD_ALIAS) AS SELFCARE_METHOD_ALIAS
    ,TRIM(EXECUTION_TIME)        AS EXECUTION_TIME
    ,CASE WHEN TRIM(USERID) IS NULL OR TRIM(USERID) = "" OR TRIM(USERID) = "null" THEN -1 ELSE TRIM(USERID) END AS USERID
    ,CASE WHEN TRIM(CLIENT_ID) IS NULL OR TRIM(CLIENT_ID) = "" OR TRIM(CLIENT_ID) = "null" THEN -1 ELSE TRIM(CLIENT_ID) END AS CLIENT_ID
    ,TRIM(MSISDN)                AS MSISDN
    ,TRIM(ANDROID_VERSION)       AS ANDROID_VERSION
    ,TRIM(LANGUAGE)              AS LANGUAGE
    ,TRIM(SOURCE)                AS SOURCE
    ,TRIM(AUTHENTICATION_TYPE)   AS AUTHENTICATION_TYPE
    ,TRIM(ENTERPRISE_USER)       AS ENTERPRISE_USER
    ,TRIM(RESULT)                AS RESULT
    ,TRIM(METHOD_PARAMETERS)     AS METHOD_PARAMETERS
    ,CASE WHEN NULL_COLUMN2 IS NULL OR NULL_COLUMN2 = "" OR NULL_COLUMN2 = "null" THEN -1 ELSE NULL_COLUMN2 END AS NULL_COLUMN2
    ,CASE WHEN NULL_COLUMN3 IS NULL OR NULL_COLUMN3 = "" OR NULL_COLUMN3 = "null" THEN -1 ELSE NULL_COLUMN3 END AS NULL_COLUMN3
    ,Load_TS
    ,REPLACE(SUBSTRING(a.FileName, POSITION('lz/' IN a.FileName)),'lz/','') AS FileName
    ,Last_Updated_By
    ,coalesce(c.HUB_ID,a.Derived_HUB_ID) AS HUB_ID
    ,CASE WHEN TRIM(b.MARKET_ID) IS NULL THEN -1 ELSE TRIM(b.MARKET_ID) END AS MARKET_ID
    ,corruptrecord
    ,Data_dt
    ,Trigger_Key
    ,CASE WHEN length(TRIM(MSISDN))>=10 
                AND length(TRIM(MSISDN))<=12 
                AND TRIM(MSISDN) IS NOT NULL 
                AND a.corruptrecord IS NULL THEN 1 ELSE 0 END AS IsValid
    ,CASE WHEN d.Container_Name IS NULL THEN "group" ELSE d.Container_Name END AS Container_Name
    from temp1_selfcare_name as a
    left outer join temp_MarketCode as b 
                    on b.MARKET_SH_CODE1= SUBSTRING(TRIM(a.MSISDN),1,1) 
                    OR b.MARKET_SH_CODE1= SUBSTRING(TRIM(a.MSISDN),1,2) 
                    OR b.MARKET_SH_CODE1= SUBSTRING(TRIM(a.MSISDN),1,3) 
                    OR b.MARKET_SH_CODE1= SUBSTRING(TRIM(a.MSISDN),1,4) 
                    OR b.MARKET_SH_CODE1= SUBSTRING(TRIM(a.MSISDN),1,5)
    left outer join temp1_HUBID as c 
    on TRIM(b.MARKET_ID) = TRIM(c.MARKET_ID)
    left outer join dgcdimensions_dl.containername as d
    on TRIM(b.MARKET_ID) = TRIM(d.MARKET_ID)
''')
Selfcare_final = SelfcareFinal_CN.withColumn("snapshot_dt", find_date(col("FileName")))
Selfcare_final.createOrReplaceTempView("temp2_SelfcareFinal_CN")

# COMMAND ----------

# DBTITLE 1,Temp Table Data Load
dflogstemp = spark.sql("SELECT DISTINCT Flag_Temp FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"'  AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TF         = dflogstemp.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()
if spark._jsparkSession.catalog().tableExists('stgods_dl', table_name) and TF == ['Y']:
  print("stgods_dl."+table_name+" table exists")
else:
  dbutils.fs.rm(path,recurse=True)
  spark.sql("CREATE OR REPLACE TABLE stgods_dl."+table_name+"  USING DELTA LOCATION '"+path+"' PARTITIONED BY (Container_Name) AS SELECT * FROM temp2_SelfcareFinal_CN")
  
  ##DELETE
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
        break;
      except Exception as e2:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e2)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e2)):
          time.sleep(30)
          continue
        else:
          raise Exception(e2)
          
  ##INSERT INTO LOG TABLE  
  spark.sql("INSERT INTO stgods_dl."+log_table+" VALUES ('"+windowStartTime+"','"+dt+"','"+source_container+"',int('"+trigger_key+"'),'N','N','N','N','N','N','N') ")
  
  ##Update Logging table
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Temp = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl."+table_name+" and Flag_Temp updated to Y in stgods_dl."+log_table+" for Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e3:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e3):
        time.sleep(30)
        continue
      else:
        raise Exception(e3)

# COMMAND ----------

# DBTITLE 1,Country Wise Incremental Load
dflogsDL = spark.sql("SELECT DISTINCT Flag_DataLake FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TDL      = dflogsDL.select("Flag_DataLake").distinct().rdd.flatMap(lambda x: x).collect()
if TDL == ['N']:
  MK=Selfcare_final.select("Container_Name").distinct().rdd.flatMap(lambda x: x).collect()
  print(MK)
  for t in MK :    
    ##DELETE
    for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM "+t+"_dl."+FeedName+" where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e4:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e4)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e4)):
          time.sleep(30)
          continue
        else:
          raise Exception(e)
  
    ##INSERT INTO COUNTRY WISE DELTA TABLE     
    spark.sql(" INSERT INTO "+t+"_dl."+FeedName+" SELECT DATETIMESTAMP,USER_AGENT,IP_REQUESTER,SELFCARE_METHOD,SELFCARE_METHOD_ALIAS,EXECUTION_TIME ,USERID,CLIENT_ID,MSISDN,ANDROID_VERSION,LANGUAGE,SOURCE,AUTHENTICATION_TYPE,ENTERPRISE_USER,RESULT,METHOD_PARAMETERS,NULL_COLUMN2,NULL_COLUMN3,Load_TS,FileName,Last_Updated_By,int(HUB_ID),int(MARKET_ID),Data_dt,int(Trigger_Key),REPLACE(snapshot_dt,'-','') as DATE_CODE FROM stgods_dl."+table_name+" WHERE Container_Name = '"+t+"' AND  IsValid = 1")
    
  #Update logging flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Datalake = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      print("Data inserted into "+FeedName+" Data Lake tables and Flag_Datalake updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e5:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e5):
        time.sleep(180)
        continue
      else:
        raise Exception(e5)
else:
  print("Data in "+FeedName+" Data Lake tables exists for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Datalake = "+str(TDL))

# COMMAND ----------

# DBTITLE 1,For ODS Layer Processing 
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
  ##DELETE
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM stgods_dl.ods_"+FeedName+" where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
      break;
    except Exception as e6:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e6)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e6)):
          time.sleep(30)
          continue
        else:
          raise Exception(e6)
          
  ##INSERT INTO stgods_dl.ODS_Selfcare_Usage 
  spark.sql("INSERT INTO stgods_dl.ods_"+FeedName+" SELECT * FROM stgods_dl."+table_name+" where IsValid = 1 ")
    
  ##update logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_StgODS  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl.ods_"+FeedName+" and Flag_StgODS updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e7:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e7):
        time.sleep(30)
        continue
      else:
        raise Exception(e7) 
else:
  print("Data exists in stgods_dl.ods_"+FeedName+" table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_StgODS = "+str(TT))

# COMMAND ----------

# DBTITLE 1,cfw.Table_load_status INSERT
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['Y']:
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM cfw.Table_load_status where FeedName = '"+FeedName+"' AND  Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e8:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
          time.sleep(30)
          continue
        else:
          raise Exception(e8)
          
  ##INSERT INTO cfw.Table_load_status 
  spark.sql('''
  INSERT INTO cfw.Table_load_status 
  SELECT DISTINCT int(Trigger_Key)
                 ,Load_TS
                 ,snapshot_dt
                 ,CAST(HUB_ID as int) AS HUB_ID
                 ,"'''+FeedName +'''" AS FeedName
                 ,"REDKNEE" AS Source_System
                 ,"stgods_dl" as target_db
                 ,LOWER(CONCAT("ods_","'''+FeedName +'''")) as target_table_name
                 ,'Y' as load_status
                 ,count(*) as total_records
                 ,count(distinct FileName) as total_files
                 ,"'''+str(PipelineRunId)+'''" AS Pipeline_Run_Id
                 ,current_timestamp()
                 ,null FROM stgods_dl.'''+table_name+''' WHERE IsValid = 1
                 group by snapshot_dt,HUB_ID,Source_System,Trigger_Key,Load_TS ''')
  print("Data inserted into cfw.Table_load_status")

# COMMAND ----------

# DBTITLE 1,Invalid Records Segregation and alert
Schema_Mismatch_cnt = spark.sql("select * from stgods_dl."+table_name+" Where (corruptrecord IS NOT NULL AND corruptrecord <> '' AND corruptrecord <>' ')").count()
Invalid_MSISDNs_cnt = spark.sql("select * from stgods_dl."+table_name+" Where IsValid = 0 AND (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ')").count()
dflogserror = spark.sql("SELECT DISTINCT Flag_Error FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TE          = dflogserror.select("Flag_Error").distinct().rdd.flatMap(lambda x: x).collect()
if TE == ['N']:
  if Schema_Mismatch_cnt != 0 or Invalid_MSISDNs_cnt!=0:
    for i in range(1,5,1):
      try:
        spark.sql(" DELETE FROM error_dl.reject_"+FeedName+" where Error_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_key=int('"+trigger_key+"') ")
        spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key=int('"+trigger_key+"') AND Alert_Type = 'Invalid_Data' ")
        break;
      except Exception as e9:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e9)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e9)):
          time.sleep(30)
          continue
        else:
          raise Exception(e9)
   
    ##INSERT INTO ERROR and Alert control Table
    spark.sql(" INSERT INTO TABLE error_dl.reject_"+FeedName +" SELECT a.*,CASE WHEN a.IsValid = 0 AND (a.corruptrecord IS NULL OR a.corruptrecord = '' OR a.corruptrecord = ' ') THEN 'Invalid_MSISDNs' ELSE 'Schema_Mismatch' END AS Error_Type FROM stgods_dl."+table_name+" as a WHERE IsValid = 0 ")
    data_error = '{"title": "' +FeedName+ ' Invalid Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Erroneous Records Moved To {dgc{xxx}datalake01}","error":{"Message":"Invalid Records","TableName":"error_dl.reject_'+FeedName.lower()+'","Path":"/error/mydigicelapp/reject_'+FeedName.lower()+'","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+ '","Total Invalid Records": "' +str(Schema_Mismatch_cnt)+ '"}}}'
    
    spark.sql('''
     INSERT INTO cfw.etl_alert_control 
     select DISTINCT int("'''+trigger_key+'''") AS trigger_key
                     ,"'''+FeedName+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+windowStartTime+'''") AS Load_TS
                     ,"Invalid_Data" as Alert_Type
                     ,'"'''+str(data_error)+'''"' AS Alert_Reason
                     ,"'''+source_container+'''" as Data_Storage
                     ,LOWER(CONCAT("error_dl.reject_","'''+FeedName +'''")) as Data_location
                     ,"'''+str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+'''" as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID 
              ''')
  ##Update logging  Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Error  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into error_dl.reject_"+FeedName +" and Flag_Error updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as err:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(err):
        time.sleep(180)
        continue
      else:
        raise Exception(err)

else:
  print("Data exists in error_dl.reject_"+FeedName +" table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TE))

# COMMAND ----------

# DBTITLE 1,Audit Data Collection
df_audit = spark.sql(''' SELECT LOAD_TS
		                       ,FileName
                               ,count(*) AS total_RecordCount
		                       ,SUM(CASE 
                                        WHEN FileName IS NOT NULL AND IsValid = 1
					                    THEN 1
				                    ELSE 0
				                    END) AS RecordCount_FN
		                       ,SUM(CASE 
		                      		     WHEN IsValid = 0
                                         AND (corruptrecord IS NULL OR corruptrecord = "" OR corruptrecord = " ")
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_InvalidMSISDNs
                               ,SUM(CASE 
		                      		     WHEN (corruptrecord IS NOT NULL AND corruptrecord <> "" AND corruptrecord <> " ")
                                         AND IsValid = 0
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_SchemaMismatch
	                     FROM stgods_dl.'''+table_name+'''
	                     GROUP BY FileName,LOAD_TS ''')
df_audit.createOrReplaceTempView("df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,For Rejected Files Audit
spark.sql("DROP TABLE IF EXISTS stgods_dl."+FeedName+"_audit_"+trigger_key)
spark.sql("CREATE TABLE stgods_dl."+FeedName+"_audit_"+trigger_key+ " AS SELECT DISTINCT Load_TS,FileName,total_RecordCount,RecordCount_FN,cnt_InvalidMSISDns,cnt_SchemaMismatch FROM df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,Audit Data details for cfw.DL_File_Details Table
dflogsaudit = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TA          = dflogsaudit.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if TA == ['N']:
  for i in range(1,5,1):
    try:
      bflag = 0
      spark.sql('''MERGE INTO cfw.DL_File_Details AS TARGET 
      USING (
      select a.*
            ,CONCAT('{"Invalid MSISDNs":"',cnt_InvalidMSISDNs,'","Schema Mismatch":"',cnt_SchemaMismatch,'"}') as error_reason from df_audit_'''+FeedName+''' a) SOURCE 
      on trim(SOURCE.FILENAME)=trim(TARGET.FILENAME) 
      and TARGET.File_load_status != 'D' 
      and TARGET.Load_TS = TO_TIMESTAMP("'''+ windowStartTime+'''") 
      and TARGET.TRIGGER_KEY= int("'''+trigger_key+'''")
      WHEN MATCHED  THEN UPDATE SET 
      TARGET.File_load_status='Y'
     ,TARGET.Total_Records=SOURCE.total_RecordCount
     ,TARGET.valid_records=SOURCE.RecordCount_FN
     ,TARGET.error_records=SOURCE.cnt_InvalidMSISDNs + SOURCE.cnt_SchemaMismatch
     ,TARGET.error_reason= SOURCE.error_reason ''')
                        
      bflag = 1
      
      #logging
      if bflag == 1:
        spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Audit  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
        print("Data inserted into cfw.DL_File_Details and Flag_Audit updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" ")     
      break;
    except Exception as e11:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e11):
        time.sleep(180)
        continue
      else:
        raise Exception(e11)
  
  ##IF Merge could not happen successfully
  if bflag == 0:
      raise Exception("Retry!")
else:
  print("Data exists in cfw.DL_File_Details table for "+FeedName +" feed for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TA))

# COMMAND ----------

# DBTITLE 1,Rejected and Empty Files Logging
db = "stgods_dl"
FeedName              = FeedName
feedaudit             = FeedName+"_audit_"+trigger_key
foldername            = "GR_PRODUCT_DELIVERY/GLOBAL/SELFCAREUSAGE/lz/"     
tempaudit_rejected    = FeedName+"_rejected_"+trigger_key
file_location_reject  = "/mnt/"+target_container+"/"+FeedName+"/reject/"+source_container+"/"+dt+"/"+dateId
file_location_working = file_location+filedirectory

#Rejected Files Flow
dflogsreject = spark.sql("SELECT DISTINCT Flag_reject FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TR          = dflogsreject.select("Flag_reject").distinct().rdd.flatMap(lambda x: x).collect()
if TR  == ['N']:
  dbutils.notebook.run("/Shared/DWH_Transformation/DATALAKE/Generic/RejectedFiles", 43200,{"windowStartTime": windowStartTime, "PipelineRunId": PipelineRunId, "DataFactoryName":DataFactoryName,"PipelineName":PipelineName,"trigger_key":trigger_key,"db":db,"feedaudit":feedaudit,"foldername":foldername,"file_location":file_location,"file_location_reject":file_location_reject,"file_location_working":file_location_working,"tempaudit_rejected":tempaudit_rejected,"FeedName":FeedName,"source_container":source_container})
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Reject  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      print("Data Moved to rejected folder for rejected files and to cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND source_container = "+source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e13:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e13):
        time.sleep(180)
        continue
      else:
        raise Exception(e13)
else:
  print("Data exists in cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND source_container = "+source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TR))

# COMMAND ----------

# DBTITLE 1, Notebook status Logging
for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_NotebookStatus  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Flag_Temp = 'Y' AND Flag_Datalake = 'Y' AND Flag_StgODS = 'Y' AND Flag_Error = 'Y' AND Flag_Audit = 'Y' AND Flag_reject = 'Y' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      break;
    except Exception as e14:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e14):
        time.sleep(30)
        continue
      else:
        raise Exception(e14)     

# COMMAND ----------

# DBTITLE 1,Clean up and Exit
dflogs_final = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
NFS =dflogs_final.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
print(NFS)
if NFS == ['Y']:
  spark.sql(" DROP TABLE IF EXISTS stgods_dl."+table_name+" ")
  dbutils.fs.rm(path,recurse=True)
  dbutils.notebook.exit("Successful")
/Shared/SELFCARE_USAGE
# MAGIC %md
# MAGIC ###### Purpose: Notebook to process SHAKE IT WON REWARD raw data
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |Feb 25, 2021|Shagufa Akhzar| Created the notebook to process Shake It Won Reward raw data | 10 min |
# MAGIC |Mar 15, 2021|Shagufa Akhzar| Updated the logic with temp table creation | 1.5 min |
# MAGIC |Apr 14, 2021|Shagufa Akhzar| Updated stg_auditcontrol logic to store erroneous records in a single column| 1.5 min |
# MAGIC |May 11, 2021|Shagufa Akhzar| Updated the logic for receiving files from multiple hubs and added logging at each level| 2 min |
# MAGIC |Dec 27, 2021|Ayush Kothari/Deepti Mahajan | Added Duplicate Check, Audit tables handling | 5 min |

# COMMAND ----------

# DBTITLE 1,Create widgets to extract parameters from ADF
dbutils.widgets.removeAll()
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("trigger_key", "")
dbutils.widgets.text("FeedName", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("Source_container", "")
dbutils.widgets.text("Target_container", "")
dbutils.widgets.text("File_delimiter", "")
dbutils.widgets.text("hub_id", "")

DataFactoryName = dbutils.widgets.get("DataFactoryName")
PipelineName    = dbutils.widgets.get("PipelineName")
PipelineRunId   = dbutils.widgets.get("PipelineRunId")
windowStartTime = dbutils.widgets.get("windowStartTime")
trigger_key           = str(dbutils.widgets.get("trigger_key"))
FeedName              = dbutils.widgets.get("FeedName")
PipelineRunId         = dbutils.widgets.get("PipelineRunId")
source_container      = dbutils.widgets.get("Source_container")
target_container      = dbutils.widgets.get("Target_container")
File_delimiter        = dbutils.widgets.get("File_delimiter")
hub_id                = str(dbutils.widgets.get("hub_id"))

# COMMAND ----------

# DBTITLE 1,Optimization Hints
# MAGIC %sql
# MAGIC set spark.sql.shuffle.partitions = 400;
# MAGIC set spark.sql.files.maxPartitionBytes=1073741824;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;

# COMMAND ----------

# DBTITLE 1,Refresh tables
# MAGIC %sql
# MAGIC REFRESH TABLE dgcdimensions_dl.ContainerName;
# MAGIC REFRESH TABLE dgcdimensions_dl.AreaCode;
# MAGIC REFRESH TABLE stgods_dl.shakeitwonreward_logs;
# MAGIC REFRESH TABLE dgcdimensions_dl.countrycode;

# COMMAND ----------

# DBTITLE 1,Import Required Parameters and packages
import glob
from pyspark.sql import Row
import time
import os
import json
import pyspark
import requests
import pyspark.sql.functions as F
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit,col,lower,regexp_replace,split,when,concat
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
spark.conf.set("spark.databricks.io.cache.enabled", "true")
spark.sql("set spark.sql.files.ignoreCorruptFiles=true")
spark.sql("REFRESH TABLE stgods_dl."+FeedName+"_logs")

# COMMAND ----------

# DBTITLE 1,Create dynamic variables 
dt = windowStartTime[:10].replace('-','')
dateId = windowStartTime.replace('-','').replace(':','')
file_location = "/mnt/"+target_container+"/"+FeedName+"/working/"+source_container+"/"+dt+"/"+dateId
duplicate_file_location = "/mnt/"+target_container+"/"+FeedName+"/duplicate/"+source_container+"/"+dt+"/"+dateId
table_name      = 'ShakeItWonReward_'+dateId+hub_id+'_tmp'
path = "/mnt/stgods/"+FeedName+"/stg_cn_"+FeedName+dateId+hub_id
log_table = FeedName.lower()+"_logs"
print("log_table:",log_table)
print("path:",path)
print(table_name)
print(file_location)
print(duplicate_file_location)

# COMMAND ----------

# DBTITLE 1,Check Notebook status for current Load timestamp
NotebookStatus = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
NS_status = NotebookStatus.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
if NS_status == ['Y']:
  dbutils.notebook.exit( FeedName+" Notebook Status is already successful for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_NotebookStatus = "+str(NS_status))
else:
  print("Execute Further Notebook Commands")

# COMMAND ----------

# DBTITLE 1,For Market ID Derivation
dfcode=spark.sql("SELECT distinct MARKET_ID, CAST(MARKET_SH_CODE1 AS STRING) AS MARKET_SH_CODE1 FROM dgcdimensions_dl.areacode  where LINE_OF_BUSINESS = 'GSM' ")
dfcode.createOrReplaceTempView("temp_MarketCode")
dfHUBID=spark.sql("SELECT DISTINCT Market_Key AS MARKET_ID,HUB_ID FROM ods_dl.market")
dfHUBID.createOrReplaceTempView("market_vw")

# COMMAND ----------

# DBTITLE 1,Function for Check Duplicate File
# MAGIC %run /Shared/DWH_Transformation/TOOLS/CheckDuplicateFile_v2

# COMMAND ----------

# DBTITLE 1,Duplicate File Checking
filedirectory="/*/*/SHAKEITREWARD/lz/*"
dflogsduplicate = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
DP          = dflogsduplicate.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if DP != ['Y']:
  dfdffileinfo=checkDuplicateFile(File_Location=str(file_location),File_Directory=str(filedirectory),FeedName=str(FeedName),Trigger_Key=str(trigger_key),WindowStartTime=str(windowStartTime),PipelineRunID=str(PipelineRunId),FileLocation_dup =str(duplicate_file_location))

# COMMAND ----------

# DBTITLE 1,Store all the flag values in a dataframe
df_flag = spark.sql('''
          SELECT DISTINCT Source_Container
                          , Trigger_key
                          , Flag_Temp
                          , Flag_Datalake
                          , Flag_StgODS
                          , Flag_Error
                          , Flag_Audit
                          , Flag_Reject
                          , Flag_NotebookStatus
          FROM stgods_dl.'''+log_table+''' WHERE Data_dt = '''"'"+dt+"'"''' AND Load_TS = '''"'"+windowStartTime+"'"'''
          ''')

# COMMAND ----------

# DBTITLE 1,Schema Enforcement
nofilefound=False
schema = StructType([ \
     StructField("ID",StringType(),True), \
     StructField("MSISDN",StringType(),True), \
     StructField("STATUS",StringType(),True), \
     StructField("REWARD_TYPE",StringType(),True), \
     StructField("REWARD_ID",StringType(),True), \
     StructField("WON_DATE",StringType(),True), \
     StructField("GUID",StringType(),True), \
     StructField("REDEMPTION_DATE",StringType(),True), \
     StructField("REWARD_SNAPSHOT",StringType(),True), \
     StructField("BENEFICIARY_MSISDN",StringType(),True), \
     StructField("GROUP_ID",StringType(),True), \
     StructField("CATEGORY_ID",StringType(),True), \
     StructField("INITIAL_REWARD_ID",StringType(),True), \
     StructField("corruptrecord",StringType(),True) \

   ]) 

try:
    spark.sql("set spark.sql.files.ignoreCorruptFiles=true")  
    dfshakeitreward = spark.read.format("csv")\
    .option("header", "false")\
    .schema(schema)\
    .option("sep", "|")\
    .option("quote", "\"")\
    .option("multiline","true")\
    .load(file_location+filedirectory)
    shakeitreward_with_schemaAllhubs= dfshakeitreward.withColumn("Load_TS",lit(windowStartTime)).\
    withColumn("FileName",input_file_name()).withColumn("Last_Updated_By",lit("BAU")).\
    withColumn("Data_dt",lit(dt)).withColumn("Trigger_Key",lit(trigger_key)).\
    withColumn("MSISDN",regexp_replace(col("MSISDN"), "\t", "")).withColumn("MSISDN",regexp_replace(col("MSISDN")," ", ""))
    shakeitreward_with_schemaAllhubs.createOrReplaceTempView("temp_shakeit_wonreward")
    Row_count = shakeitreward_with_schemaAllhubs.count()
    print(Row_count)
except Exception as e:
    print(file_location+filedirectory)
    print(e)
    nofilefound=True                     

# COMMAND ----------

# DBTITLE 1,Insert into Alert Control for Duplicate Files
if DP != ['Y']:  
    dupfileslist=[]
    vduplicatesfiles=dfdffileinfo.filter(col('Duplicate')==1).select(concat('File_Location',lit('/'),'FileName'))
    for x in vduplicatesfiles.collect():
        dupfileslist.append(x[0])
        dupfilestr='\n'.join(dupfileslist)
        dup_data = '{"title": "' +FeedName+ ' Duplicate Files Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Duplicate Files Moved To {dwhadls{xxx}01}","error":{"Message":"Duplicate Files","TableName":"cfw.DL_File_Details","Path":"' +duplicate_file_location+ '","Duplicate Files":{"Name":"' +dupfilestr+ '"}}}'
        
    if (vduplicatesfiles.count() > 0):
        print('Duplicates Found')
        for i in range(1,5,1):
            try:
                spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key= int('"+trigger_key+"') AND Alert_Type = 'Duplicate_Files' ")
                break;
            except Exception as e1:
                if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e1)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e1)):
                   time.sleep(30)
                   continue
                else:
                   raise Exception(e1)
            
        ##INSERT INTO cfw.etl_alert_control for duplicate Files
        spark.sql('''INSERT INTO cfw.etl_alert_control 
                   SELECT '''+trigger_key+''' AS trigger_key
                         ,'''"'"+FeedName+"'"''' AS FeedName
                         ,TO_TIMESTAMP('''"'"+windowStartTime+"'"''') AS Load_TS
                         ,'Duplicate_Files' AS Alert_Type
                         ,'''"'"+dup_data+"'"''' AS Alert_Reason
                         ,'''"'"+source_container+"'"''' AS Data_Storage
                         ,'cfw.DL_File_Details' AS Data_location
                         ,'''+str(vduplicatesfiles.count())+''' AS Data_Count 
                         ,'N' AS Alert_Sent
                         ,CURRENT_TIMESTAMP() AS Insert_Ts
                         ,NULL AS Update_ts
                         ,'''"'"+PipelineRunId+"'"''' AS Pipeline_Run_ID
              ''')
    else:
        print('Duplicates NOT Found in '+FeedName+' for HUB_ID ='+hub_id)

# COMMAND ----------

# DBTITLE 1,If File not Found
if nofilefound is True:
    dbutils.notebook.exit("No Files Found")

# COMMAND ----------

# DBTITLE 1,Use database ShakeIT_dl
# MAGIC %sql
# MAGIC USE ShakeIT_dl;

# COMMAND ----------

# DBTITLE 1,Snapshot Date Helper
# MAGIC %run /Shared/DWH_Transformation/TOOLS/Func_Snapshot_Date

# COMMAND ----------

# DBTITLE 1,Create main ShakeItWonReward temp table 
Flag_tmptable = df_flag.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()

if ((spark._jsparkSession.catalog().tableExists('shakeit_dl', table_name) == False) and (Flag_tmptable == ['N'] or Flag_tmptable == [])):
    print("Logging start of cell - Flag_Temp is 'N' in stgods_dl.shakeitwonreward_logs !!!")
    dbutils.fs.rm("dbfs:/user/hive/warehouse/shakeit_dl/"+table_name,True)
    
    ShakeitWonReward_tmp = spark.sql('''CREATE TABLE '''+table_name+''' USING DELTA PARTITIONED BY(Container_Name) AS 
	SELECT TRIM(a.ID) AS ID
		,TRIM(a.MSISDN) AS MSISDN
		,CASE 
          WHEN TRIM(a.STATUS) IS NULL 
              OR TRIM(a.STATUS) = "" 
              OR TRIM(a.STATUS) = "null" 
              THEN '-1' 
          ELSE TRIM(a.STATUS) 
          END AS STATUS
        ,TRIM(a.REWARD_TYPE) AS REWARD_TYPE
		,TRIM(a.REWARD_ID) AS REWARD_ID
		,TRIM(a.WON_DATE) AS WON_DATE
		,TRIM(a.GUID) AS GUID
		,TRIM(a.REDEMPTION_DATE) AS REDEMPTION_DATE
		,TRIM(a.REWARD_SNAPSHOT) AS REWARD_SNAPSHOT
		,TRIM(a.BENEFICIARY_MSISDN) AS BENEFICIARY_MSISDN
        ,CASE 
          WHEN a.GROUP_ID IS NULL 
              OR a.GROUP_ID = "" 
              OR a.GROUP_ID = "null" 
              THEN '-1' 
          ELSE a.GROUP_ID 
          END AS GROUP_ID
        ,CASE 
          WHEN a.CATEGORY_ID IS NULL 
              OR a.CATEGORY_ID = "" 
              OR a.CATEGORY_ID = "null" 
              THEN '-1' 
          ELSE a.CATEGORY_ID 
          END AS CATEGORY_ID
        ,TRIM(a.INITIAL_REWARD_ID) AS INITIAL_REWARD_ID
		,IFNULL(b.MARKET_ID, - 1) AS MARKET_ID
        ,a.LOAD_TS
		,RIGHT(a.FileName,CHARINDEX('/',REVERSE(a.FileName))-1) AS FileName
		,a.Last_Updated_by
        ,a.Data_dt
        ,a.Trigger_Key
        ,m.HUB_ID AS HUB_ID
        ,a.corruptrecord
		,CASE 
            WHEN (LENGTH(TRIM(a.MSISDN)) BETWEEN 10 and 12) 
                AND (LENGTH(TRIM(a.BENEFICIARY_MSISDN)) BETWEEN 10 and 12 AND TRIM(a.BENEFICIARY_MSISDN) IS NOT NULL) 
                AND a.corruptrecord IS NULL
				THEN 1
			ELSE 0
			END AS IsValid
        ,IFNULL(TRIM(c.Container_Name), 'group') AS Container_Name
	FROM temp_shakeit_wonreward a 
	LEFT JOIN temp_MarketCode AS b ON SUBSTRING(TRIM(a.MSISDN), 1, 5) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 4) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 3) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 2) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 1) = b.MARKET_SH_CODE1
LEFT JOIN dgcdimensions_dl.ContainerName c ON b.MARKET_ID = TRIM(c.MARKET_ID)
Left JOIN market_vw m ON b.MARKET_ID = TRIM(m.MARKET_ID) ''')
    
    ShakeitWonReward_CN = spark.sql('''select * from '''+table_name+''' ''')
        
    print("TempTable is created !!!")
    ShakeitWonReward_final = ShakeitWonReward_CN.withColumn("snapshot_dt", find_date(col("FileName")))
    ShakeitWonReward_final.createOrReplaceTempView("temp2_ShakeitWonReward_CN") 
else:
    print("Table "+table_name+" already exists for the trigger time : "+windowStartTime) 

# COMMAND ----------

# DBTITLE 1,Temp Table Data Load
dflogstemp = spark.sql("SELECT DISTINCT Flag_Temp FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"'  AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TF         = dflogstemp.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()
if spark._jsparkSession.catalog().tableExists('stgods_dl', table_name) and TF == ['Y']:
  print("stgods_dl."+table_name+" table exists")
else:
  dbutils.fs.rm(path,recurse=True)
  spark.sql("CREATE OR REPLACE TABLE stgods_dl."+table_name+"  USING DELTA LOCATION '"+path+"' PARTITIONED BY (Container_Name) AS SELECT * FROM temp2_ShakeitWonReward_CN")
  
  ##DELETE
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
        break;
      except Exception as e2:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e2)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e2)):
          time.sleep(30)
          continue
        else:
          raise Exception(e2)
          
  ##INSERT INTO LOG TABLE  
  spark.sql("INSERT INTO stgods_dl."+log_table+" VALUES ('"+windowStartTime+"','"+dt+"','"+source_container+"',int('"+trigger_key+"'),'N','N','N','N','N','N','N') ")
  
  ##Update Logging table
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Temp = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl."+table_name+" and Flag_Temp updated to Y in stgods_dl."+log_table+" for Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e3:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e3):
        time.sleep(30)
        continue
      else:
        raise Exception(e3)

# COMMAND ----------

# DBTITLE 1,Country wise Incremental Load
dflogsDL = spark.sql("SELECT DISTINCT Flag_DataLake FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TDL      = dflogsDL.select("Flag_DataLake").distinct().rdd.flatMap(lambda x: x).collect()
if TDL == ['N']:
  MK = spark.sql("SELECT DISTINCT Container_Name FROM "+table_name).rdd.flatMap(lambda x: x).collect()
  print(MK)
  for t in MK :    
    ##DELETE
    for i in range(1,5,1):
      try:
        spark.sql('''
        DELETE FROM '''+t+'''_dl.'''+FeedName+''' where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_key = "'''+trigger_key+'''" ''')
        break;
      except Exception as e4:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e4)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e4)):
          time.sleep(30)
          continue
        else:
          raise Exception(e4)
  
    ##INSERT INTO COUNTRY WISE DELTA TABLE     
    spark.sql(" INSERT INTO "+t+"_dl."+FeedName+" SELECT ID,MSISDN,STATUS,REWARD_TYPE,REWARD_ID,WON_DATE,GUID,REDEMPTION_DATE,REWARD_SNAPSHOT,BENEFICIARY_MSISDN,GROUP_ID,CATEGORY_ID,INITIAL_REWARD_ID,LOAD_TS,FileName,Last_Updated_by,Data_dt,MARKET_ID,Trigger_Key,int(HUB_ID),REPLACE(snapshot_dt,'-','') as date_code FROM stgods_dl."+table_name+" WHERE Container_Name = '"+t+"' AND  IsValid = 1")
    
  #Update logging flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Datalake = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      print("Data inserted into "+FeedName+" Data Lake tables and Flag_Datalake updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e5:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e5):
        time.sleep(180)
        continue
      else:
        raise Exception(e5)
else:
  print("Data in "+FeedName+" Data Lake tables exists for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Datalake = "+str(TDL))

# COMMAND ----------

# DBTITLE 1,For ODS Layer Processing
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
  ##DELETE
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM stgods_dl.ODS_ShakeIt_WonReward where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
      break;
    except Exception as e6:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e6)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e6)):
          time.sleep(30)
          continue
        else:
          raise Exception(e6)
          
  ##INSERT INTO stgods_dl.ODS_ShakeitWonReward 
  spark.sql("INSERT INTO stgods_dl.ODS_ShakeIt_WonReward SELECT ID,MSISDN,STATUS,REWARD_TYPE,REWARD_ID,WON_DATE,GUID,REDEMPTION_DATE,REWARD_SNAPSHOT,BENEFICIARY_MSISDN,GROUP_ID,CATEGORY_ID,INITIAL_REWARD_ID,Market_Id,LOAD_TS,FileName,Last_Updated_by,Data_dt,Trigger_Key,int(HUB_ID),snapshot_dt FROM stgods_dl."+table_name+" where IsValid = 1 ")
    
  ##update logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_StgODS  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl.ODS_ShakeIt_WonReward and Flag_StgODS updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as e7:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e7):
        time.sleep(30)
        continue
      else:
        raise Exception(e7) 
else:
  print("Data exists in stgods_dl.ODS_ShakeIt_WonReward table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_StgODS = "+str(TT))

# COMMAND ----------

# DBTITLE 1,Insert into cfw.table_load_status
dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['Y']:
  for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM cfw.Table_load_status where FeedName = '"+FeedName+"' AND  Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND Trigger_Key = '"+trigger_key+"' ")
        break;
      except Exception as e8:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
          time.sleep(30)
          continue
        else:
          raise Exception(e8)
          
  ##INSERT INTO cfw.Table_load_status 
  spark.sql('''
  INSERT INTO cfw.Table_load_status 
  SELECT DISTINCT int(Trigger_Key)
                 ,Load_TS
                 ,snapshot_dt
                 ,CAST("'''+hub_id+'''" as int) AS HUB_ID
                 ,"'''+FeedName +'''" AS FeedName
                 ,"REDKNEE" AS Source_System
                 ,"stgods_dl" as target_db
                 ,LOWER(CONCAT("ods_","'''+FeedName +'''")) as target_table_name
                 ,'Y' as load_status
                 ,count(*) as total_records
                 ,count(distinct FileName) as total_files
                 ,"'''+str(PipelineRunId)+'''" AS Pipeline_Run_Id
                 ,current_timestamp()
                 ,null
                 FROM stgods_dl.'''+table_name+''' WHERE IsValid = 1
                 group by snapshot_dt,HUB_ID,Source_System,Trigger_Key,Load_TS ''')
  print("Data inserted into cfw.Table_load_status")

# COMMAND ----------

# DBTITLE 1,Invalid record segregation
Schema_Mismatch_cnt = spark.sql("select * from stgods_dl."+table_name+" Where (corruptrecord IS NOT NULL AND corruptrecord <> '' AND corruptrecord <>' ')").count()
Invalid_MSISDNs_cnt = spark.sql("select * from stgods_dl."+table_name+" Where IsValid = 0 AND (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ')").count()
dflogserror = spark.sql("SELECT DISTINCT Flag_Error FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TE          = dflogserror.select("Flag_Error").distinct().rdd.flatMap(lambda x: x).collect()
if TE == ['N']:
  if Schema_Mismatch_cnt != 0 or Invalid_MSISDNs_cnt!=0:
    for i in range(1,5,1):
      try:
        spark.sql(" DELETE FROM error_dl.reject_"+FeedName+" where Error_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_key=int('"+trigger_key+"') ")
        spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key=int('"+trigger_key+"') AND Alert_Type = 'Invalid_Data' ")
        break;
      except Exception as e9:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e9)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e9)):
          time.sleep(30)
          continue
        else:
          raise Exception(e9)
   
    ##INSERT INTO ERROR and Alert control Table
    spark.sql(" INSERT INTO TABLE error_dl.reject_"+FeedName +" SELECT ID,MSISDN,STATUS,REWARD_TYPE,REWARD_ID,WON_DATE,GUID,REDEMPTION_DATE,REWARD_SNAPSHOT,BENEFICIARY_MSISDN,GROUP_ID,CATEGORY_ID,INITIAL_REWARD_ID,LOAD_TS,FileName,Last_Updated_by,snapshot_dt,corruptrecord,snapshot_dt as Error_dt,CASE WHEN a.IsValid = 0 AND (a.corruptrecord IS NULL OR a.corruptrecord = '' OR a.corruptrecord = ' ') THEN 'Invalid_MSISDNs' ELSE 'Schema_Mismatch' END as Error_Type,Data_dt,trigger_key,hub_id FROM stgods_dl."+table_name+" as a WHERE IsValid = 0 ")
    data_error = '{"title": "' +FeedName+ ' Invalid Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Erroneous Records Moved To {dgc{xxx}datalake01}","error":{"Message":"Invalid Records","TableName":"error_dl.reject_'+FeedName.lower()+'","Path":"/error/shakeit/reject_'+FeedName.lower()+'","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+ '","Total Invalid Records": "' +str(Schema_Mismatch_cnt)+ '"}}}'
    
    spark.sql('''
     INSERT INTO cfw.etl_alert_control 
     select DISTINCT int("'''+trigger_key+'''") AS trigger_key
                     ,"'''+FeedName+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+windowStartTime+'''") AS Load_TS
                     ,"Invalid_Data" as Alert_Type
                     ,'"'''+str(data_error)+'''"' AS Alert_Reason
                     ,"'''+source_container+'''" as Data_Storage
                     ,LOWER(CONCAT("error_dl.reject_","'''+FeedName +'''")) as Data_location
                     ,"'''+str(Schema_Mismatch_cnt+Invalid_MSISDNs_cnt)+'''" as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID 
              ''')
  ##Update logging  Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Error  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into error_dl.reject_"+FeedName +" and Flag_Error updated to Y in stgods_dl."+log_table+" Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
      break;
    except Exception as err:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(err):
        time.sleep(180)
        continue
      else:
        raise Exception(err)

else:
  print("Data exists in error_dl.reject_"+FeedName +" table for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and  Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TE))

# COMMAND ----------

# DBTITLE 1,Audit Data Collection
df_audit = spark.sql(''' SELECT LOAD_TS
		                       ,FileName
                               ,count(*) AS total_RecordCount
		                       ,SUM(CASE 
                                        WHEN FileName IS NOT NULL AND IsValid = 1
					                    THEN 1
				                    ELSE 0
				                    END) AS RecordCount_FN
		                       ,SUM(CASE 
		                      		     WHEN IsValid = 0
                                         AND (corruptrecord IS NULL OR corruptrecord = "" OR corruptrecord = " ")
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_InvalidMSISDNs
                               ,SUM(CASE 
		                      		     WHEN (corruptrecord IS NOT NULL AND corruptrecord <> "" AND corruptrecord <> " ")
                                         AND IsValid = 0
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_SchemaMismatch
	                     FROM stgods_dl.'''+table_name+'''
	                     GROUP BY FileName,LOAD_TS ''')
df_audit.createOrReplaceTempView("df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,For Reject files Audit
spark.sql("DROP TABLE IF EXISTS stgods_dl."+FeedName+"_audit_"+trigger_key)
spark.sql("CREATE TABLE stgods_dl."+FeedName+"_audit_"+trigger_key+ " AS SELECT DISTINCT Load_TS,FileName,total_RecordCount,RecordCount_FN,cnt_InvalidMSISDns,cnt_SchemaMismatch FROM df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,Audit Data details for cfw.DL_File_Details Table
dflogsaudit = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TA          = dflogsaudit.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if TA == ['N']:
  for i in range(1,5,1):
    try:
      bflag = 0
      spark.sql('''MERGE INTO cfw.DL_File_Details AS TARGET 
      USING (
      select a.*
            ,CONCAT('{"Invalid MSISDNs":"',cnt_InvalidMSISDNs,'","Schema Mismatch":"',cnt_SchemaMismatch,'"}') as error_reason from df_audit_'''+FeedName+''' a) SOURCE 
      on trim(SOURCE.FILENAME)=trim(TARGET.FILENAME) 
      and TARGET.File_load_status != 'D' 
      and TARGET.Load_TS = TO_TIMESTAMP("'''+ windowStartTime+'''") 
      and TARGET.TRIGGER_KEY= int("'''+trigger_key+'''")
      WHEN MATCHED  THEN UPDATE SET 
      TARGET.File_load_status='Y'
     ,TARGET.Total_Records=SOURCE.total_RecordCount
     ,TARGET.valid_records=SOURCE.RecordCount_FN
     ,TARGET.error_records=SOURCE.cnt_InvalidMSISDNs + SOURCE.cnt_SchemaMismatch
     ,TARGET.error_reason= SOURCE.error_reason ''')
                        
      bflag = 1
      
      #logging
      if bflag == 1:
        spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Audit  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
        print("Data inserted into cfw.DL_File_Details and Flag_Audit updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" ")     
      break;
    except Exception as e11:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e11):
        time.sleep(180)
        continue
      else:
        raise Exception(e11)
  
  ##IF Merge could not happen successfully
  if bflag == 0:
      raise Exception("Retry!")
else:
  print("Data exists in cfw.DL_File_Details table for "+FeedName +" feed for Load_TS = "+windowStartTime+" and source_container = "+source_container+" and Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TA))

# COMMAND ----------

# DBTITLE 1,Rejected and Empty Files Logging
db = "stgods_dl"
FeedName              = FeedName
feedaudit             = FeedName+"_audit_"+trigger_key
foldername            = ""     
tempaudit_rejected    = FeedName+"_rejected_"+trigger_key
file_location_reject  = "/mnt/"+target_container+"/"+FeedName+"/reject/"+source_container+"/"+dt+"/"+dateId
file_location_working = file_location+filedirectory

#Rejected Files Flow
dflogsreject = spark.sql("SELECT DISTINCT Flag_reject FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND trigger_key='"+trigger_key+"' ")
TR          = dflogsreject.select("Flag_reject").distinct().rdd.flatMap(lambda x: x).collect()
if TR  == ['N']:
  dbutils.notebook.run("/Shared/DWH_Transformation/DATALAKE/Generic/RejectedFiles", 43200,{"windowStartTime": windowStartTime, "PipelineRunId":PipelineRunId,"DataFactoryName":DataFactoryName,"PipelineName":PipelineName,"trigger_key":trigger_key,"db":db,"feedaudit":feedaudit,"foldername":foldername,"file_location":file_location,"file_location_reject":file_location_reject,"file_location_working":file_location_working,"tempaudit_rejected":tempaudit_rejected,"FeedName":FeedName,"Source_Container":source_container})
  spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Reject  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
  print("Data Moved to rejected folder for rejected files and cfw.DL_File_Details and alert control table ")
else:
  print("Data exists in stgods_dl.dl_file_details table for Load_TS = "+windowStartTime+" and Flag_Error = "+str(TR))

# COMMAND ----------

# DBTITLE 1,Update Notebook Status
for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_NotebookStatus  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Flag_Temp = 'Y' AND Flag_Datalake = 'Y' AND Flag_StgODS = 'Y' AND Flag_Error = 'Y' AND Flag_Audit = 'Y' AND Flag_reject = 'Y' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      break;
    except Exception as e14:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e14):
        time.sleep(30)
        continue
      else:
        raise Exception(e14)

# COMMAND ----------

# DBTITLE 1,Exit the notebook 
dflogs_final = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND source_container = '"+source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
NFS =dflogs_final.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
print(NFS)
if NFS == ['Y']:
  spark.sql(" DROP TABLE IF EXISTS stgods_dl."+table_name+" ")
  dbutils.fs.rm(path,recurse=True)
  dbutils.notebook.exit("Successful")
/Shared/ShakeIT_WonReward
# MAGIC %md
# MAGIC ###### Purpose: Notebook for ODS Digicel App Daily Full Load Table
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |March 20,2021|Divya Verma | Created the notebook to insert data into Digicel App table | 5 Mins |  
# MAGIC |Dec 30,2021|Parv Agarwal | Created the notebook to insert data into Digicel App table | 5 Mins |  

# COMMAND ----------

# DBTITLE 1,Refresh Table
# MAGIC %sql
# MAGIC REFRESH TABLE ods_dl.Digicel_App;

# COMMAND ----------

# DBTITLE 1,Variables from adf Pipeline
dbutils.widgets.text("TableName", "")
dbutils.widgets.text("snapshot_dt", "")
table_name = dbutils.widgets.get("table_name")
snapshot_dt = dbutils.widgets.get("snapshot_dt")

# COMMAND ----------

# DBTITLE 1,Temp table creation
spark.sql(''' WITH CTE_PRIM (select distinct ID,NAME,Last_Updated_By,int(HUB_ID),snapshot_dt,REPLACE(snapshot_dt,'-','') as DATE_CODE FROM stgods_dl.'''+table_name+''' WHERE corruptrecord IS NULL OR corruptrecord == ' ')
                  select * from CTE_PRIM
                  WHERE snapshot_dt = to_date("'''+snapshot_dt+'''") ''').createOrReplaceTempView("ODS_AAP")

# COMMAND ----------

# DBTITLE 1,Merge Data in ODS Digicel_App Table
spark.sql('''MERGE INTO  ods_dl.Digicel_App AS target
             USING ( 
                select cast(ID as int) as Digicel_App_Key 
                       ,NAME as Digicel_App_Name
                       ,HUB_ID
                       ,current_timestamp() AS Insert_TS
                       ,to_timestamp(null) AS Update_TS
                       ,Last_Updated_By
                       ,DATE_CODE
                from ODS_AAP) AS source
             ON source.Digicel_App_Key = target.Digicel_App_Key 
             WHEN MATCHED THEN
             UPDATE SET  target.Digicel_App_Name = source.Digicel_App_Name
                        ,target.Last_Updated_By  = source.Last_Updated_By
                        ,target.HUB_ID           = source.HUB_ID
                        ,target.Update_TS        = current_timestamp()
                        ,target.DATE_CODE  = source.DATE_CODE
             WHEN NOT MATCHED THEN
             INSERT * 
         ''')

# COMMAND ----------

# DBTITLE 1,Exit
dbutils.notebook.exit("Successful")
/Shared/ODS_Digicel_App
# MAGIC %md
# MAGIC ###### Purpose: Notebook for ODS Digicel App User Daily Full Load Table
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |MAR 20, 2021|Divya Verma | Created the notebook to insert data into Digicel App User table | 10 Mins |
# MAGIC |JUL  01, 2021 |Divya Verma| Subcriber Key Derivation from ods_dl.Prim_rsrc_value_keys | 10 mins |
# MAGIC |FEB 02, 2022 |Divya Verma| Pre-Load Procedure to fit Subscriber Key | 10 mins |

# COMMAND ----------

# DBTITLE 1,Refresh Tables
# MAGIC %sql
# MAGIC REFRESH TABLE ods_dl.Digicel_App_User;
# MAGIC REFRESH TABLE ods_dl.Prim_rsrc_value_keys;

# COMMAND ----------

# DBTITLE 1,Variables from adf
dbutils.widgets.text("TableName", "")
dbutils.widgets.text("snapshot_dt", "")
TableName = dbutils.widgets.get("TableName")
snapshot_dt = dbutils.widgets.get("snapshot_dt")

# COMMAND ----------

# DBTITLE 1,Supporting Tables
spark.sql('''WITH CTE_PRIM (select distinct Primary_Resource_value,Subscriber_Key, ROW_NUMBER() OVER (PARTITION BY Primary_Resource_value ORDER BY Subscriber_Key DESC) rnk_prim FROM ods_dl.Prim_rsrc_value_keys where UPPER(Current_Flag) = "Y" AND UPPER(source_system) != 'FTTH')
SELECT distinct * FROM CTE_PRIM WHERE rnk_prim = 1''').createOrReplaceTempView("ODS_SubscriberKeys_vw")
spark.sql('''SELECT DISTINCT * FROM ods_dl.Digicel_App_User  where Subscriber_Key  like '-1|%' or Subscriber_Key is null or Subscriber_Key in (1,-1) ''').createOrReplaceTempView("ODS_AAPUser_Delta")

# COMMAND ----------

# DBTITLE 1,Pre-Load Procedure to fit Subscriber Keys
for i in range(1,5,1):
  try:
    spark.sql('''
    MERGE INTO ods_dl.Digicel_App_User AS tgt 
          USING( WITH CTE_fitting (
                 SELECT DISTINCT *, coalesce(TRIM(b.Subscriber_Key),concat('-1|',IFNULL(MSISDN,-1))) AS Subscriber_Key_Fitted,
                 ROW_NUMBER() OVER (PARTITION BY Digicel_App_User_Key ORDER BY Last_Login_Date DESC) rnk_keys
                 FROM ODS_AAPUser_Delta as a
                 LEFT JOIN ODS_SubscriberKeys_vw as b
                 ON TRIM(a.MSISDN) = b.Primary_Resource_value) 
                 SELECT DISTINCT * FROM CTE_fitting WHERE rnk_keys = 1) AS src 
    ON  tgt.Digicel_App_User_Key = src.Digicel_App_User_Key
    and tgt.HUB_ID = src.HUB_ID
    WHEN MATCHED THEN UPDATE SET 
    tgt.Subscriber_Key = src.Subscriber_Key_Fitted
   ,tgt.update_ts      = current_timestamp() 
    ''')
    print("ods_dl.Digicel_App_User fitted sussessfully")
    break;
  except Exception as e:
    if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
      time.sleep(120)
      continue
    else:
      raise Exception(e)

# COMMAND ----------

# DBTITLE 1,Subscriber_Key Derivation from ods_dl.Prim_rsrc_value_keys
spark.sql(''' WITH CTE_PRIM (select *,row_number() over (partition by primary_resource_value,activation_date ORDER BY IFNULL(Disconnection_Date, TO_DATE('9999-12-31')) desc) rnk from ods_dl.subscriber where Source_System_Id <> 'FTTH' and primary_resource_value is not null ),
                  cte_distinct_prim (SELECT * FROM CTE_PRIM WHERE rnk = 1)
                  select a.*,IFNULL(b.Subscriber_Key,concat('-1|',TRIM(a.MSISDN))) AS Subscriber_Key 
                  FROM stgods_dl.'''+TableName+''' as a 
                  LEFT JOIN cte_distinct_prim as b 
                  ON TRIM(a.MSISDN) = TRIM(b.Primary_Resource_value) 
                  WHERE snapshot_dt = to_date("'''+snapshot_dt+'''") ''').createOrReplaceTempView("ODS_AAPUser")

# COMMAND ----------

# DBTITLE 1,Merge into ODS Digicel_App_User 
spark.sql('''MERGE INTO ods_dl.Digicel_App_User AS target
             USING(       
             WITH CTE_User (
             select  DISTINCT cast (ID as int)    as Digicel_App_User_Key
                     ,IFNULL(Subscriber_Key,'-1') AS Subscriber_Key 
                     ,MSISDN
                     ,DATE_OF_BIRTH     as Date_of_Birth 
                     ,ENABLED           as Enabled_Flag 
                     ,COUNTRY_CODE      as Country_Code 
                     ,GENDER            as Gender
                     ,COUNTRY_TERRITORY as Country_Territory 
                     ,LAST_LOGIN_DATE   as Last_Login_Date
                     ,LANGUAGE          as LANGUAGE	
                     ,GUID
                     ,NVL(HUB_ID,-1)          AS HUB_ID
                     ,cast(MARKET_ID as int)  as Market_Key
                     ,cast(ACTIVE as int)     as Active
                     ,cast(DISABLE_ID as int) as Disable_Id
                     ,DISABLE_REASON          as Disable_Reason
                     ,current_timestamp()     AS Insert_TS
                     ,null                    AS Update_TS
                     ,Last_Updated_By
                     ,ROW_NUMBER() OVER (partition by NVL(cast (ID as int),0) order by Last_Login_Date DESC )rnk_user
                     from ODS_AAPUser)
              SELECT DISTINCT Digicel_App_User_Key
                             ,Subscriber_Key 
                             ,MSISDN
                             ,Date_of_Birth 
                             ,Enabled_Flag 
                             ,Country_Code 
                             ,Gender
                             ,Country_Territory 
                             ,Last_Login_Date
                             ,LANGUAGE	
                             ,GUID
                             ,HUB_ID
                             ,Market_Key
                             ,Active
                             ,Disable_Id
                             ,Disable_Reason
                             ,Insert_TS
                             ,Update_TS
                             ,Last_Updated_By FROM CTE_USER WHERE rnk_user = 1
              ) AS source
              ON source.Digicel_App_User_Key= target.Digicel_App_User_Key
              WHEN MATCHED THEN 
              UPDATE SET target.Subscriber_Key    = source.Subscriber_Key 
                        ,target.MSISDN            = source.MSISDN 
                        ,target.Date_of_Birth     = source.Date_of_Birth 
                        ,target.Enabled_Flag      = source.Enabled_Flag 
                        ,target.Country_Code      = source.Country_Code     
                        ,target.Gender            = source.Gender
                        ,target.Country_Territory = source.Country_Territory
                        ,target.Last_Login_Date   = source.Last_Login_Date
                        ,target.LANGUAGE          = source.LANGUAGE
                        ,target.GUID	          = source.GUID
                        ,target.HUB_ID	          = source.HUB_ID
                        ,target.Market_Key        = source.Market_Key
                        ,target.Active            = source.Active 
                        ,target.Disable_Id        = source.Disable_Id 
                        ,target.Disable_Reason    = source.Disable_Reason
                        ,target.Update_TS         = current_timestamp() 
                        ,target.Last_Updated_By   = source.Last_Updated_By
              WHEN NOT MATCHED THEN
              INSERT * ''') 

# COMMAND ----------

# DBTITLE 1,Exit
dbutils.notebook.exit("Successful")
/Shared/ODS_Digicel_App_User
# MAGIC %md
# MAGIC ###### Purpose: Notebook to process ODS data of MDA Selfcase Usage
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |   Execution Time   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |Feb 20, 2021|Divya Verma| Created the notebook to process ODS data of MDA Selfcase Usage| 5 min |
# MAGIC |Apr 22, 2021|Divya Verma| Concatenated key Logic : App_Usage_session_key| 5 min |
# MAGIC |July 01, 2021|Divya Verma| Subcriber Key Derivation from ods_dl.Prim_rsrc_value_keys | 5 min |
# MAGIC |Dec 21, 2021|Divya Verma| Insert status into Audit table i.e. cfw.table_load_status | 5 min |

# COMMAND ----------

# DBTITLE 1,Refresh Tables
# MAGIC %sql
# MAGIC REFRESH TABLE ods_dl.Prim_rsrc_value_keys;
# MAGIC REFRESH TABLE ods_dl.Digicel_App_Usage_Event;
# MAGIC REFRESH TABLE stgods_dl.ODS_Selfcare_Usage;

# COMMAND ----------

# DBTITLE 1,Variable from adf pipeline
dbutils.widgets.removeAll()
dbutils.widgets.text("triggerTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("trigger_key", "")
triggerTime     = dbutils.widgets.get("triggerTime")
PipelineRunId   = dbutils.widgets.get("PipelineRunId")
trigger_key     = dbutils.widgets.get("trigger_key")

# COMMAND ----------



# COMMAND ----------

# DBTITLE 1, Subscriber_Key Derivation from ods_dl.Prim_rsrc_value_keys
spark.sql(''' WITH CTE_PRIM (select *,row_number() over (partition by primary_resource_value,activation_date ORDER BY IFNULL(Disconnection_Date, TO_DATE('9999-12-31')) desc) rnk from ods_dl.subscriber where Source_System_Id <> 'FTTH' and primary_resource_value is not null ),
                  cte_distinct_prim (SELECT * FROM CTE_PRIM WHERE rnk = 1)
                  select a.*,IFNULL(b.Subscriber_Key,'-1') AS Subscriber_Key 
                  FROM stgods_dl.ODS_Selfcare_Usage as a 
                  LEFT JOIN cte_distinct_prim as b 
                  ON TRIM(a.MSISDN) = TRIM(b.Primary_Resource_value) ''').createOrReplaceTempView("ODS_Selfcare_Usage")

# COMMAND ----------

# DBTITLE 1,Selfcare ODS Incremental Daily Load
spark.sql(" DELETE FROM ods_dl.Digicel_App_Usage_Event where TriggerTime = '"+triggerTime+"' ")
spark.sql('''
WITH CTE_Selfcare( SELECT 
         count(*) as Number_of_Events	
        ,IFNULL(Subscriber_Key,'-1') AS Subscriber_Key
        ,CASE WHEN DATETIMESTAMP IS NULL OR DATETIMESTAMP = "" OR DATETIMESTAMP = "null" THEN "1950/01/01" ELSE DATETIMESTAMP END AS Login_timestamp 
        ,CASE WHEN USER_AGENT IS NULL OR USER_AGENT = "" OR USER_AGENT = "null" THEN -1 ELSE USER_AGENT END AS User_Agent 
        ,CASE WHEN IP_REQUESTER IS NULL OR IP_REQUESTER = "" OR IP_REQUESTER = "null" THEN -1 ELSE IP_REQUESTER END AS Requester_IP 
        ,CASE WHEN SELFCARE_METHOD IS NULL OR SELFCARE_METHOD = "" OR SELFCARE_METHOD = "null" THEN -1 ELSE SELFCARE_METHOD END AS Selfcare_Method 
        ,CASE WHEN cast (CLIENT_ID as bigint ) IS NULL OR cast (CLIENT_ID as bigint ) = "" THEN -1 ELSE cast (CLIENT_ID as bigint) END as Digicel_App_Key 
        ,CASE WHEN cast (USERID as bigint) IS NULL OR cast (USERID as bigint) = "" THEN -1 ELSE cast (USERID as bigint) END as Digicel_App_User_Key 
        ,CASE WHEN SELFCARE_METHOD_ALIAS IS NULL OR SELFCARE_METHOD_ALIAS = "" THEN -1 ELSE SELFCARE_METHOD_ALIAS END as Selfcare_Method_Alias 
        ,CASE WHEN cast(EXECUTION_TIME as int) IS NULL OR cast(EXECUTION_TIME as int) = "" THEN -1 ELSE cast(EXECUTION_TIME as int) END as Execution_Time	
        ,MSISDN	
        ,CASE WHEN cast(ANDROID_VERSION as int) IS NULL OR cast(ANDROID_VERSION as int)= "" THEN -1 ELSE cast(ANDROID_VERSION as int) END as Android_Version 
        ,CASE WHEN LANGUAGE IS NULL OR LANGUAGE = "" OR LANGUAGE = "null" THEN -1 ELSE LANGUAGE END as Language 
        ,CASE WHEN SOURCE IS NULL OR SOURCE = "" OR SOURCE = "null" THEN -1 ELSE SOURCE END as Source	
        ,CASE WHEN AUTHENTICATION_TYPE IS NULL OR AUTHENTICATION_TYPE = "" OR AUTHENTICATION_TYPE = "null" THEN -1 ELSE AUTHENTICATION_TYPE END as Authentication_Type 
        ,CASE WHEN ENTERPRISE_USER IS NULL OR ENTERPRISE_USER = "" OR ENTERPRISE_USER = "null" THEN -1 ELSE ENTERPRISE_USER END as Enterprise_Username 
        ,CASE WHEN RESULT IS NULL OR RESULT = "" OR RESULT = "null" THEN -1 ELSE RESULT END as Result 
        ,CASE WHEN METHOD_PARAMETERS IS NULL OR METHOD_PARAMETERS = "" OR METHOD_PARAMETERS = "null" THEN -1 ELSE METHOD_PARAMETERS END as Method_Parameters
        ,int(NVL(HUB_ID,-1)) AS HUB_ID
        ,int(MARKET_ID) as Market_Key
from ODS_Selfcare_Usage WHERE Load_TS  < "'''+triggerTime+'''"
group by 
DATETIMESTAMP
,Subscriber_Key
,USER_AGENT
,IP_REQUESTER 
,SELFCARE_METHOD
,CLIENT_ID
,USERID
,SELFCARE_METHOD_ALIAS
,EXECUTION_TIME
,MSISDN	
,ANDROID_VERSION
,LANGUAGE
,SOURCE 
,AUTHENTICATION_TYPE 
,ENTERPRISE_USER 
,RESULT
,METHOD_PARAMETERS
,HUB_ID
,MARKET_ID) 
INSERT INTO ods_dl.Digicel_App_Usage_Event
select concat(Login_timestamp,'_',User_Agent,'_',Requester_IP,'_',Selfcare_Method,'_',Digicel_App_Key,'_',Digicel_App_User_Key,'_',Selfcare_Method_Alias,'_',Execution_Time,'_',MSISDN,'_',Android_Version,'_',Language,'_',Source,'_',Authentication_Type,'_',Enterprise_Username,'_',Result,'_',Method_Parameters,'_',Market_Key) AS App_Usage_Session_Key, a.*,current_timestamp(),current_timestamp(), "BAU" AS Last_Updated_By,"'''+triggerTime+'''" AS TriggerTime,REPLACE(substring(current_timestamp(),1,10),'-','') as Data_dt FROM CTE_Selfcare AS a
''')

# COMMAND ----------

# DBTITLE 1,cfw.table_load_status 
for i in range(1,5,1):
      try:
        spark.sql("DELETE FROM cfw.Table_load_status where FeedName = 'ODS_SELFCARE_USAGE' AND  Load_TS = TO_TIMESTAMP('"+triggerTime+"') AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e8:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
          time.sleep(30)
          continue
        else:
          raise Exception(e8)
          
##INSERT INTO cfw.Table_load_status 
spark.sql('''
INSERT INTO cfw.Table_load_status 
SELECT int("'''+str(trigger_key)+'''") as trigger_key
       ,TO_TIMESTAMP("'''+triggerTime+'''") as Load_TS
       ,snapshot_dt
       ,hub_id
       ,upper('ODS_SELFCARE_USAGE') as FeedName
       ,null as source_system
       ,'ods_dl' as target_db
       ,'Digicel_App_Usage_Event' as target_table_name
       ,'Y' as load_status
       ,count(*) as total_records
       ,count(FileName) as total_files
       , "'''+ str(PipelineRunId)+'''" as pipeline_run_id
       ,current_timestamp() as insert_ts
       ,null as update_ts 
       FROM stgods_dl.ODS_Selfcare_Usage where Load_TS < "'''+triggerTime+'''" group by Hub_id,snapshot_dt ''')

# COMMAND ----------

# DBTITLE 1,Clean up and Exit 
spark.sql(" DELETE FROM stgods_dl.ODS_Selfcare_Usage where Load_TS <'"+triggerTime+"' ") 
dbutils.notebook.exit("Successful")
/Shared/ODS_Selfcare_Incremental
-- Databricks notebook source
-- MAGIC %md
-- MAGIC ###### Purpose: Notebook to process ODS of ShakeIT Attempt
-- MAGIC ###### Revision History:
-- MAGIC | Date      |    Author     |   Description   |   Execution Time   |
-- MAGIC |-----------|:-------------:|----------------:|
-- MAGIC |Mar 03, 2021|Shagufa Akhzar| Created the notebook to process ODS of ShakeIT Attempt| 2 min |  
-- MAGIC |May 17, 2021|Shagufa Akhzar| Updated the notebook to include ShakeIt_Attempt_Key(ID_MarketKey) and updated merge script| 2 min |  
-- MAGIC |Jul 01, 2021 |Divya Verma| Subcriber Key Derivation from ods_dl.Prim_rsrc_value_keys | 2 min |
-- MAGIC |Jan 03, 2022 |           | Added Auditing changes | 2 min |

-- COMMAND ----------

-- DBTITLE 1,Create widgets to extract parameters from ADF
-- MAGIC %python
-- MAGIC dbutils.widgets.removeAll()
-- MAGIC dbutils.widgets.text("Trigger_key", "")
-- MAGIC dbutils.widgets.text("PipelineRunId","")
-- MAGIC dbutils.widgets.text("windowStartTime", "")
-- MAGIC dbutils.widgets.text("hub_id", "")
-- MAGIC PipelineRunId         = dbutils.widgets.get("PipelineRunId")
-- MAGIC trigger_key           = dbutils.widgets.get("Trigger_key")
-- MAGIC hub_id                = dbutils.widgets.get("hub_id")
-- MAGIC Extract_Start_Date    = dbutils.widgets.get("windowStartTime")
-- MAGIC load_ts = str(Extract_Start_Date)[:19].replace(' ','T')
-- MAGIC print(load_ts)
-- MAGIC print(trigger_key)
-- MAGIC print(hub_id)
-- MAGIC print(Extract_Start_Date)

-- COMMAND ----------

-- DBTITLE 1,Refresh staging tables
REFRESH TABLE stgods_dl.ODS_ShakeIt_Attempt;
REFRESH TABLE ods_dl.Prim_rsrc_value_keys

-- COMMAND ----------

-- DBTITLE 1,Drop and create stgShakeItAttempt_tmp table
-- MAGIC %python
-- MAGIC path = "dbfs:/user/hive/warehouse/ods_dl/stgshakeitattempt_tmp_"+trigger_key
-- MAGIC dbutils.fs.rm(path,True)
-- MAGIC spark.sql('''DROP TABLE IF EXISTS ods_dl.stgShakeItAttempt_tmp_'''+trigger_key)
-- MAGIC spark.sql('''CREATE TABLE ods_dl.stgShakeItAttempt_tmp_'''+trigger_key+'''
-- MAGIC 	WITH CTE_stgods_attempt AS (
-- MAGIC 			SELECT *
-- MAGIC 			FROM stgods_dl.ODS_ShakeIt_Attempt 
-- MAGIC 			where snapshot_dt = to_date("'''+Extract_Start_Date+'''")
-- MAGIC             and hub_id = int('''+hub_id+''')
-- MAGIC 			)
-- MAGIC 		,CTE_PRIM AS (
-- MAGIC 			SELECT DISTINCT TRIM(A.Primary_Resource_value) AS Primary_Resource_value
-- MAGIC 				,TRIM(A.Subscriber_Key) AS Subscriber_Key
-- MAGIC 				,ROW_NUMBER() OVER (
-- MAGIC 					PARTITION BY TRIM(A.Primary_Resource_value) ORDER BY IFNULL(TO_DATE(TRIM(A.Activation_date), 'yyyyMMdd'),TRIM(A.Subscriber_Key)) DESC
-- MAGIC 					) AS Row_Num
-- MAGIC 			FROM ods_dl.prim_rsrc_value_keys A
-- MAGIC 			WHERE UPPER(TRIM(A.Current_Flag)) = 'Y'
-- MAGIC 			)
-- MAGIC         ,CTE_Distinct_Prim AS (
-- MAGIC         SELECT * FROM CTE_PRIM WHERE Row_Num = 1
-- MAGIC         ) 
-- MAGIC SELECT A.*
-- MAGIC 	,IFNULL(B.Subscriber_Key, '-1') AS Subscriber_Key
-- MAGIC FROM CTE_stgods_attempt AS A
-- MAGIC LEFT JOIN CTE_Distinct_Prim AS B ON A.MSISDN = B.Primary_Resource_value
-- MAGIC ''')
-- MAGIC spark.sql("SELECT DISTINCT LOAD_TS FROM ods_dl.stgShakeItAttempt_tmp_"+trigger_key).display()

-- COMMAND ----------

-- DBTITLE 1,Drop and create ShakeIt_Attempt_tmp table
-- MAGIC %python
-- MAGIC path = "dbfs:/user/hive/warehouse/ods_dl/shakeit_attempt_tmp_"+trigger_key
-- MAGIC dbutils.fs.rm(path,True)
-- MAGIC spark.sql("DROP TABLE IF EXISTS ods_dl.ShakeIt_Attempt_tmp_"+trigger_key)
-- MAGIC spark.sql('''CREATE TABLE ods_dl.ShakeIt_Attempt_tmp_'''+trigger_key+'''(
-- MAGIC WITH CTE_Shakeit
-- MAGIC AS (
-- MAGIC 	SELECT CONCAT(ID,'_',MARKET_ID) AS ShakeIt_Attempt_Key
-- MAGIC         ,CAST(ID AS INT) AS ShakeIt_Attempt_Id
-- MAGIC 		,Subscriber_Key
-- MAGIC 		,CAST(REWARD_ID AS INT) AS Reward_Key
-- MAGIC 		,MSISDN
-- MAGIC 		,CAST(INSERT_DATE AS TIMESTAMP) AS Insert_Date
-- MAGIC 		,CAST(ELIGIBLE_PLAN_ID AS INT) AS Eligible_Plan_Id
-- MAGIC 		,CAST(WON_REWARD_ID AS INT) AS Won_Reward_Id
-- MAGIC 		,RESULT AS Result_Flag
-- MAGIC 		,RESULT_REASON AS Result_Reason
-- MAGIC 		,MARKET_ID AS Market_Key
-- MAGIC 		,Last_Updated_By
-- MAGIC         ,snapshot_dt
-- MAGIC 	FROM ods_dl.stgShakeItAttempt_tmp_'''+trigger_key+'''
-- MAGIC 	)
-- MAGIC 	,CTE_RowNum
-- MAGIC AS (
-- MAGIC 	SELECT ROW_NUMBER() OVER (
-- MAGIC 			PARTITION BY ShakeIt_Attempt_Key ORDER BY IFNULL(Insert_Date, ShakeIt_Attempt_Key) DESC
-- MAGIC 			) AS RowID
-- MAGIC 		,ShakeIt_Attempt_Key
-- MAGIC         ,ShakeIt_Attempt_Id
-- MAGIC 		,Subscriber_Key
-- MAGIC 		,Reward_Key
-- MAGIC 		,MSISDN
-- MAGIC 		,Insert_Date
-- MAGIC 		,Eligible_Plan_Id
-- MAGIC 		,Won_Reward_Id
-- MAGIC 		,Result_Flag
-- MAGIC 		,Result_Reason
-- MAGIC 		,Market_Key
-- MAGIC 		,Last_Updated_By
-- MAGIC         ,snapshot_dt
-- MAGIC 	FROM CTE_Shakeit
-- MAGIC 	)
-- MAGIC 	SELECT DISTINCT ShakeIt_Attempt_Key
-- MAGIC 		,ShakeIt_Attempt_Id
-- MAGIC         ,Subscriber_Key
-- MAGIC 		,Reward_Key
-- MAGIC 		,MSISDN
-- MAGIC 		,Insert_Date
-- MAGIC 		,Eligible_Plan_Id
-- MAGIC 		,Won_Reward_Id
-- MAGIC 		,Result_Flag
-- MAGIC 		,Result_Reason
-- MAGIC 		,Market_Key
-- MAGIC 		,Last_Updated_By
-- MAGIC         ,'''"'"+load_ts+"'"''' AS TriggerTime
-- MAGIC         ,cast("'''+hub_id+'''" as Int) as Hub_id    
-- MAGIC         ,snapshot_dt
-- MAGIC 	FROM CTE_RowNum
-- MAGIC 	WHERE RowID = 1
-- MAGIC 	)''')

-- COMMAND ----------

-- MAGIC %python
-- MAGIC count = spark.sql("select count(*) from ods_dl.ShakeIt_Attempt_tmp_"+trigger_key).collect()[0][0]
-- MAGIC print(count)

-- COMMAND ----------

-- DBTITLE 1,Merge(upsert) into ShakeIt_Attempt
-- MAGIC %python
-- MAGIC spark.sql('''
-- MAGIC MERGE INTO ods_dl.ShakeIt_Attempt AS TARGET
-- MAGIC USING (
-- MAGIC 	SELECT ShakeIt_Attempt_Key
-- MAGIC 		,ShakeIt_Attempt_Id
-- MAGIC 		,Subscriber_Key
-- MAGIC 		,Reward_Key
-- MAGIC 		,MSISDN
-- MAGIC 		,Insert_Date
-- MAGIC 		,Eligible_Plan_Id
-- MAGIC 		,Won_Reward_Id
-- MAGIC 		,Result_Flag
-- MAGIC 		,Result_Reason
-- MAGIC 		,Market_Key
-- MAGIC 		,CURRENT_TIMESTAMP() AS Insert_TS
-- MAGIC 		,CAST(NULL AS TIMESTAMP) AS Update_TS
-- MAGIC 		,Last_Updated_By
-- MAGIC 		,TriggerTime
-- MAGIC         ,HUB_ID
-- MAGIC 	FROM ods_dl.ShakeIt_Attempt_tmp_'''+trigger_key+'''
-- MAGIC 	) AS SOURCE
-- MAGIC 	ON SOURCE.ShakeIt_Attempt_Key = TARGET.ShakeIt_Attempt_Key
-- MAGIC     AND SOURCE.HUB_ID = TARGET.HUB_ID
-- MAGIC WHEN MATCHED
-- MAGIC 	THEN
-- MAGIC 		UPDATE
-- MAGIC 		SET TARGET.ShakeIt_Attempt_Key = SOURCE.ShakeIt_Attempt_Key
-- MAGIC 			,TARGET.ShakeIt_Attempt_Id = SOURCE.ShakeIt_Attempt_Id
-- MAGIC 			,TARGET.Subscriber_Key = SOURCE.Subscriber_Key
-- MAGIC 			,TARGET.Reward_Key = SOURCE.Reward_Key
-- MAGIC 			,TARGET.MSISDN = SOURCE.MSISDN
-- MAGIC 			,TARGET.Insert_Date = SOURCE.Insert_Date
-- MAGIC 			,TARGET.Eligible_Plan_Id = SOURCE.Eligible_Plan_Id
-- MAGIC 			,TARGET.Won_Reward_Id = SOURCE.Won_Reward_Id
-- MAGIC 			,TARGET.Result_Flag = SOURCE.Result_Flag
-- MAGIC 			,TARGET.Result_Reason = SOURCE.Result_Reason
-- MAGIC 			,TARGET.Market_Key = SOURCE.Market_Key
-- MAGIC 			,TARGET.Update_TS = CURRENT_TIMESTAMP()
-- MAGIC 			,TARGET.Last_Updated_By = SOURCE.Last_Updated_By
-- MAGIC             ,TARGET.HUB_ID = SOURCE.HUB_ID
-- MAGIC WHEN NOT MATCHED
-- MAGIC 	THEN
-- MAGIC 		INSERT (
-- MAGIC 			ShakeIt_Attempt_Key
-- MAGIC 			,ShakeIt_Attempt_Id
-- MAGIC 			,Subscriber_Key
-- MAGIC 			,Reward_Key
-- MAGIC 			,MSISDN
-- MAGIC 			,Insert_Date
-- MAGIC 			,Eligible_Plan_Id
-- MAGIC 			,Won_Reward_Id
-- MAGIC 			,Result_Flag
-- MAGIC 			,Result_Reason
-- MAGIC 			,Market_Key
-- MAGIC 			,Insert_TS
-- MAGIC 			,Update_TS
-- MAGIC 			,Last_Updated_By
-- MAGIC 			,TriggerTime
-- MAGIC             ,HUB_ID
-- MAGIC 			)
-- MAGIC 		VALUES (
-- MAGIC 			SOURCE.ShakeIt_Attempt_Key
-- MAGIC 			,SOURCE.ShakeIt_Attempt_Id
-- MAGIC 			,SOURCE.Subscriber_Key
-- MAGIC 			,SOURCE.Reward_Key
-- MAGIC 			,SOURCE.MSISDN
-- MAGIC 			,SOURCE.Insert_Date
-- MAGIC 			,SOURCE.Eligible_Plan_Id
-- MAGIC 			,SOURCE.Won_Reward_Id
-- MAGIC 			,SOURCE.Result_Flag
-- MAGIC 			,SOURCE.Result_Reason
-- MAGIC 			,SOURCE.Market_Key
-- MAGIC 			,SOURCE.Insert_TS
-- MAGIC 			,SOURCE.Update_TS
-- MAGIC 			,SOURCE.Last_Updated_By
-- MAGIC 			,SOURCE.TriggerTime
-- MAGIC             ,SOURCE.HUB_ID
-- MAGIC 			)
-- MAGIC ''')

-- COMMAND ----------

-- MAGIC %python
-- MAGIC for i in range(1,5,1):
-- MAGIC       try:
-- MAGIC         spark.sql("DELETE FROM cfw.Table_load_status where FeedName = 'SHAKEIT_WON_REWARD' AND  Load_TS = TO_TIMESTAMP('"+load_ts+"') AND Trigger_Key = int('"+trigger_key+"') ")
-- MAGIC         break;
-- MAGIC       except Exception as e8:
-- MAGIC         if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
-- MAGIC           time.sleep(30)
-- MAGIC           continue
-- MAGIC         else:
-- MAGIC           raise Exception(e8)
-- MAGIC           
-- MAGIC ##INSERT INTO cfw.Table_load_status 
-- MAGIC spark.sql('''
-- MAGIC INSERT INTO cfw.Table_load_status 
-- MAGIC SELECT int("'''+trigger_key+'''") as trigger_key
-- MAGIC        ,TO_TIMESTAMP("'''+load_ts+'''") as Load_TS
-- MAGIC        ,snapshot_dt
-- MAGIC        ,hub_id
-- MAGIC        ,upper('ODS_SHAKEIT_ATTEMPT') as FeedName
-- MAGIC        ,null as source_system
-- MAGIC        ,'ods_dl' as target_db
-- MAGIC        ,'ShakeIt_Attempt' as target_table_name
-- MAGIC        ,'Y' as load_status
-- MAGIC        ,'''+str(count)+''' as total_records
-- MAGIC        ,1 as total_files
-- MAGIC        , "'''+ str(PipelineRunId)+'''" as pipeline_run_id
-- MAGIC        ,current_timestamp() as insert_ts
-- MAGIC        ,null as update_ts 
-- MAGIC        FROM ods_dl.ShakeIt_Attempt_tmp_'''+trigger_key+''' group by Hub_id,snapshot_dt ''')

-- COMMAND ----------

-- MAGIC %python
-- MAGIC trgfile="/mnt/datafactorytemp/JobTrigger/"+trigger_key+"_ods_dl_shakeit_attempt.trg"
-- MAGIC print(trgfile)
-- MAGIC dbutils.fs.rm(trgfile,recurse=True)

-- COMMAND ----------

-- DBTITLE 1,Drop temp tables
-- MAGIC %python
-- MAGIC spark.sql("DROP TABLE IF EXISTS ods_dl.stgShakeItAttempt_tmp_"+trigger_key)
-- MAGIC spark.sql("DROP TABLE IF EXISTS ods_dl.ShakeIt_Attempt_tmp_"+trigger_key)
/Shared/ODS_ShakeITAttempt
# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC     
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

# COMMAND ----------

from pyspark.sql.functions import col
# File location and type
file_location = "/FileStore/tables/circuits-3.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

# spark.read\
#     .option("header","true")\
#     .option("inferschema","true")\
#     .csv("/FileStore/tables/circuits-2.csv")\
#     .write\
#     .mode('overwrite')\
#     .csv("/FileStore/tables/ind-2.csv")
    
df=spark.read\
    .option("header","true")\
    .option("inferschema","true")\
    .csv("/FileStore/tables/circuits-2.csv")

#df.printSchema()

df=df.withColumn("CXX", col("lat"))
df.createOrReplaceTempView("Data")
df1=spark.sql("select *from Data")
df1.write.mode("overwrite").csv("mycsv.csv")
/Shared/Data-Delivery/EDW/NB_CargaTbContratoVarejo
# MAGIC %md 
# MAGIC #Querying the event log
# MAGIC
# MAGIC A Delta Live Tables event log is created and maintained for every pipeline and serves as the single source of truth for all information related to the pipeline, including audit logs, data quality checks, pipeline progress, and data lineage. The event log is exposed via the `/events` API and is displayed in the Delta Live Tables UI. The event log is also stored as a delta table and can be easily accessed in a Databricks Notebook to perform more complex analyis. This notebook demonstrates simple examples of how to query and extract useful data from the event log.
# MAGIC
# MAGIC This notebook requires Databricks Runtime 8.1 or above to access the JSON SQL operators that are used in some queries.

# COMMAND ----------

# DBTITLE 1,Create the storage location text box
# MAGIC %fs
# MAGIC
# MAGIC ls
# MAGIC
# MAGIC # Creates the input box at the top of the notebook.
# MAGIC #dbutils.widgets.text('storage', 'dbfs:/pipelines/production-data', 'Storage Location')

# COMMAND ----------

# DBTITLE 1,Event log view
# MAGIC %md
# MAGIC The examples in this notebook use a view named `event_log_raw` to simplify queries against the event log. To create the `event_log_raw` view:
# MAGIC
# MAGIC 1. Enter the path to the event log in the **Storage Location** text box. To find the path, click **Settings** on the **Pipeline details** page for your pipeline to display the **Edit pipeline settings** dialog. The path is the value in the **Storage location** field. The path is also available in the JSON settings. Click the **JSON** button on the **Edit pipeline settings** dialog and look for the `storage` field.
# MAGIC 2. Run the following commands to create the `event_log_raw` view.

# COMMAND ----------

# DBTITLE 1,Create the event log view
# Replace the text in the Storage Location input box to the desired pipeline storage location. This can be found in the pipeline configuration under 'storage'.
storage_location = dbutils.widgets.get('storage')
event_log_path = storage_location + "/system/events"

# Read the event log into a temporary view so it's easier to query.
event_log = spark.read.format('delta').load(event_log_path)
event_log.createOrReplaceTempView("event_log_raw")

# COMMAND ----------

# DBTITLE 1,Event log schema
# MAGIC %md
# MAGIC The following is the top level schema for the event log:
# MAGIC
# MAGIC | Field       | Description |
# MAGIC | ----------- | ----------- |
# MAGIC | id          | The ID of the pipeline. |
# MAGIC | sequence    | JSON containing metadata for the sequencing of events. This can be used to understand dependencies and event order. |
# MAGIC | origin      | JSON containing metadata for the origin of the event, including the cloud, region, user ID, pipeline ID, notebook name, table names, and flow names. |
# MAGIC | timestamp   | Timestamp at which the event was recorded. |
# MAGIC | message     | A human readable message describing the event. This message is displayed in the event log component in the main DLT UI. |
# MAGIC | level       | The severity level of the message. |
# MAGIC | error       | If applicable, an error stack trace associated with the event. |
# MAGIC | details     | JSON containing structured details of the event. This is the primary field for creating analyses with event log data. |
# MAGIC | event_type  | The event type. |

# COMMAND ----------

# DBTITLE 1,View a sample of event log records
# MAGIC %sql
# MAGIC SELECT * FROM event_log_raw LIMIT 100

# COMMAND ----------

# DBTITLE 1,Audit Logging
# MAGIC %md
# MAGIC A common and important use case for data pipelines is to create an audit log of actions users have performed. The events containing information about user actions have the event type `user_action`. The following is an example to query the timestamp, user action type, and the user name of the person taking the action with the following:

# COMMAND ----------

# DBTITLE 1,Example query for user events auditing
# MAGIC %sql
# MAGIC SELECT timestamp, details:user_action:action, details:user_action:user_name FROM event_log_raw WHERE event_type = 'user_action'

# COMMAND ----------

# DBTITLE 1,Pipeline update details
# MAGIC %md
# MAGIC Each instance of a pipeline run is called an *update*. The following examples extract information for the most recent update, representing the latest iteration of the pipeline.

# COMMAND ----------

# DBTITLE 1,Get the ID of the most recent pipeline update
# Save the most recent update ID as a Spark configuration setting so it can used in queries.
latest_update_id = spark.sql("SELECT origin.update_id FROM event_log_raw WHERE event_type = 'create_update' ORDER BY timestamp DESC LIMIT 1").collect()[0].update_id
spark.conf.set('latest_update.id', latest_update_id)

# COMMAND ----------

# DBTITLE 1,Lineage
# MAGIC %md
# MAGIC Lineage is exposed in the UI as a graph. You can use a query to extract this information to generate reports for compliance or to track data dependencies across an organization. The information related to lineage is stored in the `flow_definition` events and contains the necessary information to infer the relationships between different datasets:

# COMMAND ----------

# DBTITLE 1,Example query for pipeline lineage
# MAGIC %sql
# MAGIC SELECT details:flow_definition.output_dataset, details:flow_definition.input_datasets FROM event_log_raw WHERE event_type = 'flow_definition' AND origin.update_id = '${latest_update.id}'

# COMMAND ----------

# DBTITLE 1,Data quality
# MAGIC %md
# MAGIC The event log maintains metrics related to data quality. Information related to the data quality checks defined with Delta Live Tables expectations is stored in the `flow_progress` events. The following query extracts the number of passing and failing records for each data quality rule defined for each dataset:
# MAGIC

# COMMAND ----------

# DBTITLE 1,Example query for data quality
# MAGIC %sql
# MAGIC SELECT
# MAGIC   row_expectations.dataset as dataset,
# MAGIC   row_expectations.name as expectation,
# MAGIC   SUM(row_expectations.passed_records) as passing_records,
# MAGIC   SUM(row_expectations.failed_records) as failing_records
# MAGIC FROM
# MAGIC   (
# MAGIC     SELECT
# MAGIC       explode(
# MAGIC         from_json(
# MAGIC           details :flow_progress :data_quality :expectations,
# MAGIC           "array<struct<name: string, dataset: string, passed_records: int, failed_records: int>>"
# MAGIC         )
# MAGIC       ) row_expectations
# MAGIC     FROM
# MAGIC       event_log_raw
# MAGIC     WHERE
# MAGIC       event_type = 'flow_progress'
# MAGIC       AND origin.update_id = '${latest_update.id}'
# MAGIC   )
# MAGIC GROUP BY
# MAGIC   row_expectations.dataset,
# MAGIC   row_expectations.name
/Shared/Querying the Delta Live Tables event log
# MAGIC %md
# MAGIC ###### Purpose: Notebook for EDR Feed Raw Data Processing
# MAGIC ###### Revision History:
# MAGIC | Date      |    Author     |   Description   |
# MAGIC |-----------|:-------------:|----------------:|
# MAGIC |JUL 18, 2022|Shyam Prasad| Created the notebook to insert data into datalake and ods staging tables|

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC     
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

# MAGIC %scala
# MAGIC import scala.util.parsing.json.JSON
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC import za.co.absa.spline.agent.AgentConfig
# MAGIC import za.co.absa.spline.harvester.postprocessing.AbstractPostProcessingFilter
# MAGIC import za.co.absa.spline.harvester.postprocessing.PostProcessingFilter
# MAGIC import org.apache.commons.configuration.Configuration
# MAGIC import za.co.absa.spline.harvester.conf.StandardSplineConfigurationStack
# MAGIC import za.co.absa.spline.harvester.HarvestingContext
# MAGIC import za.co.absa.spline.producer.model.ExecutionPlan
# MAGIC import za.co.absa.spline.producer.model.ExecutionEvent
# MAGIC import za.co.absa.spline.producer.model.ReadOperation
# MAGIC import za.co.absa.spline.producer.model.WriteOperation
# MAGIC import za.co.absa.spline.producer.model.DataOperation
# MAGIC import za.co.absa.spline.harvester.ExtraMetadataImplicits._
# MAGIC import za.co.absa.spline.harvester.SparkLineageInitializer._
# MAGIC
# MAGIC val notebookInformationJson = dbutils.notebook.getContext.toJson
# MAGIC val outerMap = JSON.parseFull(notebookInformationJson).getOrElse(0).asInstanceOf[Map[String,String]]
# MAGIC val tagMap = outerMap("tags").asInstanceOf[Map[String,String]]
# MAGIC val extraContextMap = outerMap("extraContext").asInstanceOf[Map[String,String]]
# MAGIC val notebookPath = extraContextMap("notebook_path").split("/")
# MAGIC val workspaceUrl=tagMap("browserHostName")
# MAGIC     
# MAGIC val workspaceName=dbutils.notebook().getContext().notebookPath.get
# MAGIC val notebookURL = tagMap("browserHostName")+"/?o="+tagMap("orgId")+tagMap("browserHash")
# MAGIC val user = tagMap("user")
# MAGIC val name = notebookPath(notebookPath.size-1)
# MAGIC val notebookInfo = Map("notebookURL" -> notebookURL,
# MAGIC "user" -> user,
# MAGIC "workspaceName" ->workspaceName,
# MAGIC "workspaceUrl" -> workspaceUrl,                       
# MAGIC "name" -> name,
# MAGIC "mounts" -> dbutils.fs.ls("/FileStore/tables").map(_.path),
# MAGIC "timestamp" -> System.currentTimeMillis)
# MAGIC val notebookInfoJson = scala.util.parsing.json.JSONObject(notebookInfo)
# MAGIC
# MAGIC
# MAGIC class CustomFilter extends PostProcessingFilter {
# MAGIC   def this(conf: Configuration) = this()
# MAGIC
# MAGIC   override def processExecutionEvent(event: ExecutionEvent, ctx: HarvestingContext): ExecutionEvent =
# MAGIC     event.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processExecutionPlan(plan: ExecutionPlan, ctx: HarvestingContext ): ExecutionPlan =
# MAGIC     plan.withAddedExtra(Map( "notebookInfo" -> notebookInfoJson))
# MAGIC
# MAGIC   override def processReadOperation(op: ReadOperation, ctx: HarvestingContext ): ReadOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processWriteOperation(op: WriteOperation, ctx: HarvestingContext): WriteOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC
# MAGIC   override def processDataOperation(op: DataOperation, ctx: HarvestingContext  ): DataOperation =
# MAGIC     op.withAddedExtra(Map("foo" -> "bar"))
# MAGIC }
# MAGIC
# MAGIC
# MAGIC
# MAGIC val myInstance = new CustomFilter()
# MAGIC
# MAGIC
# MAGIC spark.enableLineageTracking(
# MAGIC   AgentConfig.builder()
# MAGIC     .postProcessingFilter(myInstance)
# MAGIC     .build()
# MAGIC )

# COMMAND ----------

sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)


# COMMAND ----------

# DBTITLE 1,Get Variables from adf pipeline

dbutils.widgets.removeAll()
dbutils.widgets.text("DataFactoryName", "")
dbutils.widgets.text("PipelineName", "")
dbutils.widgets.text("trigger_key", "")
dbutils.widgets.text("FeedName", "")
dbutils.widgets.text("windowStartTime", "")
dbutils.widgets.text("PipelineRunId", "")
dbutils.widgets.text("Source_container", "")
dbutils.widgets.text("Target_container", "")
dbutils.widgets.text("File_delimiter", "")
dbutils.widgets.text("File_Extension", "")
dbutils.widgets.text("hub_id", "")
PipelineName          = dbutils.widgets.get("PipelineName")
DataFactoryName       = dbutils.widgets.get("DataFactoryName")
trigger_key           = str(dbutils.widgets.get("trigger_key"))
FeedName              = dbutils.widgets.get("FeedName")
windowStartTime       = dbutils.widgets.get("windowStartTime")
PipelineRunId         = dbutils.widgets.get("PipelineRunId")
Source_container      = dbutils.widgets.get("Source_container")
Target_container      = dbutils.widgets.get("Target_container")
File_delimiter        = dbutils.widgets.get("File_delimiter")
File_Extension        = dbutils.widgets.get("File_Extension")
hub_id                = dbutils.widgets.get("hub_id")

# COMMAND ----------

# DBTITLE 1,Optimization
# MAGIC
# MAGIC %sql
# MAGIC set spark.sql.shuffle.partitions = 400;
# MAGIC set spark.sql.files.maxPartitionBytes=1073741824;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;
# MAGIC set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;

# COMMAND ----------

# DBTITLE 1,Refresh Dimension Tables
# MAGIC
# MAGIC %sql
# MAGIC REFRESH TABLE dgcdimensions_dl.ContainerName;
# MAGIC REFRESH TABLE dgcdimensions_dl.AreaCode;
# MAGIC REFRESH TABLE stgods_dl.BulkSMS_JAM_logs;

# COMMAND ----------

# DBTITLE 1,Import Required Packages

import time
import os
import json
import pyspark
import requests
import pyspark.sql.functions as F
import pyspark.sql.types as T
from datetime import datetime
from pyspark.sql.types import * 
from pyspark.sql import SparkSession 
from pyspark.sql.functions import input_file_name
from pyspark.sql.functions import lit,col,lower,regexp_replace,split,when,concat
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,TimestampType
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")

# COMMAND ----------

# DBTITLE 1,Create Path variable to fetch file from temp adls working

dt = windowStartTime[:10].replace('-','')
dateId = windowStartTime.replace('-','').replace(':','')
file_location = "/FileStore/shared_uploads"
duplicate_file_location = "/mnt/"+Target_container+"/"+FeedName+"/duplicate/"+Source_container+"/"+dt+"/"+dateId
table_name ='stg_BulkSMS_JAM_'+trigger_key+'_'+Source_container+'_'+dateId 
table_name = table_name.replace('-','')
path = "/FileStore/shared_uploads"+dateId
log_table = "BulkSMS_JAM_logs"
print(table_name)
print(file_location)

# COMMAND ----------

# DBTITLE 1,Check Notebook status for current Load timestamp

NotebookStatus = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
NS_BulkSMS_JAM = NotebookStatus.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
if NS_BulkSMS_JAM == ['Y']:
  dbutils.notebook.exit( FeedName+" Notebook Status is already successful for Load_TS = "+windowStartTime+" and Source_container = "+Source_container+" and  Trigger_Key = "+trigger_key+" and Flag_NotebookStatus = "+str(NS_BulkSMS_JAM))
else:
  print("Execute Further Notebook Commands")

# COMMAND ----------

# DBTITLE 1,Duplicate File Function Check
#%run /Shared/DWH_Transformation/TOOLS/CheckDuplicateFile

# COMMAND ----------

# DBTITLE 1,Duplicate File Check

#filedirectory="/*/*/*/lz/*"
filedirectory="/*"
dflogsduplicate = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
DP          = dflogsduplicate.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if DP != ['Y']:  
    print('Hey')
 #dfdffileinfo=checkDuplicateFile(File_Location=str(file_location),File_Directory=str(filedirectory),FeedName=str(FeedName),Trigger_Key=str(trigger_key),WindowStartTime=str(windowStartTime),PipelineRunID=str(PipelineRunId),Source_Container=Source_container,Target_Container=('Target_container'))

# COMMAND ----------

# DBTITLE 1,Schema Enforcement


nofilefound=False
schemaBulkSMS_JAM = StructType([ \
StructField("TIMESTAMP",StringType(),True),\
StructField("MESSAGE_ID",StringType(),True),\
StructField("SYSTEM_ID",StringType(),True),\
StructField("IP_ADDRESS",StringType(),True),\
StructField("SOURCE_ADDRESS",StringType(),True),\
StructField("MSISDN",StringType(),True),\
StructField("IMSI",StringType(),True),\
StructField("MSC",StringType(),True),\
StructField("RESULT",StringType(),True),\
StructField("ERRORCODE",StringType(),True),\
StructField("ERRORCAUSE",StringType(),True),\
StructField("FILLER_A_registered",StringType(),True),\
StructField("FILLER_B_errorhandlertable",StringType(),True),\
StructField("corruptrecord",StringType(),True) \
])

spark.sql("set spark.sql.files.ignoreCorruptFiles=true")

try:
  BulkSMS_JAM_with_schema = spark.read.option("header", "false")\
  .schema(schemaBulkSMS_JAM)\
  .option("sep", File_delimiter)\
  .option("quote", "\"")\
  .option("multiline","true")\
  .option("encoding", "US-ASCII")\
  .csv(file_location+filedirectory)

  BulkSMS_JAM_with_schemaAllhubs=      BulkSMS_JAM_with_schema.withColumn("Load_TS",lit(windowStartTime)).withColumn("FileName",input_file_name()).\
  withColumn("Last_Updated_By",lit("BAU")).withColumn("Data_dt",lit(dt)).\
  withColumn("Trigger_Key",lit(trigger_key))
  BulkSMS_JAM_with_schemaAllhubs.createOrReplaceTempView("temp_BulkSMS_JAM")
  Row_count = BulkSMS_JAM_with_schemaAllhubs.count()
  print(Row_count)
except Exception as e:
  print(file_location+filedirectory)
  print(e)
  nofilefound=True

# COMMAND ----------

# MAGIC
# MAGIC %sql
# MAGIC select * from temp_BulkSMS_JAM

# COMMAND ----------

# DBTITLE 1,If File Not Found
# if nofilefound is True:
#   dbutils.notebook.exit("No Files Found")

# COMMAND ----------

# DBTITLE 1,Snapshot_Date Helper
#%run /Shared/DWH_Transformation/TOOLS/Func_Snapshot_Date

# COMMAND ----------

# DBTITLE 1,Area_Code Helper

dfcode=spark.sql('''SELECT MARKET_ID, CAST(MARKET_SH_CODE1 AS STRING) AS MARKET_SH_CODE1, LINE_OF_BUSINESS FROM dgcdimensions_dl.areacode 
where UPPER(LINE_OF_BUSINESS)='GSM' ''')
dfcode.createOrReplaceTempView("temp_AreaCode")

# COMMAND ----------

# DBTITLE 1,Market_Code Helper

dfHUBID=spark.sql("SELECT DISTINCT Market_Key AS MARKET_ID,HUB_ID FROM ods_dl.market")
dfHUBID.createOrReplaceTempView("temp_MarketID")

# COMMAND ----------

# DBTITLE 1,Data Preparation


BulkSMS_JAM = spark.sql('''
SELECT
CASE WHEN TRIM(b.MARKET_ID) IS NULL THEN -1 ELSE TRIM(b.MARKET_ID) END AS MARKET_ID
,TRIM(a.TIMESTAMP) as TIMESTAMP
,TRIM(a.MESSAGE_ID)		AS	MESSAGE_ID
,TRIM(a.SYSTEM_ID) 				AS	SYSTEM_ID
,TRIM(a.IP_ADDRESS)				AS	IP_ADDRESS
,TRIM(a.SOURCE_ADDRESS)			AS	SOURCE_ADDRESS
,TRIM(a.MSISDN)			AS	MSISDN
,TRIM(a.IMSI)		AS	IMSI
,TRIM(a.MSC)						AS	MSC
,TRIM(a.RESULT)						AS	RESULT
,TRIM(a.ERRORCODE)						AS	ERRORCODE
,TRIM(a.ERRORCAUSE)				AS	ERRORCAUSE
,TRIM(a.FILLER_A_registered)						AS	FILLER_A_registered
,TRIM(a.FILLER_B_errorhandlertable)			AS	FILLER_B_errorhandlertable
,TRIM(a.corruptrecord) as CORRUPTRECORD,
       load_ts,
       substring_index(a.FileName,'/',-1) AS FileName,
       last_updated_by,
       data_dt,
       trigger_key,
       d.hub_id,
      CASE 
			WHEN (LENGTH(TRIM(a.MSISDN)) BETWEEN 10
					AND 12
                AND TRIM(a.MSISDN) RLIKE '^[0-9]*$'
                AND (TRIM(a.MSISDN) IS NOT NULL) )        
				THEN 1
			ELSE 0
			END AS IsValid_MSISDN
       ,CASE WHEN c.Container_Name IS NULL THEN "group" ELSE c.Container_Name END AS Container_Name
       from temp_BulkSMS_JAM a
LEFT JOIN temp_AreaCode AS b ON SUBSTRING(TRIM(a.MSISDN), 1, 5) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 4) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 3) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 2) = b.MARKET_SH_CODE1
	OR SUBSTRING(TRIM(a.MSISDN), 1, 1) = b.MARKET_SH_CODE1
LEFT JOIN dgcdimensions_dl.ContainerName c ON b.MARKET_ID = TRIM(c.MARKET_ID)
LEFT JOIN temp_MarketId d ON b.MARKET_ID = TRIM(d.MARKET_ID)
''')

BulkSMS_JAM_allhubs_final = BulkSMS_JAM.withColumn("snapshot_dt",col("FileName")).withColumn("Load_TS",lit(windowStartTime)).\
withColumn("Last_Updated_By",lit("BAU")).withColumn("Data_dt",lit(dt)).withColumn("trigger_key",lit(trigger_key))
BulkSMS_JAM_allhubs_final.createOrReplaceTempView("temp_BulkSMS_JAM_allhubs_final")

# COMMAND ----------

# MAGIC
# MAGIC %sql 
# MAGIC select * from temp_BulkSMS_JAM_allhubs_final

# COMMAND ----------

# DBTITLE 1,Temp Table Data Load
# %python
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogstemp = spark.sql("SELECT DISTINCT Flag_Temp FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TF         = dflogstemp.select("Flag_Temp").distinct().rdd.flatMap(lambda x: x).collect()
if spark._jsparkSession.catalog().tableExists('stgods_dl', table_name) and TF == ['Y']:
  print("stgods_dl."+table_name+" table exists")
else:
  spark.sql("CREATE OR REPLACE TABLE stgods_dl."+table_name+"  USING DELTA LOCATION '"+path+"' PARTITIONED BY (Container_Name) AS SELECT * FROM temp_BulkSMS_JAM_allhubs_final")
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      break;
    except Exception as e2:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e2)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e2)):
        time.sleep(30)
        continue
      else:
        raise Exception(e2)
   
  print(log_table)
  ##INSERT INTO LOG TABLE 
  spark.sql("INSERT INTO stgods_dl."+log_table+" VALUES ('"+windowStartTime+"','"+dt+"','"+Source_container+"',int('"+trigger_key+"'),'N','N','N','N','N','N','N') ")
  
  ##Update Logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Temp = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl."+table_name+" and Flag_Temp updated to Y in stgods_dl."+log_table+" for  Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e3:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e3):
        time.sleep(30)
        continue
      else:
        raise Exception(e3)

# COMMAND ----------

# DBTITLE 1,For Datalake Layer Processing
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogsDL = spark.sql("SELECT DISTINCT Flag_DataLake FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
TDL      = dflogsDL.select("Flag_DataLake").distinct().rdd.flatMap(lambda x: x).collect()
if TDL == ['N']:
  MK= BulkSMS_JAM_allhubs_final.select("Container_Name").distinct().rdd.flatMap(lambda x: x).collect()
  print(MK)
  for t in MK :
    
    ##DELETE
    for i in range(1,5,1):
      try:
        #spark.sql("truncate table "+t+"_dl.BulkSMS_JAM")
        spark.sql(" DELETE FROM "+t+"_dl.BulkSMS_JAM where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        break;
      except Exception as e4:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e4)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e4)):
          time.sleep(30)
          continue
        else:
          raise Exception(e4)
   
    ##INSERT INTO COUNTRY WISE DELTA TABLE
    spark.sql(" INSERT INTO TABLE "+t+"_dl.BulkSMS_JAM select TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord,int(MARKET_ID),Load_TS,FileName,Last_Updated_By,Data_dt,int(Trigger_Key),int(HUB_ID),IsValid_MSISDN,snapshot_dt FROM stgods_dl."+table_name+" WHERE CORRUPTRECORD is null and IsValid_MSISDN in (1,2) and Container_Name = '"+t+"'")
  
  ##Update logging flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Datalake = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into BulkSMS_JAM Data Lake tables and Flag_Datalake updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+"  and Source_Container = "+Source_container+" and  Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      time.sleep(30)
      continue
else:
  print("Data in BulkSMS_JAM Data Lake tables exists for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Datalake = "+str(TDL))

# COMMAND ----------

# MAGIC
# MAGIC %sql
# MAGIC select * from jamaica_dl.BulkSMS_JAM

# COMMAND ----------

# DBTITLE 1,Invalid Records Segregation and alertitled
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

Schema_Mismatch_cnt = spark.sql('''select * from stgods_dl.'''+table_name+''' Where IsValid_MSISDN = 0  
                                   AND ((corruptrecord IS NOT NULL AND corruptrecord <> " " AND corruptrecord <> "")
                                   OR  ((corruptrecord IS NULL OR corruptrecord = " " OR corruptrecord = "")
                                   AND ((TRIM(MSISDN) NOT RLIKE '^[0-9]*$' AND TRIM(MSISDN) IS NOT NULL 
                                         AND TRIM(MSISDN) <> "" AND TRIM(MSISDN) <> " " ))))''').count()

Invalid_MSISDNs_cnt = spark.sql("select * from stgods_dl."+table_name+" Where IsValid_MSISDN = 0 AND (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ') AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL OR TRIM(MSISDN) <> ' '  OR TRIM(MSISDN) <> '') ").count()

dflogserror = spark.sql("SELECT DISTINCT Flag_Error FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TE          = dflogserror.select("Flag_Error").distinct().rdd.flatMap(lambda x: x).collect()
                                
if TE == ['N']:
  if Schema_Mismatch_cnt!=0 or Invalid_MSISDNs_cnt !=0:
    ##DELETE
    for i in range(1,5,1):
      try:
        spark.sql(" DELETE FROM error_dl.reject_BulkSMS_JAM where Error_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"') ")
        spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND Trigger_Key = int('"+trigger_key+"') AND Alert_Type = 'Invalid_Data' ")
        break;
      except Exception as e9:
        if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e9)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e9)):
          time.sleep(30)
          continue
        else:
          raise Exception(e9)
        
    ##INSERT INTO ERROR and Alert control Table  
    spark.sql('''INSERT INTO TABLE error_dl.reject_BulkSMS_JAM SELECT TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord ,MARKET_ID ,Load_TS ,FileName ,Last_Updated_By ,Data_dt as Error_dt,Trigger_Key,IsValid_MSISDN ,HUB_ID, snapshot_dt
                                           ,CASE WHEN IsValid_MSISDN = 0
                                           AND (corruptrecord IS NULL OR corruptrecord = "" OR corruptrecord = " ") 
                                           AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL 
                                                 OR TRIM(MSISDN) <> " "  OR TRIM(MSISDN) <> "")
                                           THEN 'Invalid_MSISDNs' 
                                           ELSE 'Schema_Mismatch' END AS Error_Type FROM stgods_dl.'''+table_name+''' as a where IsValid_MSISDN = 0 ''')
		
		
			
    
    data_error = '{"title": "' +FeedName+ ' Invalid Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Erroneous Records Moved To {dgc{xxx}datalake01}","error":{"Message":"Invalid Records","TableName":"error_dl.reject_BulkSMS_JAM","Path":"/error/BulkSMS_JAM/reject_BulkSMS_JAM","RowsWritten":{"Invalid MSISDNs":"' +str(Invalid_MSISDNs_cnt)+ '","Schema Mismatch": "' +str(Schema_Mismatch_cnt)+ '","Total Invalid Records": "' +str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+ '"}}}'
    
    spark.sql('''
     INSERT INTO cfw.etl_alert_control 
     select DISTINCT int("'''+str(trigger_key)+'''") AS trigger_key
                     ,"'''+str(FeedName)+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+str(windowStartTime)+'''") AS Load_TS
                     ,"Invalid_Data" as Alert_Type
                     ,'"'''+str(data_error)+'''"' AS Alert_Reason
                     ,"'''+str(Source_container)+'''" as Data_Storage
                     ,"error_dl.reject_BulkSMS_JAM" as Data_location
                     ,int("'''+str(Invalid_MSISDNs_cnt+Schema_Mismatch_cnt)+'''") as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID  ''')
    
  #Update logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Error  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'")
      print("Data inserted into error_dl.reject_BulkSMS_JAM and Flag_Error updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(180)
        continue
      else:
        raise Exception(e)

else:
  print("Data exists in error_dl.reject_BulkSMS_JAM table for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TE))

# COMMAND ----------

# DBTITLE 1,For ODS Layer Processing
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs = spark.sql("SELECT DISTINCT Flag_StgODS FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
TT=dflogs.select("Flag_StgODS").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['N']:
  ##DELETE
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM stgods_dl.stgods_BulkSMS_JAM where Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Trigger_Key = int('"+trigger_key+"')")
      break;
    except Exception as e6:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e6)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e6)):
        time.sleep(30)
        continue
      else:
        raise Exception(e6)
   
  ##INSERT INTO stgods_dl.stgods_BulkSMS_JAM
  spark.sql("INSERT INTO TABLE stgods_dl.stgods_BulkSMS_JAM select TIMESTAMP,MESSAGE_ID,SYSTEM_ID,IP_ADDRESS,SOURCE_ADDRESS,CASE WHEN MSISDN not like '5%' THEN '5'||substr(MSISDN,3,2)||substr(MSISDN,2,10) ELSE MSISDN  END as MSISDN,IMSI,MSC,RESULT,ERRORCODE,ERRORCAUSE,FILLER_A_registered,FILLER_B_errorhandlertable,corruptrecord,MARKET_ID,Load_TS,FileName,Last_Updated_By,Data_dt,Trigger_Key,HUB_ID, IsValid_MSISDN,snapshot_dt FROM temp_BulkSMS_JAM_allhubs_final ")

            
  ##Update Logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_StgODS  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
      print("Data inserted into stgods_dl.stgods_BulkSMS_JAM and Flag_StgODS updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e) 
else:
  print("Data exists in stgods_dl.stgods_BulkSMS_JAM table for Load_TS = "+windowStartTime+" AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'and  Flag_StgODS = "+str(TT))

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from stgods_dl.stgods_BulkSMS_JAM 

# COMMAND ----------

# DBTITLE 1,cfw.Table_load_status INSERT
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs = spark.sql("SELECT DISTINCT Flag_Stgods FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"'  AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TT=dflogs.select("Flag_Stgods").distinct().rdd.flatMap(lambda x: x).collect()
if TT == ['Y']:
  for i in range(1,5,1):
    try:
      spark.sql("DELETE FROM cfw.Table_load_status WHERE Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key='"+trigger_key+"'  AND Hub_id='"+hub_id+"' ")
      break;
    except Exception as e8:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e8)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e8)):
        time.sleep(30)
        continue
      else:
        raise Exception(e8)
## INSERT INTO cfw.Table_load_status 
  spark.sql('''
  INSERT INTO cfw.Table_load_status 
  SELECT DISTINCT CAST(Trigger_Key as int) AS Trigger_Key
                 ,CAST(Load_TS AS TIMESTAMP) AS Load_TS
                 ,snapshot_dt
                 ,CAST(hub_id as int) AS HUB_ID
                 ,"'''+FeedName +'''" AS FeedName
                 ,"BulkSMS" AS Source_System
                 ,"stgods_dl" as target_db
                 ,"stgods_BulkSMS_JAM" as target_table_name
                 ,'Y' as load_status
                 ,count(*) as total_records
                 ,count(distinct FileName) as total_files
                 ,"'''+PipelineRunId+'''" AS Pipeline_Run_Id
                 ,current_timestamp()
                 ,null FROM stgods_dl.'''+table_name+''' group by snapshot_dt,hub_id,Trigger_Key,Load_TS ''')
  print("Data inserted into cfw.Table_load_status")

# COMMAND ----------

# MAGIC
# MAGIC %sql
# MAGIC select * from cfw.table_load_status where FeedName = 'BulkSMS_JAM'

# COMMAND ----------

# DBTITLE 1,Audit Data Collection
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

df_audit = spark.sql(''' SELECT LOAD_TS
		                       ,FileName
                               ,count(*) AS total_RecordCount
		                       ,SUM(CASE 
                                        WHEN FileName IS NOT NULL AND IsValid_MSISDN in (1,2)
					                    THEN 1
				                    ELSE 0
				                    END) AS RecordCount_FN
		                       ,SUM(
                                    CASE
                                    WHEN IsValid_MSISDN  = 0 
                                    and (corruptrecord IS NULL OR corruptrecord = '' OR corruptrecord = ' ') 
                                   AND (TRIM(MSISDN) NOT RLIKE '^[0-9]*$' OR TRIM(MSISDN) IS NOT NULL OR TRIM(MSISDN) <> ' ' 
                                    OR TRIM(MSISDN) <> '')
                                    THEN 1 
                                  
                                  ELSE 0
                                  END ) AS cnt_InvalidMSISDNs
                              ,SUM(CASE 
		                      		     WHEN IsValid_MSISDN in (1,2)
					                     AND ((corruptrecord IS NOT NULL AND corruptrecord <> " " AND corruptrecord <> ""))
					                     THEN 1
				                    ELSE 0
				                    END) AS cnt_SchemaMismatch
	                     FROM stgods_dl.'''+table_name+'''
	                     GROUP BY FileName,LOAD_TS ''')
df_audit.createOrReplaceTempView("df_audit_"+FeedName)

# COMMAND ----------

# DBTITLE 1,For Rejected Files Audit
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

spark.sql("DROP TABLE IF EXISTS stgods_dl.BulkSMS_JAM_audit_"+trigger_key)
spark.sql("CREATE TABLE stgods_dl.BulkSMS_JAM_audit_"+trigger_key+" AS SELECT DISTINCT Load_TS,FileName AS FileName ,total_RecordCount,RecordCount_FN,cnt_InvalidMSISDNs,cnt_SchemaMismatch FROM df_audit_BulkSMS_JAM ")

# COMMAND ----------

# DBTITLE 1,cfw.DL_File_Details Table :Valid and Non-Empty Files Audit Data 
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

spark.sql("set spark.databricks.delta.checkLatestSchemaOnRead=FALSE")
dflogsaudit = spark.sql("SELECT DISTINCT Flag_Audit FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TA          = dflogsaudit.select("Flag_Audit").distinct().rdd.flatMap(lambda x: x).collect()
if TA == ['N']:
  for i in range(1,5,1):
    try:
      spark.sql('''MERGE INTO cfw.DL_File_Details AS TARGET 
      USING (
      select a.*
            ,CONCAT('{"Invalid MESSAGEIDs":"',cnt_InvalidMSISDNs,'","Schema Mismatch":"',cnt_SchemaMismatch,'"}') as error_reason from df_audit_BulkSMS_JAM a) SOURCE 
      on trim(SOURCE.FileName)=trim(TARGET.FileName) 
      and TARGET.File_load_status != 'D' 
      and TARGET.Load_TS = "'''+ windowStartTime+'''" 
      and TARGET.TRIGGER_KEY="'''+trigger_key+'''" 
      WHEN MATCHED  THEN UPDATE SET 
      TARGET.File_load_status='Y'
     ,TARGET.Total_Records=SOURCE.total_RecordCount
     ,TARGET.valid_records=SOURCE.RecordCount_FN
     ,TARGET.error_records=SOURCE.cnt_InvalidMSISDNs + SOURCE.cnt_SchemaMismatch
     ,TARGET.error_reason= SOURCE.error_reason ''')
      
      #logging
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Audit  = 'Y',Flag_reject = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"'  ")
      print("Data inserted into cfw.DL_File_Details and Flag_Audit updated to Y in stgods_dl."+log_table+" for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e)  
else:
  print("Data exists in cfw.DL_File_Details table for Load_TS = "+windowStartTime+"  AND Source_Container = "+Source_container+" AND Trigger_Key = '"+trigger_key+"' and  Flag_Error = "+str(TA))

# COMMAND ----------

# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

db = "stgods_dl"
#FeedName             = "BulkSMS_JAM"
feedaudit             = "BulkSMS_JAM_audit_"+trigger_key
foldername            = "BulkSMS_JAM/"            
tempaudit_rejected    = "BulkSMS_JAM_rejected_"+trigger_key
file_location_reject  = "/mnt/"+Target_container+"/"+FeedName+"/reject/"+Source_container+"/"+dt+"/"+dateId
file_location_working = file_location+filedirectory

#Rejected Files Flow
dflogsreject = spark.sql("SELECT DISTINCT Flag_reject FROM stgods_dl."+log_table+" WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = '"+trigger_key+"' ")
TR           = dflogsreject.select("Flag_reject").distinct().rdd.flatMap(lambda x: x).collect()
if TR  == ['N']:
  dbutils.notebook.run("/Shared/DWH_Transformation/DATALAKE/Generic/RejectedFiles", 43200,{"windowStartTime": windowStartTime, "PipelineRunId": PipelineRunId, "DataFactoryName":DataFactoryName,"PipelineName":PipelineName,"trigger_key":trigger_key,"db":db,"feedaudit":feedaudit,"foldername":foldername,"file_location":file_location,"file_location_reject":file_location_reject,"file_location_working":file_location_working,"tempaudit_rejected":tempaudit_rejected,"FeedName":FeedName,"Source_container":Source_container})
  
  ##Update Logging Flag
  for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_Reject  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_Container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      print("Data Moved to rejected folder for rejected files and to cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+"  and Source_Container = "+Source_container+" and  Trigger_Key = "+trigger_key+" ")
      break;
    except Exception as e:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e):
        time.sleep(30)
        continue
      else:
        raise Exception(e)
else:
  print("Data exists in cfw.DL_File_Details and alert control table for Load_TS = "+windowStartTime+" AND Source_Container = "+Source_container+" AND Trigger_Key = "+trigger_key+" and Flag_Error = "+str(TR))

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from cfw.dl_file_details where feedname = 'BulkSMS_JAM'

# COMMAND ----------

# DBTITLE 1,Alert details capture for non true duplicate rows for composite PK
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dfPK_Alert = spark.sql(''' WITH CTE_PK(SELECT TIMESTAMP,MESSAGE_ID,SYSTEM_ID,count(*) FROM stgods_dl.'''+table_name+''' group by TIMESTAMP,MESSAGE_ID,SYSTEM_ID HAVING count(*)>1) SELECT * FROM CTE_PK ''').count()
print(dfPK_Alert)

if dfPK_Alert != 0:
  data_PK = '{"title": "' +FeedName+ ' Duplicate Records Found !!!","color": "red","DataFactoryName":"' +DataFactoryName+ '","PipelineName":"' +PipelineName+ '","PipelineRunID":"' +PipelineRunId+ '","time":"' +windowStartTime+ '","FileName":"' +FeedName+' Duplicate Records Found After Primary Key Validation","error":{"Message":"Duplicate Records","TableName":"stgods_dl.ods_'+FeedName.lower()+',"Requirement":"One record per {CDRID} column/s must be present in '+FeedName+'","Total Duplicate Records":"' +str(dfPK_Alert)+ '"}}'  
  
  for i in range(1,5,1):
    try:
      spark.sql(" DELETE FROM cfw.etl_alert_control where FeedName = '"+FeedName+"' AND Load_TS = TO_TIMESTAMP('"+windowStartTime+"') AND trigger_key = int('"+trigger_key+"') AND Alert_Type = 'Duplicate_Records' ")
      break;
    except Exception as e10:
      if ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteReadException' in str(e10)) or ('com.databricks.sql.transaction.tahoe.ConcurrentDeleteDeleteException' in str(e10)):
        time.sleep(30)
        continue
      else:
        raise Exception(e10)
        
  ##INSERT INTO cfw.etl_alert_control 
  spark.sql('''
  INSERT INTO cfw.etl_alert_control 
  select DISTINCT int("'''+trigger_key+'''") AS trigger_key
                     ,"'''+FeedName+'''" AS FeedName
                     ,TO_TIMESTAMP("'''+windowStartTime+'''") AS Load_TS
                     ,"Duplicate_Records" as Alert_Type
                     ,'"'''+data_PK+'''"' AS Alert_Reason
                     ,"'''+Source_container+'''" as Data_Storage
                     ,LOWER(CONCAT("stgods_dl.ods_","'''+FeedName+'''")) as Data_location
                     ,"'''+str(dfPK_Alert)+'''" as Data_Count
                     ,"N" as Alert_Sent
                     ,current_timestamp() AS Insert_Ts
                     ,null AS Update_ts
                     ,"'''+str(PipelineRunId)+'''" as Pipeline_Run_ID 
           ''')
  print("Duplicate number of rows :",dfPK_Alert)
  print("Data inserted into cfw.etl_alert_control for stgods_dl.ods_"+FeedName+" for PK Duplicate Records")

# COMMAND ----------

# DBTITLE 1, Notebook status Logging
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

for i in range(1,5,1):
    try:
      spark.sql("UPDATE stgods_dl."+log_table+" SET Flag_NotebookStatus  = 'Y' WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Flag_Temp = 'Y' AND Flag_Datalake = 'Y' AND Flag_StgODS = 'Y' AND Flag_Error = 'Y' AND Flag_Audit = 'Y' AND Flag_reject = 'Y' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"')  ")
      break;
    except Exception as e14:
      if 'com.databricks.sql.transaction.tahoe.ConcurrentAppendException' in str(e14):
        time.sleep(30)
        continue
      else:
        raise Exception(e14)

# COMMAND ----------

# DBTITLE 1, Clean up and Exit
# sc._jvm.za.co.absa.spline.harvester.SparkLineageInitializer.enableLineageTracking(spark._jsparkSession)

dflogs_final = spark.sql("SELECT DISTINCT Flag_NotebookStatus FROM stgods_dl."+log_table+"  WHERE Data_dt = '"+dt+"' AND Load_TS = '"+windowStartTime+"' AND Source_container = '"+Source_container+"' AND Trigger_Key = int('"+trigger_key+"') ")
NFS =dflogs_final.select("Flag_NotebookStatus").distinct().rdd.flatMap(lambda x: x).collect()
print(NFS)
if NFS == ['Y']:
  spark.sql(" DROP TABLE IF EXISTS stgods_dl."+table_name+" ")
  dbutils.fs.rm(path,recurse=True)
  dbutils.notebook.exit("Successful")
/Shared/DL_BULKSMS_JAM_TEST
-- MAGIC %md
-- MAGIC ##### Drop all the tables

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_processed CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed";

-- COMMAND ----------

DROP DATABASE IF EXISTS f1_presentation CASCADE;

-- COMMAND ----------

CREATE DATABASE IF NOT EXISTS f1_presentation 
LOCATION "/mnt/formula1dl/presentation";

-- COMMAND ----------


/Formula1-Project-Solutions/Section-16-17-18/utils/1.prepare_for_incremental_load
-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Report on Dominant Formula 1 Drivers </h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_drivers
AS
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) driver_rank
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE driver_name IN (SELECT driver_name FROM v_dominant_drivers WHERE driver_rank <= 10)
GROUP BY race_year, driver_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions/Section-16-17-18/analysis/3.viz_dominant_drivers
-- MAGIC %python
-- MAGIC html = """<h1 style="color:Black;text-align:center;font-family:Ariel">Report on Dominant Formula 1 Teams </h1>"""
-- MAGIC displayHTML(html)

-- COMMAND ----------

CREATE OR REPLACE TEMP VIEW v_dominant_teams
AS
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points,
       RANK() OVER(ORDER BY AVG(calculated_points) DESC) team_rank
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT * FROM v_dominant_teams;

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------

SELECT race_year, 
       team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE team_name IN (SELECT team_name FROM v_dominant_teams WHERE team_rank <= 5)
GROUP BY race_year, team_name
ORDER BY race_year, avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions/Section-16-17-18/analysis/4.viz_dominant_teams
SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT team_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2011
GROUP BY team_name
HAVING COUNT(1) >= 100
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions/Section-16-17-18/analysis/2.find_dominant_teams
SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2011 AND 2020
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------

SELECT driver_name,
       COUNT(1) AS total_races,
       SUM(calculated_points) AS total_points,
       AVG(calculated_points) AS avg_points
  FROM f1_presentation.calculated_race_results
 WHERE race_year BETWEEN 2001 AND 2010
GROUP BY driver_name
HAVING COUNT(1) >= 50
ORDER BY avg_points DESC

-- COMMAND ----------


/Formula1-Project-Solutions/Section-16-17-18/analysis/1.find_dominant_drivers
# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.pit_stops")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/6.ingest_pit_stops_file
# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.constructors")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/3.ingest_constructors_file
# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode("overwrite").partitionBy('race_year').format("parquet").saveAsTable("f1_processed.races")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/2.ingest_races_file
CREATE DATABASE IF NOT EXISTS f1_processed
LOCATION "/mnt/formula1dl/processed"

-- COMMAND ----------

DESC DATABASE f1_processed;

-- COMMAND ----------


/Formula1-Project-Solutions/Section-16-17-18/ingestion/9.create_processed_database
# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.qualifying")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/8.ingest_qualifying_file
# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.drivers")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/4.ingest_drivers_file
# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').format("parquet").saveAsTable("f1_processed.results")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/5.ingest_results_file
# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.lap_times")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/7.ingest_lap_times_file
# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").format("parquet").saveAsTable("f1_processed.circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM f1_processed.circuits;

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-16-17-18/ingestion/1.ingest_circuits_file
v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result
/Formula1-Project-Solutions/Section-16-17-18/ingestion/0.ingest_all_files
CREATE DATABASE IF NOT EXISTS f1_raw;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for CSV files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create circuits table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.circuits;
CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,
circuitRef STRING,
name STRING,
location STRING,
country STRING,
lat DOUBLE,
lng DOUBLE,
alt INT,
url STRING
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/circuits.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.circuits;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create races table

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.races;
CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT,
year INT,
round INT,
circuitId INT,
name STRING,
date DATE,
time STRING,
url STRING)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/races.csv", header true)

-- COMMAND ----------

SELECT * FROM f1_raw.races;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for JSON files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create constructors table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.constructors;
CREATE TABLE IF NOT EXISTS f1_raw.constructors(
constructorId INT,
constructorRef STRING,
name STRING,
nationality STRING,
url STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/constructors.json")

-- COMMAND ----------

SELECT * FROM f1_raw.constructors;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create drivers table
-- MAGIC * Single Line JSON
-- MAGIC * Complex structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.drivers;
CREATE TABLE IF NOT EXISTS f1_raw.drivers(
driverId INT,
driverRef STRING,
number INT,
code STRING,
name STRUCT<forename: STRING, surname: STRING>,
dob DATE,
nationality STRING,
url STRING)
USING json
OPTIONS (path "/mnt/formula1dl/raw/drivers.json")

-- COMMAND ----------

-- MAGIC %md ##### Create results table
-- MAGIC * Single Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.results;
CREATE TABLE IF NOT EXISTS f1_raw.results(
resultId INT,
raceId INT,
driverId INT,
constructorId INT,
number INT,grid INT,
position INT,
positionText STRING,
positionOrder INT,
points INT,
laps INT,
time STRING,
milliseconds INT,
fastestLap INT,
rank INT,
fastestLapTime STRING,
fastestLapSpeed FLOAT,
statusId STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/results.json")

-- COMMAND ----------

SELECT * FROM f1_raw.results

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create pit stops table
-- MAGIC * Multi Line JSON
-- MAGIC * Simple structure

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.pit_stops;
CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(
driverId INT,
duration STRING,
lap INT,
milliseconds INT,
raceId INT,
stop INT,
time STRING)
USING json
OPTIONS(path "/mnt/formula1dl/raw/pit_stops.json", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.pit_stops;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC #### Create tables for list of files

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Lap Times Table
-- MAGIC * CSV file
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.lap_times;
CREATE TABLE IF NOT EXISTS f1_raw.lap_times(
raceId INT,
driverId INT,
lap INT,
position INT,
time STRING,
milliseconds INT
)
USING csv
OPTIONS (path "/mnt/formula1dl/raw/lap_times")

-- COMMAND ----------

SELECT * FROM f1_raw.lap_times

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##### Create Qualifying Table
-- MAGIC * JSON file
-- MAGIC * MultiLine JSON
-- MAGIC * Multiple files

-- COMMAND ----------

DROP TABLE IF EXISTS f1_raw.qualifying;
CREATE TABLE IF NOT EXISTS f1_raw.qualifying(
constructorId INT,
driverId INT,
number INT,
position INT,
q1 STRING,
q2 STRING,
q3 STRING,
qualifyId INT,
raceId INT)
USING json
OPTIONS (path "/mnt/formula1dl/raw/qualifying", multiLine true)

-- COMMAND ----------

SELECT * FROM f1_raw.qualifying

-- COMMAND ----------

DESC EXTENDED f1_raw.qualifying;

-- COMMAND ----------


/Formula1-Project-Solutions/Section-16-17-18/raw/1.create_raw_tables
# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

constructor_standings_df = race_results_df \
.groupBy("race_year", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

display(constructor_standings_df.filter("race_year = 2020"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

constructor_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = constructor_standings_df.withColumn("rank", rank().over(constructor_rank_spec))

# COMMAND ----------

display(final_df.filter("race_year = 2020"))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/constructor_standings")

# COMMAND ----------


/Formula1-Project-Solutions/Section-14/trans/3.constructor_standings
# MAGIC %md
# MAGIC ##### Produce driver standings

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------

from pyspark.sql.functions import sum, when, count, col

driver_standings_df = race_results_df \
.groupBy("race_year", "driver_name", "driver_nationality", "team") \
.agg(sum("points").alias("total_points"),
     count(when(col("position") == 1, True)).alias("wins"))

# COMMAND ----------

display(driver_standings_df.filter("race_year = 2020"))

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, asc

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

# COMMAND ----------

display(final_df.filter("race_year = 2020"))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/driver_standings")

# COMMAND ----------


/Formula1-Project-Solutions/Section-14/trans/2.driver_standings
# MAGIC %md
# MAGIC ##### Read all the data as required

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

drivers_df = spark.read.parquet(f"{processed_folder_path}/drivers") \
.withColumnRenamed("number", "driver_number") \
.withColumnRenamed("name", "driver_name") \
.withColumnRenamed("nationality", "driver_nationality") 

# COMMAND ----------

constructors_df = spark.read.parquet(f"{processed_folder_path}/constructors") \
.withColumnRenamed("name", "team") 

# COMMAND ----------

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
.withColumnRenamed("location", "circuit_location") 

# COMMAND ----------

races_df = spark.read.parquet(f"{processed_folder_path}/races") \
.withColumnRenamed("name", "race_name") \
.withColumnRenamed("race_timestamp", "race_date") 

# COMMAND ----------

results_df = spark.read.parquet(f"{processed_folder_path}/results") \
.withColumnRenamed("time", "race_time") 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join circuits to races

# COMMAND ----------

race_circuits_df = races_df.join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
.select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Join results to all other dataframes

# COMMAND ----------

race_results_df = results_df.join(race_circuits_df, results_df.race_id == race_circuits_df.race_id) \
                            .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                            .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

final_df = race_results_df.select("race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", "driver_nationality",
                                 "team", "grid", "fastest_lap", "race_time", "points", "position") \
                          .withColumn("created_date", current_timestamp())

# COMMAND ----------

display(final_df.filter("race_year == 2020 and race_name == 'Abu Dhabi Grand Prix'").orderBy(final_df.points.desc()))

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/race_results")

# COMMAND ----------


/Formula1-Project-Solutions/Section-14/trans/1.race_results
# MAGIC %md
# MAGIC ### Ingest races.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

races_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                  StructField("year", IntegerType(), True),
                                  StructField("round", IntegerType(), True),
                                  StructField("circuitId", IntegerType(), True),
                                  StructField("name", StringType(), True),
                                  StructField("date", DateType(), True),
                                  StructField("time", StringType(), True),
                                  StructField("url", StringType(), True) 
])

# COMMAND ----------

races_df = spark.read \
.option("header", True) \
.schema(races_schema) \
.csv(f"{raw_folder_path}/races.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Add ingestion date and race_timestamp to the dataframe

# COMMAND ----------

from pyspark.sql.functions import to_timestamp, concat, col, lit

# COMMAND ----------

races_with_timestamp_df = races_df.withColumn("race_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss')) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

races_with_ingestion_date_df = add_ingestion_date(races_with_timestamp_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Select only the columns required & rename as required

# COMMAND ----------

races_selected_df = races_with_ingestion_date_df.select(col('raceId').alias('race_id'), col('year').alias('race_year'), 
                                                   col('round'), col('circuitId').alias('circuit_id'),col('name'), col('ingestion_date'), col('race_timestamp'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Write the output to processed container in parquet format

# COMMAND ----------

races_selected_df.write.mode('overwrite').partitionBy('race_year').parquet(f'{processed_folder_path}/races')

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/2.ingest_races_file
# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json(f"{raw_folder_path}/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

results_with_ingestion_date_df = add_ingestion_date(results_with_columns_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_ingestion_date_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').parquet(f"{processed_folder_path}/results")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/5.ingest_results_file
# MAGIC %md
# MAGIC ### Ingest qualifying json files

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

qualifying_schema = StructType(fields=[StructField("qualifyId", IntegerType(), False),
                                      StructField("raceId", IntegerType(), True),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("constructorId", IntegerType(), True),
                                      StructField("number", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("q1", StringType(), True),
                                      StructField("q2", StringType(), True),
                                      StructField("q3", StringType(), True),
                                     ])

# COMMAND ----------

qualifying_df = spark.read \
.schema(qualifying_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/qualifying")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename qualifyingId, driverId, constructorId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

qualifying_with_ingestion_date_df = add_ingestion_date(qualifying_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = qualifying_with_ingestion_date_df.withColumnRenamed("qualifyId", "qualify_id") \
.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumnRenamed("constructorId", "constructor_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/qualifying")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/8.ingest_qualifying_file
# MAGIC %md
# MAGIC ### Ingest circuits.csv file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType

# COMMAND ----------

circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])

# COMMAND ----------

circuits_df = spark.read \
.option("header", True) \
.schema(circuits_schema) \
.csv(f"{raw_folder_path}/circuits.csv")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Select only the required columns

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

circuits_selected_df = circuits_df.select(col("circuitId"), col("circuitRef"), col("name"), col("location"), col("country"), col("lat"), col("lng"), col("alt"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename the columns as required

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

circuits_renamed_df = circuits_selected_df.withColumnRenamed("circuitId", "circuit_id") \
.withColumnRenamed("circuitRef", "circuit_ref") \
.withColumnRenamed("lat", "latitude") \
.withColumnRenamed("lng", "longitude") \
.withColumnRenamed("alt", "altitude") \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### Step 4 - Add ingestion date to the dataframe

# COMMAND ----------

circuits_final_df = add_ingestion_date(circuits_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 5 - Write data to datalake as parquet

# COMMAND ----------

circuits_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/circuits")

# COMMAND ----------

display(spark.read.parquet(f"{processed_folder_path}/circuits"))

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/1.ingest_circuits_file
# MAGIC %ls
# MAGIC ls
# MAGIC
# MAGIC v_result = dbutils.notebook.run("1.ingest_circuits_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("2.ingest_races_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("3.ingest_constructors_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("4.ingest_drivers_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("5.ingest_results_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("6.ingest_pit_stops_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("7.ingest_lap_times_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result

# COMMAND ----------

v_result = dbutils.notebook.run("8.ingest_qualifying_file", 0, {"p_data_source": "Ergast API"})

# COMMAND ----------

v_result
/Formula1-Project-Solutions/Section-12/ingestion/0.ingest_all_files
# MAGIC %md
# MAGIC ### Ingest pit_stops.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

pit_stops_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("stop", StringType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("duration", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

pit_stops_df = spark.read \
.schema(pit_stops_schema) \
.option("multiLine", True) \
.json(f"{raw_folder_path}/pit_stops.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

pit_stops_with_ingestion_date_df = add_ingestion_date(pit_stops_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = pit_stops_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/pit_stops")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/6.ingest_pit_stops_file
# MAGIC %md
# MAGIC ### Ingest lap_times folder

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the CSV file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

lap_times_schema = StructType(fields=[StructField("raceId", IntegerType(), False),
                                      StructField("driverId", IntegerType(), True),
                                      StructField("lap", IntegerType(), True),
                                      StructField("position", IntegerType(), True),
                                      StructField("time", StringType(), True),
                                      StructField("milliseconds", IntegerType(), True)
                                     ])

# COMMAND ----------

lap_times_df = spark.read \
.schema(lap_times_schema) \
.csv(f"{raw_folder_path}/lap_times")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. Rename driverId and raceId
# MAGIC 1. Add ingestion_date with current timestamp

# COMMAND ----------

lap_times_with_ingestion_date_df = add_ingestion_date(lap_times_df)

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

final_df = lap_times_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
.withColumnRenamed("raceId", "race_id") \
.withColumn("ingestion_date", current_timestamp()) \
.withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Write to output to processed container in parquet format

# COMMAND ----------

final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/lap_times")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/7.ingest_lap_times_file
# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"

# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json(f"{raw_folder_path}/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import lit

# COMMAND ----------

constructor_renamed_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

constructor_final_df = add_ingestion_date(constructor_renamed_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/constructors")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/3.ingest_constructors_file
# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

dbutils.widgets.text("p_data_source", "")
v_data_source = dbutils.widgets.get("p_data_source")

# COMMAND ----------

# MAGIC %run "../includes/configuration"

# COMMAND ----------

# MAGIC %run "../includes/common_functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json(f"{raw_folder_path}/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, lit

# COMMAND ----------

drivers_with_ingestion_date_df = add_ingestion_date(drivers_df)

# COMMAND ----------

drivers_with_columns_df = drivers_with_ingestion_date_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname"))) \
                                    .withColumn("data_source", lit(v_data_source))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").parquet(f"{processed_folder_path}/drivers")

# COMMAND ----------

dbutils.notebook.exit("Success")
/Formula1-Project-Solutions/Section-12/ingestion/4.ingest_drivers_file


 


# COMMAND ----------

# CLient ID
# 51271897-1c21-4e25-9b97-4e8d1e19d05f

# Tenant ID
# fbb6a85c-fb76-4fe5-9893-abd1eace5774

# secret Value
# GEF8Q~VyRU0HE40cyO5i8ANlwZxbvpKWTyRtsaAl


storage_account_name = "zacaystorage"
client_id            = "51271897-1c21-4e25-9b97-4e8d1e19d05f"
tenant_id            = "fbb6a85c-fb76-4fe5-9893-abd1eace5774"
client_secret        = "14k8Q~5QeUU8xuYh9hYXCEb3BpFTbbNV6LdvRaY_"

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

#dbutils.fs.unmount(mount_point = f"/mnt/{storage_account_name}/{container_name}")
container_name = "raw"
mount_point = f"/mnt/{storage_account_name}/{container_name}"

# dbutils.fs.unmount(mount_point)

dbutils.fs.mount(
  source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
  mount_point = f"/mnt/{storage_account_name}/{container_name}",
  extra_configs = configs)



# COMMAND ----------

dbutils.fs.ls("/mnt/zacaystorage/raw")

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC SELECT details:flow_definition.output_dataset, details:flow_definition.input_datasets FROM event_log_raw WHERE event_type = 'flow_definition' AND origin.update_id = '${latest_update.id}'
# MAGIC
# MAGIC
# MAGIC

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

dbutils.fs.ls("/FileStore")

# COMMAND ----------


/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-6
storage_account_name = "formula1dl"
client_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-id")
tenant_id            = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-tenant-id")
client_secret        = dbutils.secrets.get(scope="formula1-scope", key="databricks-app-client-secret")

# COMMAND ----------

configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": f"{client_id}",
           "fs.azure.account.oauth2.client.secret": f"{client_secret}",
           "fs.azure.account.oauth2.client.endpoint": f"https://login.microsoftonline.com/{tenant_id}/oauth2/token"}

# COMMAND ----------

def mount_adls(container_name):
  dbutils.fs.mount(
    source = f"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/",
    mount_point = f"/mnt/{storage_account_name}/{container_name}",
    extra_configs = configs)

# COMMAND ----------

mount_adls("raw")

# COMMAND ----------

mount_adls("processed")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/raw")

# COMMAND ----------

dbutils.fs.ls("/mnt/formula1dl/processed")

# COMMAND ----------


/Formula1-Project-Solutions/Section-06/mount_adls_storage-lesson-9
# MAGIC %md
# MAGIC ### Ingest constructors.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader

# COMMAND ----------

constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"




# COMMAND ----------

constructor_df = spark.read \
.schema(constructors_schema) \
.json("/mnt/formula1dl/raw/constructors.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Drop unwanted columns from the dataframe

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

constructor_dropped_df = constructor_df.drop(col('url'))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Rename columns and add ingestion date

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

constructor_final_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("ingestion_date", current_timestamp())

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 Write output to parquet file

# COMMAND ----------

constructor_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/constructors")

# COMMAND ----------


/Formula1-Project-Solutions/Section-09-10-11/ingestion/3.ingest_constructors_file
# MAGIC %md
# MAGIC ### Ingest results.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType

# COMMAND ----------

results_schema = StructType(fields=[StructField("resultId", IntegerType(), False),
                                    StructField("raceId", IntegerType(), True),
                                    StructField("driverId", IntegerType(), True),
                                    StructField("constructorId", IntegerType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("grid", IntegerType(), True),
                                    StructField("position", IntegerType(), True),
                                    StructField("positionText", StringType(), True),
                                    StructField("positionOrder", IntegerType(), True),
                                    StructField("points", FloatType(), True),
                                    StructField("laps", IntegerType(), True),
                                    StructField("time", StringType(), True),
                                    StructField("milliseconds", IntegerType(), True),
                                    StructField("fastestLap", IntegerType(), True),
                                    StructField("rank", IntegerType(), True),
                                    StructField("fastestLapTime", StringType(), True),
                                    StructField("fastestLapSpeed", FloatType(), True),
                                    StructField("statusId", StringType(), True)])

# COMMAND ----------

results_df = spark.read \
.schema(results_schema) \
.json("/mnt/formula1dl/raw/results.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns

# COMMAND ----------

from pyspark.sql.functions import current_timestamp

# COMMAND ----------

results_with_columns_df = results_df.withColumnRenamed("resultId", "result_id") \
                                    .withColumnRenamed("raceId", "race_id") \
                                    .withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("constructorId", "constructor_id") \
                                    .withColumnRenamed("positionText", "position_text") \
                                    .withColumnRenamed("positionOrder", "position_order") \
                                    .withColumnRenamed("fastestLap", "fastest_lap") \
                                    .withColumnRenamed("fastestLapTime", "fastest_lap_time") \
                                    .withColumnRenamed("fastestLapSpeed", "fastest_lap_speed") \
                                    .withColumn("ingestion_date", current_timestamp()) 

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted column

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

results_final_df = results_with_columns_df.drop(col("statusId"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

results_final_df.write.mode("overwrite").partitionBy('race_id').parquet("/mnt/formula1dl/processed/results")

# COMMAND ----------


/Formula1-Project-Solutions/Section-09-10-11/ingestion/5.ingest_results_file
# MAGIC %md
# MAGIC ### Ingest drivers.json file

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 1 - Read the JSON file using the spark dataframe reader API

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# COMMAND ----------

name_schema = StructType(fields=[StructField("forename", StringType(), True),
                                 StructField("surname", StringType(), True)
  
])

# COMMAND ----------

drivers_schema = StructType(fields=[StructField("driverId", IntegerType(), False),
                                    StructField("driverRef", StringType(), True),
                                    StructField("number", IntegerType(), True),
                                    StructField("code", StringType(), True),
                                    StructField("name", name_schema),
                                    StructField("dob", DateType(), True),
                                    StructField("nationality", StringType(), True),
                                    StructField("url", StringType(), True)  
])

# COMMAND ----------

drivers_df = spark.read \
.schema(drivers_schema) \
.json("/mnt/formula1dl/raw/drivers.json")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 2 - Rename columns and add new columns
# MAGIC 1. driverId renamed to driver_id  
# MAGIC 1. driverRef renamed to driver_ref  
# MAGIC 1. ingestion date added
# MAGIC 1. name added with concatenation of forename and surname

# COMMAND ----------

from pyspark.sql.functions import col, concat, current_timestamp, lit

# COMMAND ----------

drivers_with_columns_df = drivers_df.withColumnRenamed("driverId", "driver_id") \
                                    .withColumnRenamed("driverRef", "driver_ref") \
                                    .withColumn("ingestion_date", current_timestamp()) \
                                    .withColumn("name", concat(col("name.forename"), lit(" "), col("name.surname")))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 3 - Drop the unwanted columns
# MAGIC 1. name.forename
# MAGIC 1. name.surname
# MAGIC 1. url

# COMMAND ----------

drivers_final_df = drivers_with_columns_df.drop(col("url"))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Step 4 - Write to output to processed container in parquet format

# COMMAND ----------

drivers_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/drivers")

# COMMAND ----------


/Formula1-Project-Solutions/Section-09-10-11/ingestion/4.ingest_drivers_file
